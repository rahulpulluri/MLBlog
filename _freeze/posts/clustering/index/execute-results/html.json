{
  "hash": "2b99159ea93e81ff56de271b6100116e",
  "result": {
    "markdown": "---\ntitle: Clustering\nauthor: Rahul Pulluri\ndate: '2023-10-27'\nimage: cluster.jpg\n---\n\n## What is Clustering?\n\nClustering in machine learning is a type of unsupervised learning technique that involves grouping similar data points together into clusters. In this context, \"unsupervised\" means that the learning process is conducted without any predefined labels or categories; the algorithm itself discovers the inherent groupings in the data.\n\n## Types of Clustering Algorithms\n\nClustering algorithms come in various forms, each suited to handle different kinds of data and objectives:\n\n**Density-Based Clustering:** These algorithms identify clusters as areas of high density separated by areas of low density. They excel in finding clusters of arbitrary shapes and typically do not assign outliers to any cluster.\n\n**Distribution-Based Clustering:** In this approach, clusters are formed based on the likelihood of data points belonging to a certain distribution. A typical model is the Gaussian distribution, where the probability of belonging to a cluster decreases as the distance from the cluster's center increases.\n\n**Centroid-Based Clustering:** This is a widely used approach where clusters are represented by a central vector or a centroid. Each data point is assigned to the cluster with the nearest centroid. K-means is a popular example of centroid-based clustering.\n\n**Hierarchical-Based Clustering:** These algorithms create a tree of clusters, which is particularly useful for hierarchical or nested data structures. It can be either divisive (top-down approach) or agglomerative (bottom-up approach).\n\n## Applications:\n\n**Customer Segmentation:** Grouping customers based on purchasing behavior, interests, demographics, etc.\n\n**Anomaly Detection:** Identifying unusual data points which can be useful in fraud detection.\n\n**Pattern Recognition:** In areas like bioinformatics, speech recognition, and image analysis.\n\n## Common Clustering algorithms:\n\n### K-Means Clustering algorithm:\n K-means clustering, the most commonly used clustering algorithm, partitions a dataset into K distinct clusters. It does this by assigning each data point to the nearest of K centroids, which are iteratively recalculated as the mean of the points assigned to them. This process repeats until the centroids stabilize, effectively minimizing the variance within each cluster. Widely appreciated for its simplicity and efficiency, this method assumes spherical clusters and can be sensitive to the initial placement of centroids and the presence of outliers.\n\n**Implementation**\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom numpy import unique, where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import KMeans, DBSCAN\n\n# Initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# Define the KMeans model\nkmeans_model = KMeans(n_clusters=2)\n\n# Assign each data point to a cluster using KMeans\nkmeans_result = kmeans_model.fit_predict(training_data)\n\n# Define the DBSCAN model\ndbscan_model = DBSCAN(eps=0.3, min_samples=9)  # Adjust 'eps' and 'min_samples' as needed\n\n# Assign each data point to a cluster using DBSCAN\ndbscan_result = dbscan_model.fit_predict(training_data)\n\n# Get all of the unique clusters from DBSCAN results\ndbscan_clusters = unique(dbscan_result)\n\n# Plot the DBSCAN clusters\nfor dbscan_cluster in dbscan_clusters:\n    # Get data points that fall in this cluster\n    index = where(dbscan_result == dbscan_cluster)\n    # Make the plot\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n\n# Show the DBSCAN plot\npyplot.show()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\rahul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-2.png){width=569 height=411}\n:::\n:::\n\n\n### DBSCAN clustering algorithm:\n \nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a clustering algorithm that identifies clusters based on the density of data points, without requiring the number of clusters to be predefined. It categorizes points as core points, border points, or noise, depending on the number of nearby points (based on a set distance eps) and a minimum number of points (minPts) required to form a dense region. DBSCAN excels in discovering clusters of arbitrary shapes and sizes, and effectively distinguishes outliers, making it robust in handling noisy datasets. This makes it particularly useful in applications like anomaly detection, geospatial analysis, and image processing.\n\n**Implementation**\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nfrom numpy import unique\nfrom numpy import where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import DBSCAN\n\n# initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# define the model\ndbscan_model = DBSCAN(eps=0.25, min_samples=9)\n\n# train the model\ndbscan_model.fit(training_data)\n\n# assign each data point to a cluster\ndbscan_result = dbscan_model.labels_\n\n# get all of the unique clusters\ndbscan_clusters = unique(dbscan_result)\n\n# plot the DBSCAN clusters\nfor cluster in dbscan_clusters:\n    # get data points that fall in this cluster\n    index = where(dbscan_result == cluster)\n    # make the plot\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n\n# show the DBSCAN plot\npyplot.show()\n\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=569 height=411}\n:::\n:::\n\n\n### Hierarchical Clustering:\n This creates a tree of clusters. It can be either agglomerative (merging small clusters into larger ones) or divisive (splitting clusters).\n\n**Agglomerative Hierarchy clustering algorithm:** \nThis bottom-up approach starts with each data point as an individual cluster and iteratively merges the closest pairs of clusters until all points are merged into a single cluster or a specified number of clusters is reached. The process of choosing which clusters to merge at each step is based on a distance metric and a linkage criterion (like single, complete, or average linkage). Agglomerative clustering is particularly useful for small to medium-sized datasets and is often visualized using a dendrogram, which shows the hierarchical relationship between clusters.\n\n**Implementation**\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# Define the model\nagglomerative_model = AgglomerativeClustering(n_clusters=2)\n\n# Assign each data point to a cluster\nagglomerative_result = agglomerative_model.fit_predict(training_data)\n\n# Get all unique clusters\nagglomerative_clusters = np.unique(agglomerative_result)\n\n# Plot the clusters\nfor cluster_label in agglomerative_clusters:\n    # Get data points that belong to this cluster\n    cluster_points = training_data[agglomerative_result == cluster_label]\n    # Make the plot\n    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {cluster_label}')\n\n# Show the Agglomerative Hierarchy plot\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=569 height=411}\n:::\n:::\n\n\n**Divisive Hierarchy clustering algorithm:**\n Conversely, Divisive clustering is a top-down approach. It begins with all data points in a single cluster and recursively splits the most heterogeneous cluster until each cluster contains only one data point or until a specified number of clusters is achieved. This method often involves analyzing the data to determine the best way to perform the splits, typically based on a measure of dissimilarity between data points.\n\n**Implementation**\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\n\n# Initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# Define the model for divisive clustering\ndef divisive_clustering(data, stopping_condition):\n    clusters = [data]  # Start with all data points in one cluster\n\n    while len(clusters) < stopping_condition:\n        # Select the cluster to split (you can define your splitting criterion here)\n        cluster_to_split = select_cluster_to_split(clusters)\n\n        # Split the selected cluster into two subclusters\n        subcluster1, subcluster2 = split_cluster(cluster_to_split)\n\n        # Remove the original cluster and add the two subclusters\n        clusters.remove(cluster_to_split)\n        clusters.append(subcluster1)\n        clusters.append(subcluster2)\n\n    return clusters\n\n# Define your custom splitting criterion (e.g., based on variance, dissimilarity, etc.)\ndef select_cluster_to_split(clusters):\n    # For simplicity, we'll select the largest cluster to split.\n    return max(clusters, key=len)\n\n# Define a function to split a cluster (you can implement your splitting logic)\ndef split_cluster(cluster):\n    midpoint = len(cluster) // 2\n    subcluster1 = cluster[:midpoint]\n    subcluster2 = cluster[midpoint:]\n    return subcluster1, subcluster2\n\n# Perform divisive clustering with a specified stopping condition\nclusters = divisive_clustering(training_data.tolist(), stopping_condition=4)\n\n# Plot the divisive clusters\nfor i, cluster in enumerate(clusters):\n    cluster = np.array(cluster)  # Convert back to NumPy array\n    plt.scatter(cluster[:, 0], cluster[:, 1], label=f'Cluster {i+1}')\n\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=569 height=411}\n:::\n:::\n\n\n### Gaussian Mixture Model algorithm: \n\nThe Gaussian Mixture Model (GMM) algorithm is a probabilistic approach used for clustering and density estimation, overcoming K-means' circular data limitation. GMM assumes data is generated from a mixture of Gaussian distributions, making it suitable for various data shapes. It iteratively estimates Gaussian parameters (mean, covariance, mixing coefficients) to fit data, providing soft cluster assignments based on probabilities. GMMs are versatile but sensitive to initializations, requiring a predefined cluster count.\n\n**Implementation**\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.mixture import GaussianMixture\n\n# Initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# Define the model\ngaussian_model = GaussianMixture(n_components=2)\n\n# Train the model\ngaussian_model.fit(training_data)\n\n# Assign each data point to a cluster\ngaussian_result = gaussian_model.predict(training_data)\n\n# Get all unique clusters\ngaussian_clusters = np.unique(gaussian_result)\n\n# Plot the Gaussian Mixture clusters\nfor cluster_label in gaussian_clusters:\n    # Get data points that belong to this cluster\n    cluster_points = training_data[gaussian_result == cluster_label]\n    # Make the plot\n    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {cluster_label}')\n\n# Show the Gaussian Mixture plot\nplt.legend()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=569 height=411}\n:::\n:::\n\n\n### Mean-Shift clustering algorithm:\n\nThe Mean-Shift clustering algorithm is a density-based method used to find clusters in data without specifying the number of clusters in advance. It estimates the density of data points, iteratively shifts them towards denser regions, and identifies clusters where points converge. Mean-Shift is versatile and suitable for various cluster shapes and sizes, commonly used in applications like image segmentation and object tracking.\n\n**Implementation**\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nfrom numpy import unique\nfrom numpy import where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import MeanShift\n\n# initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# define the model\nmean_model = MeanShift()\n\n# assign each data point to a cluster\nmean_result = mean_model.fit_predict(training_data)\n\n# get all of the unique clusters\nmean_clusters = unique(mean_result)\n\n# plot Mean-Shift the clusters\nfor mean_cluster in mean_clusters:\n    # get data points that fall in this cluster\n    index = where(mean_result == mean_cluster)\n    # make the plot\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n\n# show the Mean-Shift plot\npyplot.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=569 height=411}\n:::\n:::\n\n\n### BIRCH algorithm:\n\nThe BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies) algorithm is a memory-efficient hierarchical clustering technique designed for large datasets, particularly numeric data. It builds a tree-like structure by recursively splitting data points into compact summaries that preserve distribution information, allowing scalable clustering. BIRCH is commonly combined with other clustering techniques and is well-suited for high-dimensional data. However, it requires data transformations for handling categorical values. This approach is frequently employed in exploratory data analysis and data compression, making it a valuable tool for various applications.\n\n**Implementation**\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nfrom numpy import unique\nfrom numpy import where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import Birch\n\n# initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# define the model\nbirch_model = Birch(threshold=0.03, n_clusters=2)\n\n# train the model\nbirch_model.fit(training_data)\n\n# assign each data point to a cluster\nbirch_result = birch_model.predict(training_data)\n\n# get all of the unique clusters\nbirch_clusters = unique(birch_result)\n\n# plot the BIRCH clusters\nfor birch_cluster in birch_clusters:\n    # get data points that fall in this cluster\n    index = where(birch_result == birch_cluster)\n    # make the plot\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n\n# show the BIRCH plot\npyplot.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=569 height=411}\n:::\n:::\n\n\n### OPTICS algorithm:\n\nOPTICS (Ordering Points To Identify the Clustering Structure) is a density-based clustering algorithm that excels in identifying clusters in data with varying density. It orders data points to detect different density clusters efficiently, similar to DBSCAN, and assigns specific cluster membership distances to each point. It doesn't require specifying the number of clusters in advance, making it suitable for various dataset shapes and sizes\n\n**Implementation**\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nfrom numpy import unique\nfrom numpy import where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import OPTICS\n\n# initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# define the model\noptics_model = OPTICS(eps=0.75, min_samples=10)\n\n# assign each data point to a cluster\noptics_result = optics_model.fit_predict(training_data)\n\n# get all of the unique clusters\noptics_clusters = unique(optics_result)\n\n# plot the OPTICS clusters\nfor optics_cluster in optics_clusters:\n    # get data points that fall in this cluster\n    index = where(optics_result == optics_cluster)\n    # make the plot\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n\n# show the OPTICS plot\npyplot.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-9-output-1.png){width=569 height=411}\n:::\n:::\n\n\n### Affinity Propagation clustering algorithm:\n\nAffinity Propagation (AP) is a unique clustering algorithm that doesn't require specifying the number of clusters beforehand. It uses data point communication to discover clusters, and exemplars are found as a consensus among data points. AP is ideal for scenarios where the number of clusters is uncertain, such as computer vision problems. This clustering algorithm models data points as nodes in a network and finds exemplar data points that represent clusters through message exchange. While effective for high-dimensional data and various cluster shapes and sizes, AP can be sensitive to the initial selection of exemplars and may not perform well on large datasets.\n\n**Implementation**\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\nfrom numpy import unique\nfrom numpy import where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import AffinityPropagation\n\n# initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# define the model\nmodel = AffinityPropagation(damping=0.7)\n\n# train the model\nmodel.fit(training_data)\n\n# assign each data point to a cluster\nresult = model.predict(training_data)\n\n# get all of the unique clusters\nclusters = unique(result)\n\n# plot the clusters\nfor cluster in clusters:\n    # get data points that fall in this cluster\n    index = where(result == cluster)\n    # make the plot\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n\n# show the plot\npyplot.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-10-output-1.png){width=569 height=411}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}