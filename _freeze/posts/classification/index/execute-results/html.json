{
  "hash": "288b1810e28380a712cf29665fda50b9",
  "result": {
    "markdown": "---\ntitle: Classification\nauthor: Rahul Pulluri\ndate: '2023-11-19'\nimage: classification_main.jpeg\n---\n\n![](Classification_main.jpeg)\n\n\n## Definition and Overview of Classification\n\nClassification in machine learning and statistics is a supervised learning approach where the objective is to categorize data into predefined classes. In simpler terms, it involves deciding which category or class a new observation belongs to, based on a training set of data containing observations whose category membership is known.\n\n## Key Components\n\n**Classes or Categories:** These are the distinct groups or categories that data points are classified into. For instance, in a binary classification, there are two classes, while in multi-class classification, there could be three or more.\n\n**Features:** These are individual independent variables that act as the input for the process. Each feature contributes to determining the output class.\n\n**Labels:** In the training dataset, each data point is tagged with the correct label, which the algorithm then learns to predict.\n\n## How Classification Works\n\n**Training Phase:** The algorithm is trained on a labeled dataset, where it learns the relationship between features and the corresponding class labels.\n\n**Model Building:** The algorithm creates a model that maps inputs (features) to desired outputs (labels). This model represents the learned patterns from the data.\n\n**Testing and Prediction:** The trained model is then used to predict the class labels of new, unseen data. The performance of the model is typically evaluated using metrics like accuracy, precision, recall, and F1 score.\n\n## Types of Classification\n\n**Binary Classification:** Involves two classes. Common examples include spam detection (spam or not spam) and medical diagnoses (sick or healthy).\n\n**Multiclass Classification:** Involves more than two classes, but each instance is assigned to only one class. An example would be classifying types of fruits.\n\n**Multilabel Classification:** Each instance may be assigned to multiple classes. For example, a news article might be categorized into multiple genres like sports, politics, and finance.\n\n## Applications\n\n**Medical Diagnosis:** Identifying diseases based on symptoms and test results.\n\n**Spam Filtering:** Categorizing emails as spam or non-spam.\n\n**Sentiment Analysis:** Classifying the sentiment of text data (positive, negative, neutral).\n\n**Image Recognition:** Categorizing images into various classes like animals, objects, etc.\nCredit Scoring: Assessing creditworthiness as high-risk or low-risk.\n\nOnce trained, the algorithm can classify new, unseen data by assessing its similarity to the patterns it has learned. It predicts the likelihood of the new data point falling into one of the predefined categories. This process is akin to your email provider recognizing whether an incoming email is spam or not based on past experiences.\n\n## Classification Algorithms\n\n\n### **Logistic Regression:**\nOverview: Logistic Regression is used for binary classification problems. It models the probability that each input belongs to a particular category.\nMechanism: This algorithm uses a logistic function to squeeze the output of a linear equation between 0 and 1. The result is the probability that the given input point belongs to a certain class.\n\n![Image Source: <https://editor.analyticsvidhya.com/uploads/82109Webp.net-resize.jpg>](82109Webp.net-resize.jpg)\n\nPros:\nSimple and efficient.\nProvides a probability score for observations.\nLow variance, avoiding overfitting.\n\nCons:\nStruggles with non-linear data.\nAssumes no missing values and that predictors are independent.\n\nApplications: Commonly used in fields like credit scoring, medical fields for disease diagnosis, and predictive analytics.\n\n### **Naive Bayes:**\nOverview: Based on Bayes' Theorem, it assumes independence among predictors.\nMechanism: It calculates the probability of each class and the conditional probability of each class given each input value. These probabilities are then used to classify the data.\n\n![Image Source: <https://av-eks-blogoptimized.s3.amazonaws.com/Bayes_rule-300x172-300x172-111664.png>](Bayes_rule-300x172-300x172-111664.png)\n\nPros:\nFast and efficient.\nPerforms well with a smaller amount of data.\nHandles multi-class prediction problems well.\n\nCons:\nThe assumption of independent features is often unrealistic.\nCan be outperformed by more complex models.\n\nApplications: Widely used in spam filtering, text analysis, and sentiment analysis.\n\n### **K-Nearest Neighbors (KNN):**\nOverview: A non-parametric method used for classification and regression.\nMechanism: Classifies data based on how its neighbors are classified. It finds the K nearest points to the new data point and classifies it based on the majority class of these points.\n\n![Image Source: <https://static.javatpoint.com/tutorial/machine-learning/images/k-nearest-neighbor-algorithm-for-machine-learning2.png>](k-nearest-neighbor-algorithm-for-machine-learning2.png)\n\nPros:\nSimple and intuitive.\nNo need to build a model or tune parameters.\nFlexible to feature/distance choices.\n\nCons:\nSlows significantly as data size increases.\nSensitive to irrelevant or redundant features.\n\nApplications: Used in recommendation systems, image recognition, and more.\n\n\n### **Support Vector Machine (SVM):**\nOverview: Effective in high dimensional spaces and best suited for binary classification.\nMechanism: Constructs a hyperplane in a multidimensional space to separate different classes. SVM finds the best margin (distance between the line and the support vectors) to separate the classes.\n\n![Image Source: <https://static.javatpoint.com/tutorial/machine-learning/images/support-vector-machine-algorithm5.png>](support-vector-machine-algorithm5.png)\n\nPros:\nEffective in high-dimensional spaces.\nUses a subset of training points (support vectors), so it's memory efficient.\n\nCons:\nNot suitable for larger datasets.\nDoes not perform well with noisy data.\n\nApplications: Used in face detection, text and hypertext categorization, classification of images.\n\n### **Decision Tree:**\nOverview: A tree-structure algorithm, where each node represents a feature, each branch a decision rule, and each leaf a class.\nMechanism: Splits the data into subsets based on feature values. This process is repeated recursively, resulting in a tree with decision nodes and leaf nodes.\n\n![Image Source: <https://lh4.googleusercontent.com/v9UQUwaQTAXVH90b-Ugyw2_61_uErfYvTBtG-RNRNB_eHUFq9AmAN_2IOdfOETnbXImnQVN-wPC7_YzDgf7urCeyhyx5UZmuSwV8BVsV8VnHxl1KtgpuxDifJ4pLE23ooYXLlnc>](dig10.PNG)\n\nPros:\nEasy to interpret and explain.\nRequires little data preparation.\nCan handle both numerical and categorical data.\n\nCons:\nProne to overfitting, especially with complex trees.\nSmall changes in data can lead to a different tree.\n\nApplications: Used in customer segmentation, fraud detection, and risk assessment.\n\n## Example: Spam Classification with Naive Bayes and Support Vector Machine\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom sklearn import feature_extraction, model_selection, naive_bayes, metrics, svm\nfrom IPython.display import Image\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline  \n```\n:::\n\n\n**Exploring the Dataset**\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndata = pd.read_csv('R:/MLBlog/posts/classification/spam.csv', encoding='latin-1')\ndata.head(n=10)\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>v1</th>\n      <th>v2</th>\n      <th>Unnamed: 2</th>\n      <th>Unnamed: 3</th>\n      <th>Unnamed: 4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ham</td>\n      <td>Go until jurong point, crazy.. Available only ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ham</td>\n      <td>Ok lar... Joking wif u oni...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>spam</td>\n      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ham</td>\n      <td>U dun say so early hor... U c already then say...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ham</td>\n      <td>Nah I don't think he goes to usf, he lives aro...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>spam</td>\n      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>ham</td>\n      <td>Even my brother is not like to speak with me. ...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>ham</td>\n      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>spam</td>\n      <td>WINNER!! As a valued network customer you have...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>spam</td>\n      <td>Had your mobile 11 months or more? U R entitle...</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n**Distribution spam/non-spam plots**\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ncount_Class=pd.value_counts(data[\"v1\"], sort= True)\ncount_Class.plot(kind= 'bar', color= [\"blue\", \"orange\"])\nplt.title('Bar chart')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=583 height=473}\n:::\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ncount_Class.plot(kind = 'pie',  autopct='%1.0f%%')\nplt.title('Pie chart')\nplt.ylabel('')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=389 height=409}\n:::\n:::\n\n\n**Text Analytics**\n\nWe want to find the frequencies of words in the spam and non-spam messages. The words of the messages will be model features.\n\nWe use the function Counter.\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ncount1 = Counter(\" \".join(data[data['v1']=='ham'][\"v2\"]).split()).most_common(20)\ndf1 = pd.DataFrame.from_dict(count1)\ndf1 = df1.rename(columns={0: \"words in non-spam\", 1 : \"count\"})\ncount2 = Counter(\" \".join(data[data['v1']=='spam'][\"v2\"]).split()).most_common(20)\ndf2 = pd.DataFrame.from_dict(count2)\ndf2 = df2.rename(columns={0: \"words in spam\", 1 : \"count_\"})\n```\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\ndf1.plot.bar(legend = False)\ny_pos = np.arange(len(df1[\"words in non-spam\"]))\nplt.xticks(y_pos, df1[\"words in non-spam\"])\nplt.title('More frequent words in non-spam messages')\nplt.xlabel('words')\nplt.ylabel('number')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=601 height=469}\n:::\n:::\n\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\ndf2.plot.bar(legend = False, color = 'orange')\ny_pos = np.arange(len(df2[\"words in spam\"]))\nplt.xticks(y_pos, df2[\"words in spam\"])\nplt.title('More frequent words in spam messages')\nplt.xlabel('words')\nplt.ylabel('number')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-8-output-1.png){width=593 height=469}\n:::\n:::\n\n\nWe can see that the majority of frequent words in both classes are stop words such as 'to', 'a', 'or' and so on.\n\nWith stop words we refer to the most common words in a lenguage, there is no simgle, universal list of stop words.\n\n**Feature engineering**\n\nText preprocessing, tokenizing and filtering of stopwords are included in a high level component that is able to build a dictionary of features and transform documents to feature vectors.\n\nWe remove the stop words in order to improve the analytics\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nf = feature_extraction.text.CountVectorizer(stop_words = 'english')\nX = f.fit_transform(data[\"v2\"])\nnp.shape(X)\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\n(5572, 8404)\n```\n:::\n:::\n\n\nWe have created more than 8400 new features. The new feature  j\n  in the row  i\n  is equal to 1 if the word  wj\n  appears in the text example  i\n . It is zero if not.\n\n**Predictive Analysis**\n\nMy goal is to predict if a new sms is spam or non-spam. I assume that is much worse misclassify non-spam than misclassify an spam. (I don't want to have false positives)\n\nThe reason is because I normally don't check the spam messages.\n\nThe two possible situations are:\n\n1. New spam sms in my inbox. (False negative).\nOUTCOME: I delete it.\n\n2. New non-spam sms in my spam folder (False positive).\nOUTCOME: I probably don't read it.\n\nI prefer the first option!!!\n\nFirst we transform the variable spam/non-spam into binary variable, then we split our data set in training set and test set.\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ndata[\"v1\"]=data[\"v1\"].map({'spam':1,'ham':0})\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, data['v1'], test_size=0.33, random_state=42)\nprint([np.shape(X_train), np.shape(X_test)])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[(3733, 8404), (1839, 8404)]\n```\n:::\n:::\n\n\n**Multinomial naive bayes classifier**\n\nWe train different bayes models changing the regularization parameter α.\n\nWe evaluate the accuracy, recall and precision of the model with the test set.\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nlist_alpha = np.arange(1/100000, 20, 0.11)\nscore_train = np.zeros(len(list_alpha))\nscore_test = np.zeros(len(list_alpha))\nrecall_test = np.zeros(len(list_alpha))\nprecision_test= np.zeros(len(list_alpha))\ncount = 0\nfor alpha in list_alpha:\n    bayes = naive_bayes.MultinomialNB(alpha=alpha)\n    bayes.fit(X_train, y_train)\n    score_train[count] = bayes.score(X_train, y_train)\n    score_test[count]= bayes.score(X_test, y_test)\n    recall_test[count] = metrics.recall_score(y_test, bayes.predict(X_test))\n    precision_test[count] = metrics.precision_score(y_test, bayes.predict(X_test))\n    count = count + 1 \n```\n:::\n\n\nLet's see the first 10 learning models and their metrics!\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nmatrix = np.matrix(np.c_[list_alpha, score_train, score_test, recall_test, precision_test])\nmodels = pd.DataFrame(data = matrix, columns = \n             ['alpha', 'Train Accuracy', 'Test Accuracy', 'Test Recall', 'Test Precision'])\nmodels.head(n=10)\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>alpha</th>\n      <th>Train Accuracy</th>\n      <th>Test Accuracy</th>\n      <th>Test Recall</th>\n      <th>Test Precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00001</td>\n      <td>0.998661</td>\n      <td>0.974443</td>\n      <td>0.920635</td>\n      <td>0.895753</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.11001</td>\n      <td>0.997857</td>\n      <td>0.976074</td>\n      <td>0.936508</td>\n      <td>0.893939</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.22001</td>\n      <td>0.997857</td>\n      <td>0.977162</td>\n      <td>0.936508</td>\n      <td>0.900763</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.33001</td>\n      <td>0.997589</td>\n      <td>0.977162</td>\n      <td>0.936508</td>\n      <td>0.900763</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.44001</td>\n      <td>0.997053</td>\n      <td>0.977162</td>\n      <td>0.936508</td>\n      <td>0.900763</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.55001</td>\n      <td>0.996250</td>\n      <td>0.976618</td>\n      <td>0.936508</td>\n      <td>0.897338</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.66001</td>\n      <td>0.996518</td>\n      <td>0.976074</td>\n      <td>0.932540</td>\n      <td>0.896947</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.77001</td>\n      <td>0.996518</td>\n      <td>0.976074</td>\n      <td>0.924603</td>\n      <td>0.903101</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.88001</td>\n      <td>0.996250</td>\n      <td>0.976074</td>\n      <td>0.924603</td>\n      <td>0.903101</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.99001</td>\n      <td>0.995982</td>\n      <td>0.976074</td>\n      <td>0.920635</td>\n      <td>0.906250</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nI select the model with the most test precision\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nbest_index = models['Test Precision'].idxmax()\nmodels.iloc[best_index, :]\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\nalpha             15.730010\nTrain Accuracy     0.979641\nTest Accuracy      0.969549\nTest Recall        0.777778\nTest Precision     1.000000\nName: 143, dtype: float64\n```\n:::\n:::\n\n\nMy best model does not produce any false positive, which is our goal.\n\nLet's see if there is more than one model with 100% precision !\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\nmodels[models['Test Precision']==1].head(n=5)\n```\n\n::: {.cell-output .cell-output-display execution_count=13}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>alpha</th>\n      <th>Train Accuracy</th>\n      <th>Test Accuracy</th>\n      <th>Test Recall</th>\n      <th>Test Precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>143</th>\n      <td>15.73001</td>\n      <td>0.979641</td>\n      <td>0.969549</td>\n      <td>0.777778</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>144</th>\n      <td>15.84001</td>\n      <td>0.979641</td>\n      <td>0.969549</td>\n      <td>0.777778</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>145</th>\n      <td>15.95001</td>\n      <td>0.979641</td>\n      <td>0.969549</td>\n      <td>0.777778</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>146</th>\n      <td>16.06001</td>\n      <td>0.979373</td>\n      <td>0.969549</td>\n      <td>0.777778</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>147</th>\n      <td>16.17001</td>\n      <td>0.979373</td>\n      <td>0.969549</td>\n      <td>0.777778</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nBetween these models with the highest possible precision, we are going to select which has more test accuracy.\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nbest_index = models[models['Test Precision']==1]['Test Accuracy'].idxmax()\nbayes = naive_bayes.MultinomialNB(alpha=list_alpha[best_index])\nbayes.fit(X_train, y_train)\nmodels.iloc[best_index, :]\n```\n\n::: {.cell-output .cell-output-display execution_count=14}\n```\nalpha             15.730010\nTrain Accuracy     0.979641\nTest Accuracy      0.969549\nTest Recall        0.777778\nTest Precision     1.000000\nName: 143, dtype: float64\n```\n:::\n:::\n\n\nConfusion matrix with naive bayes classifier\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\nm_confusion_test = metrics.confusion_matrix(y_test, bayes.predict(X_test))\npd.DataFrame(data = m_confusion_test, columns = ['Predicted 0', 'Predicted 1'],\n            index = ['Actual 0', 'Actual 1'])\n```\n\n::: {.cell-output .cell-output-display execution_count=15}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Predicted 0</th>\n      <th>Predicted 1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Actual 0</th>\n      <td>1587</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>Actual 1</th>\n      <td>56</td>\n      <td>196</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe misclassify 56 spam messages as non-spam emails whereas we don't misclassify any non-spam message.\n\n**Support Vector Machine**\n\nWe are going to apply the same reasoning applying the support vector machine model with the gaussian kernel.\n\nWe train different models changing the regularization parameter C.\n\nWe evaluate the accuracy, recall and precision of the model with the test set.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nlist_C = np.arange(500, 2000, 100) #100000\nscore_train = np.zeros(len(list_C))\nscore_test = np.zeros(len(list_C))\nrecall_test = np.zeros(len(list_C))\nprecision_test= np.zeros(len(list_C))\ncount = 0\nfor C in list_C:\n    svc = svm.SVC(C=C)\n    svc.fit(X_train, y_train)\n    score_train[count] = svc.score(X_train, y_train)\n    score_test[count]= svc.score(X_test, y_test)\n    recall_test[count] = metrics.recall_score(y_test, svc.predict(X_test))\n    precision_test[count] = metrics.precision_score(y_test, svc.predict(X_test))\n    count = count + 1 \n```\n:::\n\n\nLet's see the first 10 learning models and their metrics!\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nmatrix = np.matrix(np.c_[list_C, score_train, score_test, recall_test, precision_test])\nmodels = pd.DataFrame(data = matrix, columns = \n             ['C', 'Train Accuracy', 'Test Accuracy', 'Test Recall', 'Test Precision'])\nmodels.head(n=10)\n```\n\n::: {.cell-output .cell-output-display execution_count=17}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>C</th>\n      <th>Train Accuracy</th>\n      <th>Test Accuracy</th>\n      <th>Test Recall</th>\n      <th>Test Precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>500.0</td>\n      <td>1.0</td>\n      <td>0.979337</td>\n      <td>0.853175</td>\n      <td>0.99537</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>600.0</td>\n      <td>1.0</td>\n      <td>0.979337</td>\n      <td>0.853175</td>\n      <td>0.99537</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>700.0</td>\n      <td>1.0</td>\n      <td>0.979337</td>\n      <td>0.853175</td>\n      <td>0.99537</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>800.0</td>\n      <td>1.0</td>\n      <td>0.979337</td>\n      <td>0.853175</td>\n      <td>0.99537</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>900.0</td>\n      <td>1.0</td>\n      <td>0.979337</td>\n      <td>0.853175</td>\n      <td>0.99537</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1000.0</td>\n      <td>1.0</td>\n      <td>0.979337</td>\n      <td>0.853175</td>\n      <td>0.99537</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1100.0</td>\n      <td>1.0</td>\n      <td>0.979337</td>\n      <td>0.853175</td>\n      <td>0.99537</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>1200.0</td>\n      <td>1.0</td>\n      <td>0.979337</td>\n      <td>0.853175</td>\n      <td>0.99537</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>1300.0</td>\n      <td>1.0</td>\n      <td>0.979337</td>\n      <td>0.853175</td>\n      <td>0.99537</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>1400.0</td>\n      <td>1.0</td>\n      <td>0.979337</td>\n      <td>0.853175</td>\n      <td>0.99537</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nI select the model with the most test precision\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nbest_index = models['Test Precision'].idxmax()\nmodels.iloc[best_index, :]\n```\n\n::: {.cell-output .cell-output-display execution_count=18}\n```\nC                 500.000000\nTrain Accuracy      1.000000\nTest Accuracy       0.979337\nTest Recall         0.853175\nTest Precision      0.995370\nName: 0, dtype: float64\n```\n:::\n:::\n\n\nMy best model does not produce any false positive, which is our goal.\n\n\nConfusion matrix with support vector machine classifier.\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\nm_confusion_test = metrics.confusion_matrix(y_test, svc.predict(X_test))\npd.DataFrame(data = m_confusion_test, columns = ['Predicted 0', 'Predicted 1'],\n            index = ['Actual 0', 'Actual 1'])\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Predicted 0</th>\n      <th>Predicted 1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Actual 0</th>\n      <td>1586</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>Actual 1</th>\n      <td>37</td>\n      <td>215</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nWe misclassify 37 spam as non-spam messages whereas we don't misclassify any non-spam message.\n\n**Conclusion**\n\nThe best model I have found is support vector machine with 98.3% accuracy.\n\nIt classifies every non-spam message correctly (Model precision)\n\nIt classifies the 87.7% of spam messages correctly (Model recall)\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}