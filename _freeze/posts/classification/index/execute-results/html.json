{
  "hash": "16fada45bca3f7ae720b96a4142d0d6e",
  "result": {
    "markdown": "---\ntitle: Classification\nauthor: Rahul Pulluri\ndate: '2023-11-19'\nimage: classification_main.jpeg\n---\n\n![](Classification_main.jpeg)\n\n\n## Definition and Overview of Classification\n\nClassification in machine learning and statistics is a supervised learning approach where the objective is to categorize data into predefined classes. In simpler terms, it involves deciding which category or class a new observation belongs to, based on a training set of data containing observations whose category membership is known.\n\n## Key Components\n\n**Classes or Categories:** These are the distinct groups or categories that data points are classified into. For instance, in a binary classification, there are two classes, while in multi-class classification, there could be three or more.\n\n**Features:** These are individual independent variables that act as the input for the process. Each feature contributes to determining the output class.\n\n**Labels:** In the training dataset, each data point is tagged with the correct label, which the algorithm then learns to predict.\n\n## How Classification Works\n\n**Training Phase:** The algorithm is trained on a labeled dataset, where it learns the relationship between features and the corresponding class labels.\n\n**Model Building:** The algorithm creates a model that maps inputs (features) to desired outputs (labels). This model represents the learned patterns from the data.\n\n**Testing and Prediction:** The trained model is then used to predict the class labels of new, unseen data. The performance of the model is typically evaluated using metrics like accuracy, precision, recall, and F1 score.\n\n## Types of Classification\n\n**Binary Classification:** Involves two classes. Common examples include spam detection (spam or not spam) and medical diagnoses (sick or healthy).\n\n**Multiclass Classification:** Involves more than two classes, but each instance is assigned to only one class. An example would be classifying types of fruits.\n\n**Multilabel Classification:** Each instance may be assigned to multiple classes. For example, a news article might be categorized into multiple genres like sports, politics, and finance.\n\n## Applications\n\n**Medical Diagnosis:** Identifying diseases based on symptoms and test results.\n\n**Spam Filtering:** Categorizing emails as spam or non-spam.\n\n**Sentiment Analysis:** Classifying the sentiment of text data (positive, negative, neutral).\n\n**Image Recognition:** Categorizing images into various classes like animals, objects, etc.\nCredit Scoring: Assessing creditworthiness as high-risk or low-risk.\n\nOnce trained, the algorithm can classify new, unseen data by assessing its similarity to the patterns it has learned. It predicts the likelihood of the new data point falling into one of the predefined categories. This process is akin to your email provider recognizing whether an incoming email is spam or not based on past experiences.\n\n## Classification Algorithms\n\n\n### **Logistic Regression:**\nOverview: Logistic Regression is used for binary classification problems. It models the probability that each input belongs to a particular category.\nMechanism: This algorithm uses a logistic function to squeeze the output of a linear equation between 0 and 1. The result is the probability that the given input point belongs to a certain class.\n\n![Image Source: <https://editor.analyticsvidhya.com/uploads/82109Webp.net-resize.jpg>](82109Webp.net-resize.jpg)\n\nPros:\nSimple and efficient.\nProvides a probability score for observations.\nLow variance, avoiding overfitting.\n\nCons:\nStruggles with non-linear data.\nAssumes no missing values and that predictors are independent.\n\nApplications: Commonly used in fields like credit scoring, medical fields for disease diagnosis, and predictive analytics.\n\n### **Naive Bayes:**\nOverview: Based on Bayes' Theorem, it assumes independence among predictors.\nMechanism: It calculates the probability of each class and the conditional probability of each class given each input value. These probabilities are then used to classify the data.\n\n![Image Source: <https://av-eks-blogoptimized.s3.amazonaws.com/Bayes_rule-300x172-300x172-111664.png>](Bayes_rule-300x172-300x172-111664.png)\n\nPros:\nFast and efficient.\nPerforms well with a smaller amount of data.\nHandles multi-class prediction problems well.\n\nCons:\nThe assumption of independent features is often unrealistic.\nCan be outperformed by more complex models.\n\nApplications: Widely used in spam filtering, text analysis, and sentiment analysis.\n\n### **K-Nearest Neighbors (KNN):**\nOverview: A non-parametric method used for classification and regression.\nMechanism: Classifies data based on how its neighbors are classified. It finds the K nearest points to the new data point and classifies it based on the majority class of these points.\n\n![Image Source: <https://static.javatpoint.com/tutorial/machine-learning/images/k-nearest-neighbor-algorithm-for-machine-learning2.png>](k-nearest-neighbor-algorithm-for-machine-learning2.png)\n\nPros:\nSimple and intuitive.\nNo need to build a model or tune parameters.\nFlexible to feature/distance choices.\n\nCons:\nSlows significantly as data size increases.\nSensitive to irrelevant or redundant features.\n\nApplications: Used in recommendation systems, image recognition, and more.\n\n\n### **Support Vector Machine (SVM):**\nOverview: Effective in high dimensional spaces and best suited for binary classification.\nMechanism: Constructs a hyperplane in a multidimensional space to separate different classes. SVM finds the best margin (distance between the line and the support vectors) to separate the classes.\n\n![Image Source: <https://static.javatpoint.com/tutorial/machine-learning/images/support-vector-machine-algorithm5.png>](support-vector-machine-algorithm5.png)\n\nPros:\nEffective in high-dimensional spaces.\nUses a subset of training points (support vectors), so it's memory efficient.\n\nCons:\nNot suitable for larger datasets.\nDoes not perform well with noisy data.\n\nApplications: Used in face detection, text and hypertext categorization, classification of images.\n\n### **Decision Tree:**\nOverview: A tree-structure algorithm, where each node represents a feature, each branch a decision rule, and each leaf a class.\nMechanism: Splits the data into subsets based on feature values. This process is repeated recursively, resulting in a tree with decision nodes and leaf nodes.\n\n![Image Source: <https://lh4.googleusercontent.com/v9UQUwaQTAXVH90b-Ugyw2_61_uErfYvTBtG-RNRNB_eHUFq9AmAN_2IOdfOETnbXImnQVN-wPC7_YzDgf7urCeyhyx5UZmuSwV8BVsV8VnHxl1KtgpuxDifJ4pLE23ooYXLlnc>](dig10.PNG)\n\nPros:\nEasy to interpret and explain.\nRequires little data preparation.\nCan handle both numerical and categorical data.\n\nCons:\nProne to overfitting, especially with complex trees.\nSmall changes in data can lead to a different tree.\n\nApplications: Used in customer segmentation, fraud detection, and risk assessment.\n\n## Example: Credit Card Fraud Detection\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\n#Importing librairies\n\nimport pandas as pd \nimport numpy as np\n\n# Scikit-learn library: For SVM\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import svm\n\nimport itertools\n\n# Matplotlib library to plot the charts\nimport matplotlib.pyplot as plt\nimport matplotlib.mlab as mlab\n\n# Library for the statistic data vizualisation\nimport seaborn\n\n%matplotlib inline\n\n```\n:::\n\n\n**Data recuperation**\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\ndata = pd.read_csv('R:/Blog/posts/classification/creditcard.csv') # Reading the file .csv\ndf = pd.DataFrame(data) # Converting data to Panda DataFrame\n```\n:::\n\n\n**Data Visualization**\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\ndf = pd.DataFrame(data) # Converting data to Panda DataFrame\n```\n:::\n\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\ndf.describe() # Description of statistic features (Sum, Average, Variance, minimum, 1st quartile, 2nd quartile, 3rd Quartile and Maximum)\n```\n\n::: {.cell-output .cell-output-display execution_count=56}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>V1</th>\n      <th>V2</th>\n      <th>V3</th>\n      <th>V4</th>\n      <th>V5</th>\n      <th>V6</th>\n      <th>V7</th>\n      <th>V8</th>\n      <th>V9</th>\n      <th>...</th>\n      <th>V21</th>\n      <th>V22</th>\n      <th>V23</th>\n      <th>V24</th>\n      <th>V25</th>\n      <th>V26</th>\n      <th>V27</th>\n      <th>V28</th>\n      <th>Amount</th>\n      <th>Class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>284807.000000</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>...</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>2.848070e+05</td>\n      <td>284807.000000</td>\n      <td>284807.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>94813.859575</td>\n      <td>1.168375e-15</td>\n      <td>3.416908e-16</td>\n      <td>-1.379537e-15</td>\n      <td>2.074095e-15</td>\n      <td>9.604066e-16</td>\n      <td>1.487313e-15</td>\n      <td>-5.556467e-16</td>\n      <td>1.213481e-16</td>\n      <td>-2.406331e-15</td>\n      <td>...</td>\n      <td>1.654067e-16</td>\n      <td>-3.568593e-16</td>\n      <td>2.578648e-16</td>\n      <td>4.473266e-15</td>\n      <td>5.340915e-16</td>\n      <td>1.683437e-15</td>\n      <td>-3.660091e-16</td>\n      <td>-1.227390e-16</td>\n      <td>88.349619</td>\n      <td>0.001727</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>47488.145955</td>\n      <td>1.958696e+00</td>\n      <td>1.651309e+00</td>\n      <td>1.516255e+00</td>\n      <td>1.415869e+00</td>\n      <td>1.380247e+00</td>\n      <td>1.332271e+00</td>\n      <td>1.237094e+00</td>\n      <td>1.194353e+00</td>\n      <td>1.098632e+00</td>\n      <td>...</td>\n      <td>7.345240e-01</td>\n      <td>7.257016e-01</td>\n      <td>6.244603e-01</td>\n      <td>6.056471e-01</td>\n      <td>5.212781e-01</td>\n      <td>4.822270e-01</td>\n      <td>4.036325e-01</td>\n      <td>3.300833e-01</td>\n      <td>250.120109</td>\n      <td>0.041527</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>0.000000</td>\n      <td>-5.640751e+01</td>\n      <td>-7.271573e+01</td>\n      <td>-4.832559e+01</td>\n      <td>-5.683171e+00</td>\n      <td>-1.137433e+02</td>\n      <td>-2.616051e+01</td>\n      <td>-4.355724e+01</td>\n      <td>-7.321672e+01</td>\n      <td>-1.343407e+01</td>\n      <td>...</td>\n      <td>-3.483038e+01</td>\n      <td>-1.093314e+01</td>\n      <td>-4.480774e+01</td>\n      <td>-2.836627e+00</td>\n      <td>-1.029540e+01</td>\n      <td>-2.604551e+00</td>\n      <td>-2.256568e+01</td>\n      <td>-1.543008e+01</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>54201.500000</td>\n      <td>-9.203734e-01</td>\n      <td>-5.985499e-01</td>\n      <td>-8.903648e-01</td>\n      <td>-8.486401e-01</td>\n      <td>-6.915971e-01</td>\n      <td>-7.682956e-01</td>\n      <td>-5.540759e-01</td>\n      <td>-2.086297e-01</td>\n      <td>-6.430976e-01</td>\n      <td>...</td>\n      <td>-2.283949e-01</td>\n      <td>-5.423504e-01</td>\n      <td>-1.618463e-01</td>\n      <td>-3.545861e-01</td>\n      <td>-3.171451e-01</td>\n      <td>-3.269839e-01</td>\n      <td>-7.083953e-02</td>\n      <td>-5.295979e-02</td>\n      <td>5.600000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>84692.000000</td>\n      <td>1.810880e-02</td>\n      <td>6.548556e-02</td>\n      <td>1.798463e-01</td>\n      <td>-1.984653e-02</td>\n      <td>-5.433583e-02</td>\n      <td>-2.741871e-01</td>\n      <td>4.010308e-02</td>\n      <td>2.235804e-02</td>\n      <td>-5.142873e-02</td>\n      <td>...</td>\n      <td>-2.945017e-02</td>\n      <td>6.781943e-03</td>\n      <td>-1.119293e-02</td>\n      <td>4.097606e-02</td>\n      <td>1.659350e-02</td>\n      <td>-5.213911e-02</td>\n      <td>1.342146e-03</td>\n      <td>1.124383e-02</td>\n      <td>22.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>139320.500000</td>\n      <td>1.315642e+00</td>\n      <td>8.037239e-01</td>\n      <td>1.027196e+00</td>\n      <td>7.433413e-01</td>\n      <td>6.119264e-01</td>\n      <td>3.985649e-01</td>\n      <td>5.704361e-01</td>\n      <td>3.273459e-01</td>\n      <td>5.971390e-01</td>\n      <td>...</td>\n      <td>1.863772e-01</td>\n      <td>5.285536e-01</td>\n      <td>1.476421e-01</td>\n      <td>4.395266e-01</td>\n      <td>3.507156e-01</td>\n      <td>2.409522e-01</td>\n      <td>9.104512e-02</td>\n      <td>7.827995e-02</td>\n      <td>77.165000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>172792.000000</td>\n      <td>2.454930e+00</td>\n      <td>2.205773e+01</td>\n      <td>9.382558e+00</td>\n      <td>1.687534e+01</td>\n      <td>3.480167e+01</td>\n      <td>7.330163e+01</td>\n      <td>1.205895e+02</td>\n      <td>2.000721e+01</td>\n      <td>1.559499e+01</td>\n      <td>...</td>\n      <td>2.720284e+01</td>\n      <td>1.050309e+01</td>\n      <td>2.252841e+01</td>\n      <td>4.584549e+00</td>\n      <td>7.519589e+00</td>\n      <td>3.517346e+00</td>\n      <td>3.161220e+01</td>\n      <td>3.384781e+01</td>\n      <td>25691.160000</td>\n      <td>1.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>8 rows × 31 columns</p>\n</div>\n```\n:::\n:::\n\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\ndf_fraud = df[df['Class'] == 1] # Recovery of fraud data\nplt.figure(figsize=(15,10))\nplt.scatter(df_fraud['Time'], df_fraud['Amount']) # Display fraud amounts according to their time\nplt.title('Scratter plot amount fraud')\nplt.xlabel('Time')\nplt.ylabel('Amount')\nplt.xlim([0,175000])\nplt.ylim([0,2500])\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=1248 height=864}\n:::\n:::\n\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nnb_big_fraud = df_fraud[df_fraud['Amount'] > 1000].shape[0] # Recovery of frauds over 1000\nprint('There are only '+ str(nb_big_fraud) + ' frauds where the amount was bigger than 1000 over ' + str(df_fraud.shape[0]) + ' frauds')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThere are only 9 frauds where the amount was bigger than 1000 over 492 frauds\n```\n:::\n:::\n\n\nThere are only 9 frauds where the amount was bigger than 1000 over 492 frauds\n\n-   Unbalanced data\n\n::: {.cell execution_count=7}\n``` {.python .cell-code}\nnumber_fraud = len(data[data.Class == 1])\nnumber_no_fraud = len(data[data.Class == 0])\nprint('There are only '+ str(number_fraud) + ' frauds in the original dataset, even though there are ' + str(number_no_fraud) +' no frauds in the dataset.')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThere are only 492 frauds in the original dataset, even though there are 284315 no frauds in the dataset.\n```\n:::\n:::\n\n\nThere are only 492 frauds in the original dataset, even though there are 284315 no frauds in the dataset.\n\n::: {.cell execution_count=8}\n``` {.python .cell-code}\nprint(\"The accuracy of the classifier then would be : \"+ str((284315-492)/284315)+ \" which is the number of good classification over the number of tuple to classify\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nThe accuracy of the classifier then would be : 0.998269524998681 which is the number of good classification over the number of tuple to classify\n```\n:::\n:::\n\n\n**Correlation of features**\n\n::: {.cell execution_count=9}\n``` {.python .cell-code}\ndf_corr = df.corr() # Calculation of the correlation coefficients in pairs, with the default method:\n                    # Pearson, Standard Correlation Coefficient\n```\n:::\n\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\nplt.figure(figsize=(15,10))\nseaborn.heatmap(df_corr, cmap=\"YlGnBu\") # Displaying the Heatmap\nseaborn.set(font_scale=2,style='white')\n\nplt.title('Heatmap correlation')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-11-output-1.png){width=1161 height=875}\n:::\n:::\n\n\nAs we can notice, most of the features are not correlated with each other. This corroborates the fact that a PCA was previously performed on the data.\n\nWhat can generally be done on a massive dataset is a dimension reduction. By picking th emost important dimensions, there is a possiblity of explaining most of the problem, thus gaining a considerable amount of time while preventing the accuracy to drop too much.\n\nHowever in this case given the fact that a PCA was previously performed, if the dimension reduction is effective then the PCA wasn't computed in the most effective way. Another way to put it is that no dimension reduction should be computed on a dataset on which a PCA was computed correctly.\n\n**Correlation of features**\n\nOVERSAMPLING\n\nOne way to do oversampling is to replicate the under-represented class tuples until we attain a correct proportion between the class\n\nHowever as we haven't infinite time nor the patience, we are going to run the classifier with the undersampled training data (for those using the undersampling principle if results are really bad just rerun the training dataset definition)\n\nUNDERSAMPLING\n\n::: {.cell execution_count=11}\n``` {.python .cell-code}\nimport pandas as pd\n# We seperate ours data in two groups : a train dataset and a test dataset\n\n# First we build our train dataset\ndf_train_all = df[0:150000] # We cut in two the original dataset\ndf_train_1 = df_train_all[df_train_all['Class'] == 1] # We seperate the data which are the frauds and the no frauds\ndf_train_0 = df_train_all[df_train_all['Class'] == 0]\nprint('In this dataset, we have ' + str(len(df_train_1)) +\" frauds so we need to take a similar number of non-fraud\")\n\ndf_sample=df_train_0.sample(300)\ncombined_df = pd.concat([df_train_1, df_sample], ignore_index=True) # We gather the frauds with the no frauds. \ncombined_df = combined_df.sample(frac=1) # Then we mix our dataset\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIn this dataset, we have 293 frauds so we need to take a similar number of non-fraud\n```\n:::\n:::\n\n\nIn this dataset, we have 293 frauds so we need to take a similar number of non-fraud\n\n::: {.cell execution_count=12}\n``` {.python .cell-code}\nX_train = combined_df.drop(['Time', 'Class'], axis=1)  # Drop the features \"Time\" (useless) and \"Class\" (label)\ny_train = combined_df['Class']  # Create the label\nX_train = np.asarray(X_train)\ny_train = np.asarray(y_train)\n```\n:::\n\n\n::: {.cell execution_count=13}\n``` {.python .cell-code}\n############################## with all the test dataset to see if the model learn correctly ##################\ndf_test_all = df[150000:]\n\nX_test_all = df_test_all.drop(['Time', 'Class'],axis=1)\ny_test_all = df_test_all['Class']\nX_test_all = np.asarray(X_test_all)\n```\n:::\n\n\n**Confusion Matrix**\n\n::: {.cell execution_count=14}\n``` {.python .cell-code}\nclass_names=np.array(['0','1']) # Binary label, Class = 1 (fraud) and Class = 0 (no fraud)\n```\n:::\n\n\n::: {.cell execution_count=15}\n``` {.python .cell-code}\n# Function to plot the confusion Matrix\ndef plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = 'd' \n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n```\n:::\n\n\n**Model Selection**\n\nSo now, we'll use a SVM model classifier, with the scikit-learn library.\n\n::: {.cell execution_count=16}\n``` {.python .cell-code}\nclassifier = svm.SVC(kernel='linear') # We set a SVM classifier, the default SVM Classifier (Kernel = Radial Basis Function)\n```\n:::\n\n\n::: {.cell execution_count=17}\n``` {.python .cell-code}\nclassifier.fit(X_train, y_train) # Then we train our model, with our balanced data train.\n```\n\n::: {.cell-output .cell-output-display execution_count=69}\n```{=html}\n<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;linear&#x27;)</pre></div></div></div></div></div>\n```\n:::\n:::\n\n\n**Testing the model**\n\n::: {.cell execution_count=18}\n``` {.python .cell-code}\nprediction_SVM_all = classifier.predict(X_test_all) #And finally, we predict our data test.\n```\n:::\n\n\n::: {.cell execution_count=19}\n``` {.python .cell-code}\ncm = confusion_matrix(y_test_all, prediction_SVM_all)\nplot_confusion_matrix(cm,class_names)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-20-output-1.png){width=573 height=464}\n:::\n:::\n\n\nIn this case we are gonna try to minimize the number of errors in our prediction results. Errors are on the anti-diagonal of the confusion matrix. But we can infer that being wrong about an actual fraud is far worse than being wrong about a non-fraud transaction.\n\nThat is why using the accuracy as only classification criterion could be considered unthoughtful. During the remaining part of this study our criterion will consider precision on the real fraud 4 times more important than the general accuracy. Even though the final tested result is accuracy.\n\n::: {.cell execution_count=20}\n``` {.python .cell-code}\nprint('Our criterion give a result of ' \n      + str( ( (cm[0][0]+cm[1][1]) / (sum(cm[0]) + sum(cm[1])) + 4 * cm[1][1]/(cm[1][0]+cm[1][1])) / 5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOur criterion give a result of 0.9170704904644433\n```\n:::\n:::\n\n\nOur criterion give a result of 0.905383035408\n\n::: {.cell execution_count=21}\n``` {.python .cell-code}\nprint('We have detected ' + str(cm[1][1]) + ' frauds / ' + str(cm[1][1]+cm[1][0]) + ' total frauds.')\nprint('\\nSo, the probability to detect a fraud is ' + str(cm[1][1]/(cm[1][1]+cm[1][0])))\nprint(\"the accuracy is : \"+str((cm[0][0]+cm[1][1]) / (sum(cm[0]) + sum(cm[1]))))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWe have detected 181 frauds / 199 total frauds.\n\nSo, the probability to detect a fraud is 0.9095477386934674\nthe accuracy is : 0.9471614975483469\n```\n:::\n:::\n\n\nWe have detected 177 frauds / 199 total frauds.\n\nSo, the probability to detect a fraud is 0.889447236181\nthe accuracy is : 0.969126232317\n\n**Models Rank**\n\nThere is a need to compute the fit method again, as the dimension of the tuples to predict went from 29 to 10 because of the dimension reduction\n\n::: {.cell execution_count=22}\n``` {.python .cell-code}\n# Define a simple feature ranking function (you can customize this)\ndef rank_features(data):\n    # Implement your feature ranking logic here\n    ranked_features = data  # Replace this with your actual feature ranking code\n    return ranked_features\n\n# Now you can use the rank_features function\nX_train_rank = rank_features(X_train)\nX_test_all_rank = rank_features(X_test_all)\n\n```\n:::\n\n\n::: {.cell execution_count=23}\n``` {.python .cell-code}\nprediction_SVM = classifier.predict(X_test_all_rank)\n```\n:::\n\n\n::: {.cell execution_count=24}\n``` {.python .cell-code}\ncm = confusion_matrix(y_test_all, prediction_SVM)\nplot_confusion_matrix(cm,class_names)\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-25-output-1.png){width=573 height=464}\n:::\n:::\n\n\n::: {.cell execution_count=25}\n``` {.python .cell-code}\nprint('Our criterion give a result of ' \n      + str( ( (cm[0][0]+cm[1][1]) / (sum(cm[0]) + sum(cm[1])) + 4 * cm[1][1]/(cm[1][0]+cm[1][1])) / 5))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nOur criterion give a result of 0.9170704904644433\n```\n:::\n:::\n\n\nOur criterion give a result of 0.912995958898\n\n::: {.cell execution_count=26}\n``` {.python .cell-code}\nprint('We have detected ' + str(cm[1][1]) + ' frauds / ' + str(cm[1][1]+cm[1][0]) + ' total frauds.')\nprint('\\nSo, the probability to detect a fraud is ' + str(cm[1][1]/(cm[1][1]+cm[1][0])))\nprint(\"the accuracy is : \"+str((cm[0][0]+cm[1][1]) / (sum(cm[0]) + sum(cm[1]))))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nWe have detected 181 frauds / 199 total frauds.\n\nSo, the probability to detect a fraud is 0.9095477386934674\nthe accuracy is : 0.9471614975483469\n```\n:::\n:::\n\n\nWe have detected 179 frauds / 199 total frauds.\n\nSo, the probability to detect a fraud is 0.899497487437\nthe accuracy is : 0.966989844741\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}