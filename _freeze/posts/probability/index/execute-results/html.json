{
  "hash": "dd8acf89b2dc208bde5b17b91bfb5d66",
  "result": {
    "markdown": "---\ntitle: Probability and Random Variables\nauthor: Rahul Pulluri\ndate: '2023-11-24'\nimage: thelanguageofprobability-140608080917-phpapp02-thumbnail.webp\n---\n\n![](thelanguageofprobability-140608080917-phpapp02-thumbnail.webp)\n\n## Definition\n\nProbability theory is the mathematical study of uncertainty. It plays a central role in machine learning, as the design of learning algorithms often relies on probabilistic assumption of the data. This set of notes attempts to cover some basic probability theory that serves as a background for the class.\n\n## Probability Space\n\nWhen we speak about probability, we often refer to the probability of an event of uncertain nature taking place. For example, we speak about the probability of rain next Tuesday. Therefore, in order to discuss probability theory formally, we must first clarify what the\npossible events are to which we would like to attach probability.\n\nFormally, a probability space is defined by the triple (Ω, F, P), where\n\n- • Ω is the space of possible outcomes (or outcome space),\n\n- • F ⊆ 2^Ω (the power set of Ω) is the space of (measurable) events (or event space),\n\n- • P is the probability measure (or probability distribution) that maps an event E ∈ F to a real value between 0 and 1 (think of P as a function).\n\nGiven the outcome space Ω, there is some restrictions as to what subset of 2^Ω can be considered an event space F:\n\n- • The trivial event Ω and the empty event ∅ is in F.\n\n- • The event space F is closed under (countable) union, i.e., if α, β ∈ F, then α ∪ β ∈ F.\n\n- • The even space F is closed under complement, i.e., if α ∈ F, then (Ω \\ α) ∈ F.\n\n**Example 1.** Suppose we throw a (six-sided) dice. The space of possible outcomes Ω = {1, 2, 3, 4, 5, 6}. We may decide that the events of interest is whether the dice throw is odd\nor even. This event space will be given by F = {∅, {1, 3, 5}, {2, 4, 6}, Ω}.\n\nNote that when the outcome space Ω is finite, as in the previous example, we often take the event space F to be 2Ω. This treatment is not fully general, but it is often sufficient for practical purposes. However, when the outcome space is infinite, we must be careful to define what the event space is.\n\nGiven an event space F, the probability measure P must satisfy certain axioms.\n\n• (non-negativity) For all α ∈ F, P(α) ≥ 0.\n\n• (trivial event) P(Ω) = 1.\n\n• (additivity) For all α, β ∈ F and α ∩ β = ∅, P(α ∪ β) = P(α) + P(β).\n\n**Example 2.** Returning to our dice example, suppose we now take the event space F to be\n2Ω. Further, we define a probability distribution P over F such that\n\nP({1}) = P({2}) = · · · = P({6}) = 1/6\n\nthen this distribution P completely specifies the probability of any given event happening (through the additivity axiom). For example, the probability of an even dice throw will be\n\nP({2, 4, 6}) = P({2}) + P({4}) + P({6}) = 1/6 + 1/6 + 1/6 = 1/2 \n\nsince each of these events are disjoint.\n\n\n## Random Variables\n\nRandom variables play an important role in probability theory. The most important fact about random variables is that they are not variables. They are actually functions that\nmap outcomes (in the outcome space) to real values. In terms of notation, we usually denote random variables by a capital letter. Let’s see an example.\n\n**Example 3**. Again, consider the process of throwing a dice. Let X be a random variable that depends on the outcome of the throw. A natural choice for X would be to map the outcome i to the value i, i.e., mapping the event of throwing an “one” to the value of 1. Note that we could have chosen some strange mappings too. For example, we could have a random variable Y that maps all outcomes to 0, which would be a very boring function, or a random variable Z that maps the outcome i to the value of 2^i if i is odd and the value of −i if i is even, which would be quite strange indeed.\n\nIn a sense, random variables allow us to abstract away from the formal notion of event space, as we can define random variables that capture the appropriate events. For example,\nconsider the event space of odd or even dice throw in Example 1. We could have defined a random variable that takes on value 1 if outcome i is odd and 0 otherwise. These type of binary random variables are very common in practice, and are known as indicator variables, taking its name from its use to indicate whether a certain event has happened. So why did we introduce event space? That is because when one studies probability theory (more 2 rigorously) using measure theory, the distinction between outcome space and event space\nwill be very important. This topic is too advanced to be covered in this short review note.In any case, it is good to keep in mind that event space is not always simply the power set of the outcome space.\n\nFrom here onwards, we will talk mostly about probability with respect to random variables. While some probability concepts can be defined meaningfully without using them,\nrandom variables allow us to provide a more uniform treatment of probability theory. For notations, the probability of a random variable X taking on the value of a will be denoted by either\n\n                     P(X = a) or Px(a)\n\nWe will also denote the range of a random variable X by V al(X).\n\n\n## Probability Distribution\n\nProbability distributions are fundamental to understanding random variables in statistics and probability theory. They provide a systematic way to describe the likelihood of different outcomes from a random process. Let's explore this concept in detail for both discrete and continuous random variables:\n\n## Probability Distributions for Discrete Random Variables\n\n### **Probability Mass Function (PMF):**\n\nThe PMF is a function that gives the probability that a discrete random variable is exactly equal to some value.\nIt satisfies two conditions: The sum of probabilities for all possible outcomes equals 1, and the probability for each individual outcome is between 0 and 1.\n\n**Key Discrete Distributions:**\n\n**Binomial Distribution:** Models the number of successes in a fixed number of independent Bernoulli trials (like flipping a coin several times). It is characterized by two parameters: the number of trials (n) and the probability of success (p) in each trial.\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nfrom scipy.stats import binom\n\n# Define parameters\nn = 10  # Number of trials\np = 0.5  # Probability of success in each trial\n\n# Generate binomial random variables\nrv = binom(n, p)\n\n# Probability Mass Function (PMF)\nx = np.arange(0, n+1)\npmf = rv.pmf(x)\n\n# Plot the PMF\nimport matplotlib.pyplot as plt\nplt.bar(x, pmf)\nplt.xlabel('Number of Successes')\nplt.ylabel('Probability')\nplt.title('Binomial Distribution PMF')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-2-output-1.png){width=597 height=449}\n:::\n:::\n\n\n**Poisson Distribution:** Used for counting the number of events that occur in a fixed interval of time or space. It is characterized by its rate parameter (λ), which is the average number of events in the given interval.\n\n::: {.cell execution_count=2}\n``` {.python .cell-code}\nimport numpy as np\nfrom scipy.stats import poisson\n\n# Define rate parameter (average events in the interval)\nlambda_ = 3.0\n\n# Generate Poisson random variables\nrv = poisson(mu=lambda_)\n\n# Probability Mass Function (PMF)\nx = np.arange(0, 11)  # Example: Counting events from 0 to 10\npmf = rv.pmf(x)\n\n# Plot the PMF\nimport matplotlib.pyplot as plt\nplt.bar(x, pmf)\nplt.xlabel('Number of Events')\nplt.ylabel('Probability')\nplt.title('Poisson Distribution PMF')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-3-output-1.png){width=597 height=449}\n:::\n:::\n\n\n**Geometric Distribution:** Describes the number of Bernoulli trials needed to get one success. Its key parameter is the probability of success (p) in each trial.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nfrom scipy.stats import geom\n\n# Define probability of success in each trial\np = 0.2\n\n# Generate Geometric random variables\nrv = geom(p)\n\n# Probability Mass Function (PMF)\nx = np.arange(1, 11)  # Number of trials needed (1 to 10)\npmf = rv.pmf(x)\n\n# Plot the PMF\nimport matplotlib.pyplot as plt\nplt.bar(x, pmf)\nplt.xlabel('Number of Trials Needed')\nplt.ylabel('Probability')\nplt.title('Geometric Distribution PMF')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-4-output-1.png){width=606 height=449}\n:::\n:::\n\n\n## Probability Distributions for Continuous Random Variables\n\n### **Probability Density Function (PDF):\n\nThe PDF is used to specify the probability of the random variable falling within a particular range of values, as opposed to taking on any one specific value. The probability for any single point is zero for a continuous distribution. Instead, the area under the PDF curve within a range of values indicates the probability of falling within that range.\n\n**Key Continuous Distributions:**\n\n**Normal (Gaussian) Distribution**: One of the most important probability distributions, known for its bell-shaped curve. It is characterized by two parameters: the mean (μ), which indicates the center of the distribution, and the standard deviation (σ), which measures the spread.\n\n::: {.cell execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\nfrom scipy.stats import norm\n\n# Define parameters\nmu = 0  # Mean\nsigma = 1  # Standard Deviation\n\n# Generate normal random variables\nrv = norm(loc=mu, scale=sigma)\n\n# Probability Density Function (PDF)\nx = np.linspace(-3, 3, 100)  # Range of values\npdf = rv.pdf(x)\n\n# Plot the PDF\nimport matplotlib.pyplot as plt\nplt.plot(x, pdf)\nplt.xlabel('Value')\nplt.ylabel('Probability Density')\nplt.title('Normal Distribution PDF')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-5-output-1.png){width=597 height=449}\n:::\n:::\n\n\n**Exponential Distribution:** Used to model the time elapsed between events in a process with a constant average rate (e.g., radioactive decay). It is characterized by its rate parameter (λ).\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nimport numpy as np\nfrom scipy.stats import expon\n\n# Define rate parameter (λ)\nlambda_ = 0.5\n\n# Generate Exponential random variables\nrv = expon(scale=1/lambda_)\n\n# Probability Density Function (PDF)\nx = np.linspace(0, 10, 100)  # Range of values\npdf = rv.pdf(x)\n\n# Plot the PDF\nimport matplotlib.pyplot as plt\nplt.plot(x, pdf)\nplt.xlabel('Time')\nplt.ylabel('Probability Density')\nplt.title('Exponential Distribution PDF')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-6-output-1.png){width=589 height=449}\n:::\n:::\n\n\n**Uniform Distribution:** Describes a situation where all outcomes are equally likely. In its continuous form, it’s defined by two parameters, a and b, which are the minimum and maximum values of the distribution.\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\nimport numpy as np\nfrom scipy.stats import uniform\n\n# Define parameters\na = 0  # Minimum value\nb = 1  # Maximum value\n\n# Generate Uniform random variables\nrv = uniform(loc=a, scale=b-a)\n\n# Probability Density Function (PDF)\nx = np.linspace(a-0.1, b+0.1, 100)  # Range of values\npdf = rv.pdf(x)\n\n# Plot the PDF\nimport matplotlib.pyplot as plt\nplt.plot(x, pdf)\nplt.xlabel('Value')\nplt.ylabel('Probability Density')\nplt.title('Uniform Distribution PDF')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](index_files/figure-html/cell-7-output-1.png){width=589 height=449}\n:::\n:::\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}