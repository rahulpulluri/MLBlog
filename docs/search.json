[
  {
    "objectID": "index.html#blogs-related-to-machine-learning-concept",
    "href": "index.html#blogs-related-to-machine-learning-concept",
    "title": "Welcome to the Age of Machine Learning",
    "section": "Blogs Related to Machine Learning Concept",
    "text": "Blogs Related to Machine Learning Concept\n\n\n\n\n  \n\n\n\n\nLinear Regression\n\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2023\n\n\nRahul Pulluri\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nProbability and Random Variables\n\n\n\n\n\n\n\n\n\n\n\n\nNov 24, 2023\n\n\nRahul Pulluri\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly/Outlier detection\n\n\n\n\n\n\n\n\n\n\n\n\nNov 24, 2023\n\n\nRahul Pulluri\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nRahul Pulluri\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nNonlinear Regression\n\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nRahul Pulluri\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2023\n\n\nRahul Pulluri\n\n\n12 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/classification/index.html",
    "href": "posts/classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "(Classification_main.jpeg)"
  },
  {
    "objectID": "posts/classification/index.html#introduction-to-classification",
    "href": "posts/classification/index.html#introduction-to-classification",
    "title": "Classification",
    "section": "Introduction to Classification",
    "text": "Introduction to Classification\nIn the dynamic world of machine learning, classification stands out as a pivotal concept, providing the ability to categorize and interpret data for a wide array of applications. Whether you’re looking to filter spam emails, identify diseases from medical data, or recognize handwritten digits, classification algorithms are your go-to tools. In this blog post, we’ll delve into what classification is in machine learning, its significance, and how it’s transforming industries."
  },
  {
    "objectID": "posts/classification/index.html#defining-classification-in-machine-learning",
    "href": "posts/classification/index.html#defining-classification-in-machine-learning",
    "title": "Classification",
    "section": "Defining Classification in Machine Learning",
    "text": "Defining Classification in Machine Learning\nAt its core, classification is the process of recognizing, understanding, and grouping data into predefined categories or subgroups. It’s like having a smart assistant that can look at a new piece of data and tell you which group it belongs to based on its characteristics. This task is accomplished through the analysis of historical data, which serves as a training ground for machine learning models."
  },
  {
    "objectID": "posts/classification/index.html#the-power-of-predefined-categories",
    "href": "posts/classification/index.html#the-power-of-predefined-categories",
    "title": "Classification",
    "section": "The Power of Predefined Categories",
    "text": "The Power of Predefined Categories\nThe magic of classification lies in these predefined categories. Think of them as labels, such as “spam” and “non-spam” for emails, “fraudulent” and “legitimate” for financial transactions, or “cat” and “dog” for image recognition. The ability to organize data into these categories enables decision-making, automation, and insights that would otherwise be impractical or impossible to achieve manually."
  },
  {
    "objectID": "posts/classification/index.html#how-classification-works",
    "href": "posts/classification/index.html#how-classification-works",
    "title": "Classification",
    "section": "How Classification Works",
    "text": "How Classification Works\nTraining Phase: The algorithm is trained on a labeled dataset, where it learns the relationship between features and the corresponding class labels.\nModel Building: The algorithm creates a model that maps inputs (features) to desired outputs (labels). This model represents the learned patterns from the data.\nTesting and Prediction: The trained model is then used to predict the class labels of new, unseen data. The performance of the model is typically evaluated using metrics like accuracy, precision, recall, and F1 score."
  },
  {
    "objectID": "posts/classification/index.html#real-life-applications",
    "href": "posts/classification/index.html#real-life-applications",
    "title": "Classification",
    "section": "Real-Life Applications",
    "text": "Real-Life Applications\nClassification has found its way into countless real-world applications. From medical diagnoses to recommendation systems, here are a few examples:\n\nMedical Diagnoses: Doctors use machine learning models to predict whether a patient has a particular disease based on symptoms, medical history, and test results.\nRecommendation Systems: Companies like Netflix and Amazon employ classification to recommend movies or products to users based on their preferences and behavior.\nSentiment Analysis: Social media platforms analyze posts to classify them as positive, negative, or neutral, providing valuable insights into public opinion.\nImage Recognition: In the field of computer vision, classification helps identify objects, animals, or handwritten text in images.\n\n\nPopular Classification Algorithms:\n\nLogistic Regression: Logistic regression is a widely used classification algorithm that models the probability of an input belonging to a particular category. It’s simple, interpretable, and effective for binary and multiclass classification tasks.\nNaive Bayes: Naive Bayes is a probabilistic classification algorithm based on Bayes’ theorem. It’s particularly suited for text classification tasks and spam email filtering, where it assumes independence between features.\nK-Nearest Neighbors: K-NN is a straightforward yet powerful algorithm that classifies data points based on the majority class among their k-nearest neighbors. It’s versatile and can be applied to various types of data, but the choice of k is crucial for its performance.\nDecision Tree: Decision tree classifiers make decisions by splitting data based on features, creating a tree-like structure of decisions. They are interpretable and can handle both categorical and numerical data, making them useful in many applications.\nSupport Vector Machines: SVMs are effective for both linear and nonlinear classification tasks. They work by finding the optimal hyperplane that maximizes the margin between classes, making them robust against overfitting and suitable for high-dimensional data."
  },
  {
    "objectID": "posts/classification/index.html#example-heart-disease-prediction",
    "href": "posts/classification/index.html#example-heart-disease-prediction",
    "title": "Classification",
    "section": "Example: Heart Disease Prediction",
    "text": "Example: Heart Disease Prediction\n\n#Importing librairies\n\nimport pandas as pd \nimport numpy as np\n\n# Scikit-learn library: For SVM\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import svm\n\nimport itertools\n\n# Matplotlib library to plot the charts\nimport matplotlib.pyplot as plt\nimport matplotlib.mlab as mlab\n\n# Library for the statistic data vizualisation\nimport seaborn\n\n%matplotlib inline\n\nData recuperation\n\ndata = pd.read_csv('R:/Blog/posts/classification/creditcard.csv') # Reading the file .csv\ndf = pd.DataFrame(data) # Converting data to Panda DataFrame\n\nData Visualization\n\ndf = pd.DataFrame(data) # Converting data to Panda DataFrame\n\n\ndf.describe() # Description of statistic features (Sum, Average, Variance, minimum, 1st quartile, 2nd quartile, 3rd Quartile and Maximum)\n\n\n\n\n\n\n\n\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\n...\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\nClass\n\n\n\n\ncount\n284807.000000\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n...\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n284807.000000\n284807.000000\n\n\nmean\n94813.859575\n1.168375e-15\n3.416908e-16\n-1.379537e-15\n2.074095e-15\n9.604066e-16\n1.487313e-15\n-5.556467e-16\n1.213481e-16\n-2.406331e-15\n...\n1.654067e-16\n-3.568593e-16\n2.578648e-16\n4.473266e-15\n5.340915e-16\n1.683437e-15\n-3.660091e-16\n-1.227390e-16\n88.349619\n0.001727\n\n\nstd\n47488.145955\n1.958696e+00\n1.651309e+00\n1.516255e+00\n1.415869e+00\n1.380247e+00\n1.332271e+00\n1.237094e+00\n1.194353e+00\n1.098632e+00\n...\n7.345240e-01\n7.257016e-01\n6.244603e-01\n6.056471e-01\n5.212781e-01\n4.822270e-01\n4.036325e-01\n3.300833e-01\n250.120109\n0.041527\n\n\nmin\n0.000000\n-5.640751e+01\n-7.271573e+01\n-4.832559e+01\n-5.683171e+00\n-1.137433e+02\n-2.616051e+01\n-4.355724e+01\n-7.321672e+01\n-1.343407e+01\n...\n-3.483038e+01\n-1.093314e+01\n-4.480774e+01\n-2.836627e+00\n-1.029540e+01\n-2.604551e+00\n-2.256568e+01\n-1.543008e+01\n0.000000\n0.000000\n\n\n25%\n54201.500000\n-9.203734e-01\n-5.985499e-01\n-8.903648e-01\n-8.486401e-01\n-6.915971e-01\n-7.682956e-01\n-5.540759e-01\n-2.086297e-01\n-6.430976e-01\n...\n-2.283949e-01\n-5.423504e-01\n-1.618463e-01\n-3.545861e-01\n-3.171451e-01\n-3.269839e-01\n-7.083953e-02\n-5.295979e-02\n5.600000\n0.000000\n\n\n50%\n84692.000000\n1.810880e-02\n6.548556e-02\n1.798463e-01\n-1.984653e-02\n-5.433583e-02\n-2.741871e-01\n4.010308e-02\n2.235804e-02\n-5.142873e-02\n...\n-2.945017e-02\n6.781943e-03\n-1.119293e-02\n4.097606e-02\n1.659350e-02\n-5.213911e-02\n1.342146e-03\n1.124383e-02\n22.000000\n0.000000\n\n\n75%\n139320.500000\n1.315642e+00\n8.037239e-01\n1.027196e+00\n7.433413e-01\n6.119264e-01\n3.985649e-01\n5.704361e-01\n3.273459e-01\n5.971390e-01\n...\n1.863772e-01\n5.285536e-01\n1.476421e-01\n4.395266e-01\n3.507156e-01\n2.409522e-01\n9.104512e-02\n7.827995e-02\n77.165000\n0.000000\n\n\nmax\n172792.000000\n2.454930e+00\n2.205773e+01\n9.382558e+00\n1.687534e+01\n3.480167e+01\n7.330163e+01\n1.205895e+02\n2.000721e+01\n1.559499e+01\n...\n2.720284e+01\n1.050309e+01\n2.252841e+01\n4.584549e+00\n7.519589e+00\n3.517346e+00\n3.161220e+01\n3.384781e+01\n25691.160000\n1.000000\n\n\n\n\n8 rows × 31 columns\n\n\n\n\ndf_fraud = df[df['Class'] == 1] # Recovery of fraud data\nplt.figure(figsize=(15,10))\nplt.scatter(df_fraud['Time'], df_fraud['Amount']) # Display fraud amounts according to their time\nplt.title('Scratter plot amount fraud')\nplt.xlabel('Time')\nplt.ylabel('Amount')\nplt.xlim([0,175000])\nplt.ylim([0,2500])\nplt.show()\n\n\n\n\n\nnb_big_fraud = df_fraud[df_fraud['Amount'] &gt; 1000].shape[0] # Recovery of frauds over 1000\nprint('There are only '+ str(nb_big_fraud) + ' frauds where the amount was bigger than 1000 over ' + str(df_fraud.shape[0]) + ' frauds')\n\nThere are only 9 frauds where the amount was bigger than 1000 over 492 frauds\n\n\nThere are only 9 frauds where the amount was bigger than 1000 over 492 frauds\n\nUnbalanced data\n\n\nnumber_fraud = len(data[data.Class == 1])\nnumber_no_fraud = len(data[data.Class == 0])\nprint('There are only '+ str(number_fraud) + ' frauds in the original dataset, even though there are ' + str(number_no_fraud) +' no frauds in the dataset.')\n\nThere are only 492 frauds in the original dataset, even though there are 284315 no frauds in the dataset.\n\n\nThere are only 492 frauds in the original dataset, even though there are 284315 no frauds in the dataset.\n\nprint(\"The accuracy of the classifier then would be : \"+ str((284315-492)/284315)+ \" which is the number of good classification over the number of tuple to classify\")\n\nThe accuracy of the classifier then would be : 0.998269524998681 which is the number of good classification over the number of tuple to classify\n\n\nCorrelation of features\n\ndf_corr = df.corr() # Calculation of the correlation coefficients in pairs, with the default method:\n                    # Pearson, Standard Correlation Coefficient\n\n\nplt.figure(figsize=(15,10))\nseaborn.heatmap(df_corr, cmap=\"YlGnBu\") # Displaying the Heatmap\nseaborn.set(font_scale=2,style='white')\n\nplt.title('Heatmap correlation')\nplt.show()\n\n\n\n\nAs we can notice, most of the features are not correlated with each other. This corroborates the fact that a PCA was previously performed on the data.\nWhat can generally be done on a massive dataset is a dimension reduction. By picking th emost important dimensions, there is a possiblity of explaining most of the problem, thus gaining a considerable amount of time while preventing the accuracy to drop too much.\nHowever in this case given the fact that a PCA was previously performed, if the dimension reduction is effective then the PCA wasn’t computed in the most effective way. Another way to put it is that no dimension reduction should be computed on a dataset on which a PCA was computed correctly.\nCorrelation of features\nOVERSAMPLING\nOne way to do oversampling is to replicate the under-represented class tuples until we attain a correct proportion between the class\nHowever as we haven’t infinite time nor the patience, we are going to run the classifier with the undersampled training data (for those using the undersampling principle if results are really bad just rerun the training dataset definition)\nUNDERSAMPLING\n\nimport pandas as pd\n# We seperate ours data in two groups : a train dataset and a test dataset\n\n# First we build our train dataset\ndf_train_all = df[0:150000] # We cut in two the original dataset\ndf_train_1 = df_train_all[df_train_all['Class'] == 1] # We seperate the data which are the frauds and the no frauds\ndf_train_0 = df_train_all[df_train_all['Class'] == 0]\nprint('In this dataset, we have ' + str(len(df_train_1)) +\" frauds so we need to take a similar number of non-fraud\")\n\ndf_sample=df_train_0.sample(300)\ncombined_df = pd.concat([df_train_1, df_sample], ignore_index=True) # We gather the frauds with the no frauds. \ncombined_df = combined_df.sample(frac=1) # Then we mix our dataset\n\nIn this dataset, we have 293 frauds so we need to take a similar number of non-fraud\n\n\nIn this dataset, we have 293 frauds so we need to take a similar number of non-fraud\n\nX_train = combined_df.drop(['Time', 'Class'], axis=1)  # Drop the features \"Time\" (useless) and \"Class\" (label)\ny_train = combined_df['Class']  # Create the label\nX_train = np.asarray(X_train)\ny_train = np.asarray(y_train)\n\n\n############################## with all the test dataset to see if the model learn correctly ##################\ndf_test_all = df[150000:]\n\nX_test_all = df_test_all.drop(['Time', 'Class'],axis=1)\ny_test_all = df_test_all['Class']\nX_test_all = np.asarray(X_test_all)\n\nConfusion Matrix\n\nclass_names=np.array(['0','1']) # Binary label, Class = 1 (fraud) and Class = 0 (no fraud)\n\n\n# Function to plot the confusion Matrix\ndef plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = 'd' \n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] &gt; thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\nModel Selection\nSo now, we’ll use a SVM model classifier, with the scikit-learn library.\n\nclassifier = svm.SVC(kernel='linear') # We set a SVM classifier, the default SVM Classifier (Kernel = Radial Basis Function)\n\n\nclassifier.fit(X_train, y_train) # Then we train our model, with our balanced data train.\n\nSVC(kernel='linear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(kernel='linear')\n\n\nTesting the model\n\nprediction_SVM_all = classifier.predict(X_test_all) #And finally, we predict our data test.\n\n\ncm = confusion_matrix(y_test_all, prediction_SVM_all)\nplot_confusion_matrix(cm,class_names)\n\n\n\n\nIn this case we are gonna try to minimize the number of errors in our prediction results. Errors are on the anti-diagonal of the confusion matrix. But we can infer that being wrong about an actual fraud is far worse than being wrong about a non-fraud transaction.\nThat is why using the accuracy as only classification criterion could be considered unthoughtful. During the remaining part of this study our criterion will consider precision on the real fraud 4 times more important than the general accuracy. Even though the final tested result is accuracy.\n\nprint('Our criterion give a result of ' \n      + str( ( (cm[0][0]+cm[1][1]) / (sum(cm[0]) + sum(cm[1])) + 4 * cm[1][1]/(cm[1][0]+cm[1][1])) / 5))\n\nOur criterion give a result of 0.9267674132156849\n\n\nOur criterion give a result of 0.905383035408\n\nprint('We have detected ' + str(cm[1][1]) + ' frauds / ' + str(cm[1][1]+cm[1][0]) + ' total frauds.')\nprint('\\nSo, the probability to detect a fraud is ' + str(cm[1][1]/(cm[1][1]+cm[1][0])))\nprint(\"the accuracy is : \"+str((cm[0][0]+cm[1][1]) / (sum(cm[0]) + sum(cm[1]))))\n\nWe have detected 184 frauds / 199 total frauds.\n\nSo, the probability to detect a fraud is 0.9246231155778895\nthe accuracy is : 0.9353446037668667\n\n\nWe have detected 177 frauds / 199 total frauds.\nSo, the probability to detect a fraud is 0.889447236181 the accuracy is : 0.969126232317\nModels Rank\nThere is a need to compute the fit method again, as the dimension of the tuples to predict went from 29 to 10 because of the dimension reduction\n\n# Define a simple feature ranking function (you can customize this)\ndef rank_features(data):\n    # Implement your feature ranking logic here\n    ranked_features = data  # Replace this with your actual feature ranking code\n    return ranked_features\n\n# Now you can use the rank_features function\nX_train_rank = rank_features(X_train)\nX_test_all_rank = rank_features(X_test_all)\n\n\nprediction_SVM = classifier.predict(X_test_all_rank)\n\n\ncm = confusion_matrix(y_test_all, prediction_SVM)\nplot_confusion_matrix(cm,class_names)\n\n\n\n\n\nprint('Our criterion give a result of ' \n      + str( ( (cm[0][0]+cm[1][1]) / (sum(cm[0]) + sum(cm[1])) + 4 * cm[1][1]/(cm[1][0]+cm[1][1])) / 5))\n\nOur criterion give a result of 0.9267674132156849\n\n\nOur criterion give a result of 0.912995958898\n\nprint('We have detected ' + str(cm[1][1]) + ' frauds / ' + str(cm[1][1]+cm[1][0]) + ' total frauds.')\nprint('\\nSo, the probability to detect a fraud is ' + str(cm[1][1]/(cm[1][1]+cm[1][0])))\nprint(\"the accuracy is : \"+str((cm[0][0]+cm[1][1]) / (sum(cm[0]) + sum(cm[1]))))\n\nWe have detected 184 frauds / 199 total frauds.\n\nSo, the probability to detect a fraud is 0.9246231155778895\nthe accuracy is : 0.9353446037668667\n\n\nWe have detected 179 frauds / 199 total frauds.\nSo, the probability to detect a fraud is 0.899497487437 the accuracy is : 0.966989844741"
  },
  {
    "objectID": "posts/probability/index.html",
    "href": "posts/probability/index.html",
    "title": "Probability and Random Variables",
    "section": "",
    "text": "Contents:"
  },
  {
    "objectID": "posts/probability/index.html#random-variable",
    "href": "posts/probability/index.html#random-variable",
    "title": "Probability and Random Variables",
    "section": "Random Variable",
    "text": "Random Variable\nA random variable serves as a mathematical link between various numerical values and the outcomes of an experiment. It assigns distinct values to different results of the experiment, making it variable. As each outcome holds an element of uncertainty, the variable is considered random.\nIn probability, a common representation for a random variable is using a capital letter (e.g., X), and a specific value is denoted by a corresponding lowercase letter (e.g., x). For instance, when flipping a coin twice, the potential outcomes form the sample space:\nS = {HH, HT, TH, TT}\nFrom this sample space, the random variable X can be defined as:\nX = {HH, HT, TH, TT}\nRandom variables are categorized into two types: continuous and discrete random variables.\nDiscrete random variables are limited to specific, whole-number values and do not include fractions or decimals. They generate discrete probability distributions.\nExamples include the total outcome of rolling two dice (ranging from 2 to 12) or the count of desktops sold (starting from 0 and increasing by whole numbers).\nIn application, discrete random variables describe distinct quantities, such as the number of aircraft taking off and landing at an airport at a given time or the specific count of communication lines activated in an organization’s system.\nOn the other hand, continuous random variables encompass a broad range of values, including decimals and fractions. They give rise to continuous probability distributions.\nExamples of continuous random variables involve quantities like interest rates (such as 4.55% or 7.9%) or durations like task completion times.\nIn practical scenarios, continuous random variables are employed to represent measurements like reaction temperature errors or the probability of a construction project completing within a specific timeframe, such as between 20 and 24 months with a probability of 0.5."
  },
  {
    "objectID": "posts/probability/index.html#probability",
    "href": "posts/probability/index.html#probability",
    "title": "Probability and Random Variables",
    "section": "Probability",
    "text": "Probability\nProbability quantifies the likelihood of an event occurring, expressed as percentages or descriptive terms like “impossible” or “probable.” It encompasses the chances of various outcomes, such as rolling a specific number on a die. For instance, the probability of rolling a “4” on a six-sided die is 1 out of 6, considering there’s only one favorable outcome among six possibilities."
  },
  {
    "objectID": "posts/probability/index.html#why-probability",
    "href": "posts/probability/index.html#why-probability",
    "title": "Probability and Random Variables",
    "section": "Why Probability?",
    "text": "Why Probability?\nIn the realm of machine learning, uncertainty and stochastic elements often emerge due to incomplete observability, leading us to work primarily with sampled data.\nConsider a scenario where we aim to make reliable inferences about the behavior of a random variable, despite having access only to limited data, leaving the entire population characteristics unknown.\n\n\n\nEstimating the data-generating process (src: https://towardsdatascience.com/understanding-random-variables-and-probability-distributions-1ed1daf2e66)\n\n\nTherefore, we require a method to extrapolate from the sampled data to represent the entire population or, in simpler terms, estimate the true process generating the data. Understanding the probability distribution becomes crucial as it allows us to gauge the likelihood of specific outcomes while accommodating the variability observed in the results. This comprehension enables us to extend conclusions from the sample to the broader population, approximate the function generating the data, and enhance the accuracy of predicting the behavior of a random variable.\n\nDiscrete Probability Distribution\nDiscrete probability distributions stem from discrete data and aim to model predictions or outcomes, such as pricing options or forecasting market shocks. These distributions illustrate the possible values of a discrete random variable along with their corresponding probabilities. Examples include the Bernoulli, geometric, and binomial distributions. Here are some common examples of discrete probability distributions:\n\nBernoulli Distribution: Models a single trial with two outcomes, often used in success/failure experiments.\nBinomial Distribution: Describes the number of successes in a fixed number of independent trials with a constant probability of success.\nPoisson Distribution: Models the number of events occurring in a fixed interval of time or space, given a known average rate of occurrence.\n\n\n\nContinuous Probability Distribution\nIn contrast, continuous probability distributions cover an infinite range of values, making them uncountable, like time extending from 0 seconds indefinitely. Examples involve continuous measurements like annual rainfall in a city or the weight of newborn babies, where the range of values is limitless and not countable in a finite manner. Here are some common examples of continuous probability distributions:\n\nNormal (Gaussian) Distribution: Symmetric bell-shaped curve that describes many natural phenomena like heights or test scores.\nUniform Distribution: All values in an interval have equal probability, forming a rectangle in the probability density function.\nExponential Distribution: Models the time between events in a Poisson process, such as the time between phone calls at a call center.\n\n\nExample of discrete probability distribution\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load the Titanic dataset\ntitanic_data = sns.load_dataset('titanic')\n\n# Filter data for relevant columns (class and survival)\nsurvival_data = titanic_data[['pclass', 'survived']]\n\n# Calculate survival probabilities based on passenger class\nsurvival_probabilities = survival_data.groupby('pclass')['survived'].mean()\n\n# Plotting the probability distribution\nplt.figure(figsize=(8, 6))\nsurvival_probabilities.plot(kind='bar', color='skyblue')\nplt.title('Survival Probability by Passenger Class')\nplt.xlabel('Passenger Class')\nplt.ylabel('Survival Probability')\nplt.xticks(rotation=0)\nplt.ylim(0, 1)  # Setting y-axis limits to probability range (0 to 1)\nplt.show()\n\n\n\n\n\n\nExample of continuous probability distribution\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom scipy.stats import norm, expon\n\n# Load breast cancer dataset\ndata = load_breast_cancer()\nX = data.data\ny = data.target\n\n# Consider a specific feature for demonstration (e.g., mean radius)\nfeature_index = 0\nselected_feature = X[:, feature_index]\n\n# Summary statistics and visualization\nmean_value = np.mean(selected_feature)\nstd_dev = np.std(selected_feature)\n\nprint(f\"Mean: {mean_value}, Standard Deviation: {std_dev}\")\n\n# Plotting the histogram of the selected feature\nplt.figure(figsize=(8, 6))\nplt.hist(selected_feature, bins=30, density=True, alpha=0.6, color='blue')\n\n# Plot Gaussian distribution based on the observed mean and standard deviation\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mean_value, std_dev)\nplt.plot(x, p, 'k', linewidth=2, label='Gaussian PDF')\nplt.title('Histogram and Gaussian PDF for Mean Radius')\nplt.xlabel('Mean Radius')\nplt.ylabel('Frequency/Probability')\nplt.legend()\nplt.show()\n\n# Plotting an exponential distribution\nplt.figure(figsize=(8, 6))\n\n# Generate random data following an exponential distribution with the observed mean\nexponential_data = np.random.exponential(scale=mean_value, size=1000)\n\n# Plot histogram\nplt.hist(exponential_data, bins=30, density=True, alpha=0.6, color='green')\n\n# Plot exponential distribution PDF\nx_exp = np.linspace(0, np.max(exponential_data), 100)\np_exp = expon.pdf(x_exp, scale=mean_value)\nplt.plot(x_exp, p_exp, 'k', linewidth=2, label='Exponential PDF')\nplt.title('Histogram and Exponential PDF for Mean Radius')\nplt.xlabel('Mean Radius')\nplt.ylabel('Frequency/Probability')\nplt.legend()\nplt.show()\n\nMean: 14.127291739894552, Standard Deviation: 3.520950760711062\n\n\n\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_breast_cancer\n\n# Load the breast cancer dataset\ndata = load_breast_cancer()\ndf = pd.DataFrame(data.data, columns=data.feature_names)\n\n# Select a few features for analysis (e.g., mean radius, mean texture, mean perimeter)\nselected_features = ['mean radius', 'mean texture', 'mean perimeter']\n\n# Descriptive statistics\nprint(df[selected_features].describe())\n\n# Visualize the data using pair plots\nsns.pairplot(df[selected_features])\nplt.show()\n\n# Create probability distribution plots for selected features\nplt.figure(figsize=(15, 5))\nfor i, feature in enumerate(selected_features, 1):\n    plt.subplot(1, 3, i)\n    sns.histplot(df[feature], kde=True, stat=\"probability\", bins=20)\n    plt.title(f'Probability Distribution of {feature}')\n    plt.xlabel(feature)\n    plt.ylabel('Probability')\nplt.tight_layout()\nplt.show()\n\n       mean radius  mean texture  mean perimeter\ncount   569.000000    569.000000      569.000000\nmean     14.127292     19.289649       91.969033\nstd       3.524049      4.301036       24.298981\nmin       6.981000      9.710000       43.790000\n25%      11.700000     16.170000       75.170000\n50%      13.370000     18.840000       86.240000\n75%      15.780000     21.800000      104.100000\nmax      28.110000     39.280000      188.500000"
  },
  {
    "objectID": "posts/outlier-detection/index.html",
    "href": "posts/outlier-detection/index.html",
    "title": "Anomaly/Outlier detection",
    "section": "",
    "text": "Image Source: https://miro.medium.com/v2/resize:fit:720/format:webp/1*BaOiTGWAoFi-3_-E-rJGdg.jpeg"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Rahul Pulluri",
    "section": "",
    "text": "I am Rahul Pulluri and I am from India.\nPursuing Masters in Computer Science at Virginia Tech. As a Master of Engineering student in Computer Science at Virginia Tech, I am deeply immersed in the transformative world of Machine Learning (ML). My academic focus is on unraveling the complexities of ML algorithms and techniques, understanding their theoretical foundations, and exploring their vast potential in practical applications. This journey is driven by a fascination with how ML can revolutionize data analysis and decision-making across various sectors. My studies involve a thorough exploration of the principles behind these advanced algorithms, enabling me to grasp how they can be effectively applied to real-world scenarios.\nThe MEng program at Virginia Tech is instrumental in shaping my expertise in ML. It provides a balanced mix of rigorous academic learning and practical, hands-on experience, allowing me to apply theoretical knowledge to tangible problems. This approach not only enhances my understanding of machine learning but also prepares me to contribute meaningfully in this field. Through this program, I am honing my skills to become an innovative practitioner in ML, capable of leveraging this technology to create impactful, ethical, and forward-thinking solutions in various domains, including digital health and beyond."
  },
  {
    "objectID": "posts/clustering/index.html",
    "href": "posts/clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering in machine learning is a type of unsupervised learning technique that involves grouping similar data points together into clusters. In this context, “unsupervised” means that the learning process is conducted without any predefined labels or categories; the algorithm itself discovers the inherent groupings in the data."
  },
  {
    "objectID": "posts/clustering/index.html#clustering",
    "href": "posts/clustering/index.html#clustering",
    "title": "Clustering",
    "section": "Clustering",
    "text": "Clustering\nClustering involves the task of grouping a population or data points into distinct clusters, where items within the same cluster share more similarities with each other compared to those in different clusters. Essentially, the goal is to identify and categorize groups with similar characteristics into clusters.\nThere are various types of clustering algorithms due to the subjective nature of clustering:\n\nConnectivity models: These models rely on the premise that closer data points in the data space exhibit greater similarity. They can start by classifying each point into separate clusters and then merge them as distance decreases, or begin with one cluster and split it as distance increases. Hierarchical clustering is a prominent example of this model.\nCentroid models: These iterative algorithms gauge similarity based on a point’s proximity to cluster centroids. K-Means clustering falls into this category, requiring the number of clusters to be specified beforehand, which demands prior knowledge of the dataset.\nDistribution models: These models assess the probability that data points in a cluster belong to the same distribution, like Normal or Gaussian. The Expectation-maximization algorithm is an instance, but it tends to overfit the data.\nDensity Models: These algorithms explore data space for areas with varying densities of data points. They identify different density regions and group data points within these regions into the same cluster. DBSCAN and OPTICS are popular examples.\n\nI’ll delve deeper into two widely used clustering algorithms: K Means and Hierarchical clustering.\n\n1. KMeans Clustering:\nK-Means clustering is an unsupervised technique used to group data without pre-existing labels for training. Instead, it relies on the inherent patterns within independent features to derive insights on unseen data.\n\n\n\nSrc: https://www.kaggle.com/code/pythonkumar/clustering-k-means-dbscan-gmm-bgmm-dendogram-viz\n\n\n\n\n2. Hierarchical Clustering\nHierarchical Clustering, also known as Hierarchical Cluster Analysis (HCA), is an unsupervised clustering method that organizes clusters with a clear top-to-bottom order.\nThis algorithm groups similar objects into clusters, resembling the hierarchical organization seen in file and folder structures on a hard disk. The primary goal is to create a set of distinct clusters, each cluster being unique from the others, while objects within each cluster share substantial similarities.\nHierarchical clustering is typically classified into two types:\nAgglomerative Hierarchical Clustering\n\n\n\nSrc: https://www.kaggle.com/code/pythonkumar/clustering-k-means-dbscan-gmm-bgmm-dendogram-viz\n\n\nDivisive Hierarchical Clustering\n\n\n\n\n\n3. Density Based(DBSCAN)\n\n\n\nSrc: https://www.kaggle.com/code/pythonkumar/clustering-k-means-dbscan-gmm-bgmm-dendogram-viz\n\n\nDBSCAN (Density-based spatial clustering of applications with noise) is an algorithm designed to identify clusters of varying shapes and sizes within a dataset, even in the presence of noise and outliers.\nThe algorithm relies on two key parameters:\n\nminPts: This threshold determines the minimum number of points required to be clustered together for a region to be recognized as dense.\neps (ε): A distance measurement used to locate neighboring points around any given point.\n\nAfter completing the DBSCAN clustering, three types of points emerge:\n\nCore: These points have at least m neighboring points within a distance of n from themselves.\nBorder: Points classified as Border have at least one Core point within a distance of n.\nNoise: These points neither qualify as Core nor Border points. They have fewer than m neighboring points within a distance of n from themselves.\n\n\n\n4. Gaussian Mixture Model\nGaussian Mixture Models (GMMs) models assume multiple Gaussian distributions, each representing a cluster. GMMs use a soft clustering approach, probabilistically assigning data points to different clusters. The algorithm comprises two steps: the Expectation (E) step and the Maximization (M) step.\n\n\n\nSrc: https://www.kaggle.com/code/pythonkumar/clustering-k-means-dbscan-gmm-bgmm-dendogram-viz"
  },
  {
    "objectID": "posts/clustering/index.html#kmodes-clustering-using-cardio-data",
    "href": "posts/clustering/index.html#kmodes-clustering-using-cardio-data",
    "title": "Clustering",
    "section": "KModes Clustering using Cardio Data",
    "text": "KModes Clustering using Cardio Data\n\nfrom numpy import unique, where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import KMeans, DBSCAN\n\n# Initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# Define the KMeans model\nkmeans_model = KMeans(n_clusters=2)\n\n# Assign each data point to a cluster using KMeans\nkmeans_result = kmeans_model.fit_predict(training_data)\n\n# Define the DBSCAN model\ndbscan_model = DBSCAN(eps=0.3, min_samples=9)  # Adjust 'eps' and 'min_samples' as needed\n\n# Assign each data point to a cluster using DBSCAN\ndbscan_result = dbscan_model.fit_predict(training_data)\n\n# Get all of the unique clusters from DBSCAN results\ndbscan_clusters = unique(dbscan_result)\n\n# Plot the DBSCAN clusters\nfor dbscan_cluster in dbscan_clusters:\n    # Get data points that fall in this cluster\n    index = where(dbscan_result == dbscan_cluster)\n    # Make the plot\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n\n# Show the DBSCAN plot\npyplot.show()\n\nC:\\Users\\rahul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning"
  },
  {
    "objectID": "posts/classification/index.html#example-credit-card-fraud-detection",
    "href": "posts/classification/index.html#example-credit-card-fraud-detection",
    "title": "Classification",
    "section": "Example: Credit Card Fraud Detection",
    "text": "Example: Credit Card Fraud Detection\n\n#Importing librairies\n\nimport pandas as pd \nimport numpy as np\n\n# Scikit-learn library: For SVM\nfrom sklearn import preprocessing\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import svm\n\nimport itertools\n\n# Matplotlib library to plot the charts\nimport matplotlib.pyplot as plt\nimport matplotlib.mlab as mlab\n\n# Library for the statistic data vizualisation\nimport seaborn\n\n%matplotlib inline\n\nData recuperation\n\ndata = pd.read_csv('R:/Blog/posts/classification/creditcard.csv') # Reading the file .csv\ndf = pd.DataFrame(data) # Converting data to Panda DataFrame\n\nData Visualization\n\ndf = pd.DataFrame(data) # Converting data to Panda DataFrame\n\n\ndf.describe() # Description of statistic features (Sum, Average, Variance, minimum, 1st quartile, 2nd quartile, 3rd Quartile and Maximum)\n\n\n\n\n\n\n\n\nTime\nV1\nV2\nV3\nV4\nV5\nV6\nV7\nV8\nV9\n...\nV21\nV22\nV23\nV24\nV25\nV26\nV27\nV28\nAmount\nClass\n\n\n\n\ncount\n284807.000000\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n...\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n2.848070e+05\n284807.000000\n284807.000000\n\n\nmean\n94813.859575\n1.168375e-15\n3.416908e-16\n-1.379537e-15\n2.074095e-15\n9.604066e-16\n1.487313e-15\n-5.556467e-16\n1.213481e-16\n-2.406331e-15\n...\n1.654067e-16\n-3.568593e-16\n2.578648e-16\n4.473266e-15\n5.340915e-16\n1.683437e-15\n-3.660091e-16\n-1.227390e-16\n88.349619\n0.001727\n\n\nstd\n47488.145955\n1.958696e+00\n1.651309e+00\n1.516255e+00\n1.415869e+00\n1.380247e+00\n1.332271e+00\n1.237094e+00\n1.194353e+00\n1.098632e+00\n...\n7.345240e-01\n7.257016e-01\n6.244603e-01\n6.056471e-01\n5.212781e-01\n4.822270e-01\n4.036325e-01\n3.300833e-01\n250.120109\n0.041527\n\n\nmin\n0.000000\n-5.640751e+01\n-7.271573e+01\n-4.832559e+01\n-5.683171e+00\n-1.137433e+02\n-2.616051e+01\n-4.355724e+01\n-7.321672e+01\n-1.343407e+01\n...\n-3.483038e+01\n-1.093314e+01\n-4.480774e+01\n-2.836627e+00\n-1.029540e+01\n-2.604551e+00\n-2.256568e+01\n-1.543008e+01\n0.000000\n0.000000\n\n\n25%\n54201.500000\n-9.203734e-01\n-5.985499e-01\n-8.903648e-01\n-8.486401e-01\n-6.915971e-01\n-7.682956e-01\n-5.540759e-01\n-2.086297e-01\n-6.430976e-01\n...\n-2.283949e-01\n-5.423504e-01\n-1.618463e-01\n-3.545861e-01\n-3.171451e-01\n-3.269839e-01\n-7.083953e-02\n-5.295979e-02\n5.600000\n0.000000\n\n\n50%\n84692.000000\n1.810880e-02\n6.548556e-02\n1.798463e-01\n-1.984653e-02\n-5.433583e-02\n-2.741871e-01\n4.010308e-02\n2.235804e-02\n-5.142873e-02\n...\n-2.945017e-02\n6.781943e-03\n-1.119293e-02\n4.097606e-02\n1.659350e-02\n-5.213911e-02\n1.342146e-03\n1.124383e-02\n22.000000\n0.000000\n\n\n75%\n139320.500000\n1.315642e+00\n8.037239e-01\n1.027196e+00\n7.433413e-01\n6.119264e-01\n3.985649e-01\n5.704361e-01\n3.273459e-01\n5.971390e-01\n...\n1.863772e-01\n5.285536e-01\n1.476421e-01\n4.395266e-01\n3.507156e-01\n2.409522e-01\n9.104512e-02\n7.827995e-02\n77.165000\n0.000000\n\n\nmax\n172792.000000\n2.454930e+00\n2.205773e+01\n9.382558e+00\n1.687534e+01\n3.480167e+01\n7.330163e+01\n1.205895e+02\n2.000721e+01\n1.559499e+01\n...\n2.720284e+01\n1.050309e+01\n2.252841e+01\n4.584549e+00\n7.519589e+00\n3.517346e+00\n3.161220e+01\n3.384781e+01\n25691.160000\n1.000000\n\n\n\n\n8 rows × 31 columns\n\n\n\n\ndf_fraud = df[df['Class'] == 1] # Recovery of fraud data\nplt.figure(figsize=(15,10))\nplt.scatter(df_fraud['Time'], df_fraud['Amount']) # Display fraud amounts according to their time\nplt.title('Scratter plot amount fraud')\nplt.xlabel('Time')\nplt.ylabel('Amount')\nplt.xlim([0,175000])\nplt.ylim([0,2500])\nplt.show()\n\n\n\n\n\nnb_big_fraud = df_fraud[df_fraud['Amount'] &gt; 1000].shape[0] # Recovery of frauds over 1000\nprint('There are only '+ str(nb_big_fraud) + ' frauds where the amount was bigger than 1000 over ' + str(df_fraud.shape[0]) + ' frauds')\n\nThere are only 9 frauds where the amount was bigger than 1000 over 492 frauds\n\n\nThere are only 9 frauds where the amount was bigger than 1000 over 492 frauds\n\nUnbalanced data\n\n\nnumber_fraud = len(data[data.Class == 1])\nnumber_no_fraud = len(data[data.Class == 0])\nprint('There are only '+ str(number_fraud) + ' frauds in the original dataset, even though there are ' + str(number_no_fraud) +' no frauds in the dataset.')\n\nThere are only 492 frauds in the original dataset, even though there are 284315 no frauds in the dataset.\n\n\nThere are only 492 frauds in the original dataset, even though there are 284315 no frauds in the dataset.\n\nprint(\"The accuracy of the classifier then would be : \"+ str((284315-492)/284315)+ \" which is the number of good classification over the number of tuple to classify\")\n\nThe accuracy of the classifier then would be : 0.998269524998681 which is the number of good classification over the number of tuple to classify\n\n\nCorrelation of features\n\ndf_corr = df.corr() # Calculation of the correlation coefficients in pairs, with the default method:\n                    # Pearson, Standard Correlation Coefficient\n\n\nplt.figure(figsize=(15,10))\nseaborn.heatmap(df_corr, cmap=\"YlGnBu\") # Displaying the Heatmap\nseaborn.set(font_scale=2,style='white')\n\nplt.title('Heatmap correlation')\nplt.show()\n\n\n\n\nAs we can notice, most of the features are not correlated with each other. This corroborates the fact that a PCA was previously performed on the data.\nWhat can generally be done on a massive dataset is a dimension reduction. By picking th emost important dimensions, there is a possiblity of explaining most of the problem, thus gaining a considerable amount of time while preventing the accuracy to drop too much.\nHowever in this case given the fact that a PCA was previously performed, if the dimension reduction is effective then the PCA wasn’t computed in the most effective way. Another way to put it is that no dimension reduction should be computed on a dataset on which a PCA was computed correctly.\nCorrelation of features\nOVERSAMPLING\nOne way to do oversampling is to replicate the under-represented class tuples until we attain a correct proportion between the class\nHowever as we haven’t infinite time nor the patience, we are going to run the classifier with the undersampled training data (for those using the undersampling principle if results are really bad just rerun the training dataset definition)\nUNDERSAMPLING\n\nimport pandas as pd\n# We seperate ours data in two groups : a train dataset and a test dataset\n\n# First we build our train dataset\ndf_train_all = df[0:150000] # We cut in two the original dataset\ndf_train_1 = df_train_all[df_train_all['Class'] == 1] # We seperate the data which are the frauds and the no frauds\ndf_train_0 = df_train_all[df_train_all['Class'] == 0]\nprint('In this dataset, we have ' + str(len(df_train_1)) +\" frauds so we need to take a similar number of non-fraud\")\n\ndf_sample=df_train_0.sample(300)\ncombined_df = pd.concat([df_train_1, df_sample], ignore_index=True) # We gather the frauds with the no frauds. \ncombined_df = combined_df.sample(frac=1) # Then we mix our dataset\n\nIn this dataset, we have 293 frauds so we need to take a similar number of non-fraud\n\n\nIn this dataset, we have 293 frauds so we need to take a similar number of non-fraud\n\nX_train = combined_df.drop(['Time', 'Class'], axis=1)  # Drop the features \"Time\" (useless) and \"Class\" (label)\ny_train = combined_df['Class']  # Create the label\nX_train = np.asarray(X_train)\ny_train = np.asarray(y_train)\n\n\n############################## with all the test dataset to see if the model learn correctly ##################\ndf_test_all = df[150000:]\n\nX_test_all = df_test_all.drop(['Time', 'Class'],axis=1)\ny_test_all = df_test_all['Class']\nX_test_all = np.asarray(X_test_all)\n\nConfusion Matrix\n\nclass_names=np.array(['0','1']) # Binary label, Class = 1 (fraud) and Class = 0 (no fraud)\n\n\n# Function to plot the confusion Matrix\ndef plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = 'd' \n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] &gt; thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\nModel Selection\nSo now, we’ll use a SVM model classifier, with the scikit-learn library.\n\nclassifier = svm.SVC(kernel='linear') # We set a SVM classifier, the default SVM Classifier (Kernel = Radial Basis Function)\n\n\nclassifier.fit(X_train, y_train) # Then we train our model, with our balanced data train.\n\nSVC(kernel='linear')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.SVCSVC(kernel='linear')\n\n\nTesting the model\n\nprediction_SVM_all = classifier.predict(X_test_all) #And finally, we predict our data test.\n\n\ncm = confusion_matrix(y_test_all, prediction_SVM_all)\nplot_confusion_matrix(cm,class_names)\n\n\n\n\nIn this case we are gonna try to minimize the number of errors in our prediction results. Errors are on the anti-diagonal of the confusion matrix. But we can infer that being wrong about an actual fraud is far worse than being wrong about a non-fraud transaction.\nThat is why using the accuracy as only classification criterion could be considered unthoughtful. During the remaining part of this study our criterion will consider precision on the real fraud 4 times more important than the general accuracy. Even though the final tested result is accuracy.\n\nprint('Our criterion give a result of ' \n      + str( ( (cm[0][0]+cm[1][1]) / (sum(cm[0]) + sum(cm[1])) + 4 * cm[1][1]/(cm[1][0]+cm[1][1])) / 5))\n\nOur criterion give a result of 0.9170704904644433\n\n\nOur criterion give a result of 0.905383035408\n\nprint('We have detected ' + str(cm[1][1]) + ' frauds / ' + str(cm[1][1]+cm[1][0]) + ' total frauds.')\nprint('\\nSo, the probability to detect a fraud is ' + str(cm[1][1]/(cm[1][1]+cm[1][0])))\nprint(\"the accuracy is : \"+str((cm[0][0]+cm[1][1]) / (sum(cm[0]) + sum(cm[1]))))\n\nWe have detected 181 frauds / 199 total frauds.\n\nSo, the probability to detect a fraud is 0.9095477386934674\nthe accuracy is : 0.9471614975483469\n\n\nWe have detected 177 frauds / 199 total frauds.\nSo, the probability to detect a fraud is 0.889447236181 the accuracy is : 0.969126232317\nModels Rank\nThere is a need to compute the fit method again, as the dimension of the tuples to predict went from 29 to 10 because of the dimension reduction\n\n# Define a simple feature ranking function (you can customize this)\ndef rank_features(data):\n    # Implement your feature ranking logic here\n    ranked_features = data  # Replace this with your actual feature ranking code\n    return ranked_features\n\n# Now you can use the rank_features function\nX_train_rank = rank_features(X_train)\nX_test_all_rank = rank_features(X_test_all)\n\n\nprediction_SVM = classifier.predict(X_test_all_rank)\n\n\ncm = confusion_matrix(y_test_all, prediction_SVM)\nplot_confusion_matrix(cm,class_names)\n\n\n\n\n\nprint('Our criterion give a result of ' \n      + str( ( (cm[0][0]+cm[1][1]) / (sum(cm[0]) + sum(cm[1])) + 4 * cm[1][1]/(cm[1][0]+cm[1][1])) / 5))\n\nOur criterion give a result of 0.9170704904644433\n\n\nOur criterion give a result of 0.912995958898\n\nprint('We have detected ' + str(cm[1][1]) + ' frauds / ' + str(cm[1][1]+cm[1][0]) + ' total frauds.')\nprint('\\nSo, the probability to detect a fraud is ' + str(cm[1][1]/(cm[1][1]+cm[1][0])))\nprint(\"the accuracy is : \"+str((cm[0][0]+cm[1][1]) / (sum(cm[0]) + sum(cm[1]))))\n\nWe have detected 181 frauds / 199 total frauds.\n\nSo, the probability to detect a fraud is 0.9095477386934674\nthe accuracy is : 0.9471614975483469\n\n\nWe have detected 179 frauds / 199 total frauds.\nSo, the probability to detect a fraud is 0.899497487437 the accuracy is : 0.966989844741"
  },
  {
    "objectID": "posts/linregression/index.html",
    "href": "posts/linregression/index.html",
    "title": "Linear Regression",
    "section": "",
    "text": "Linear regression is a statistical method used in data science and machine learning for predictive analysis.\nIt establishes a linear relationship between an independent variable (predictor) and a dependent variable (outcome) for prediction.\nThe method is suitable for continuous or numeric variables like sales, salary, age, etc.\n\n\n\n\n\nUsed in stock market forecasting, portfolio management, scientific analysis, and more.\nA simple representation is a sloped straight line in a graph, depicting the best fit line for a set of data.\n\n\n\n\n\nEasy to implement and interpret.\nScalable and optimal for online settings due to its computational efficiency.\n\n\n\n\nThe equation Y = mX + b, where ‘m’ is the slope and ‘b’ is the intercept, describes the relationship. In machine learning, it’s often expressed as y(x) = p0 + p1  x, where p0 and p1 are parameters to be determined.\n\n\n\n\n\nDefinition: Simple Linear Regression is a statistical method that allows us to summarize and study relationships between two continuous (quantitative) variables. This method assumes a linear relationship between the dependent variable and the independent variable.\nEquation: Y = β0 + β1 * X + ε\n\nY: Dependent variable\nX: Independent variable\nβ0: Intercept of the regression line\nβ1: Slope of the regression line\nε: Error term\n\nApplication: For example, predicting the price of a house (dependent variable) based on its size (independent variable).\n\n\n\n\nDefinition: Multiple Linear Regression is an extension of simple linear regression and is used to predict the outcome of a dependent variable based on the value of two or more independent variables. This method helps in understanding how changes in independent variables are associated with changes in the dependent variable.\nEquation: Y = β0 + β1 * X1 + β2 * X2 + … + βn * Xn + ε\n\nY: Dependent variable\nX1, X2, …, Xn: Independent variables\nβ0: Intercept\nβ1, β2, …, βn: Slopes for each independent variable\nε: Error term\n\nApplication: Predicting a person’s weight based on their height, age, and diet (three independent variables).\n\n\n\nDefinition: Logistic Regression is a statistical method for analyzing a dataset in which there are one or more independent variables that determine an outcome. The outcome is measured with a dichotomous variable (in which there are only two possible outcomes). It is used for predicting the probability of a binary outcome based on one or more predictor variables.\nEquation: log(p/(1-p)) = β0 + β1 * X1 + … + βn * Xn\n\np: Probability of the dependent event occurring\nX1, …, Xn: Predictor variables\nβ0: Intercept\nβ1, …, βn: Coefficients for each predictor\n\nApplication: Determining whether an email is spam or not spam, based on features like the email’s content, sender, etc.\n\n\n\nDefinition: Ordinal Regression is used when the dependent variable is ordinal, meaning it reflects a scale of magnitude. It models the relationship between a set of predictor variables and an ordinal scale dependent variable, where the categories have a natural order, but the intervals between them are not assumed to be equidistant.\nCharacteristics: The categories have a ranked order, but the intervals between the ranks are not necessarily equal.\nApplication: Rating a movie as poor, fair, good, very good, and excellent. Here, the ratings have a natural order but the difference between each category is not quantified.\n\n\n\nDefinition: Multinomial Logistic Regression is a classification method that generalizes logistic regression to multiclass problems, i.e., where the dependent variable can have more than two possible nominal (unordered) outcomes. It is used when the outcome involves more than two categories.\nCharacteristics: Similar to logistic regression, but suitable for more than two classes.\nApplication: Predicting the choice of transportation (like car, bus, train, bike) based on factors like distance, cost, and time.\n\n\n\nImport Libraries and Reading Data\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.set_option('display.float_format', lambda x: '%.2f' % x)\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\ndf = pd.read_csv(\"R:/Blog/posts/linregression/Ecommerce Customers.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nEmail\nAddress\nAvatar\nAvg. Session Length\nTime on App\nTime on Website\nLength of Membership\nYearly Amount Spent\n\n\n\n\n0\nmstephenson@fernandez.com\n835 Frank Tunnel\\nWrightmouth, MI 82180-9605\nViolet\n34.50\n12.66\n39.58\n4.08\n587.95\n\n\n1\nhduke@hotmail.com\n4547 Archer Common\\nDiazchester, CA 06566-8576\nDarkGreen\n31.93\n11.11\n37.27\n2.66\n392.20\n\n\n2\npallen@yahoo.com\n24645 Valerie Unions Suite 582\\nCobbborough, D...\nBisque\n33.00\n11.33\n37.11\n4.10\n487.55\n\n\n3\nriverarebecca@gmail.com\n1414 David Throughway\\nPort Jason, OH 22070-1220\nSaddleBrown\n34.31\n13.72\n36.72\n3.12\n581.85\n\n\n4\nmstephens@davidson-herman.com\n14023 Rodriguez Passage\\nPort Jacobville, PR 3...\nMediumAquaMarine\n33.33\n12.80\n37.54\n4.45\n599.41\n\n\n\n\n\n\n\nThe variables to be used in the data were determined.\n\ndf = df.drop(['Email',\"Address\",\"Avatar\"], axis=1) \n\nAdvanced Functional Exploratory Data Analysis\nGeneral structure of the data is analyzed\n\ndef check_df(dataframe, head=5):\n    print(\"##################### Columns #####################\")\n    print(dataframe.columns)\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(head))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.describe([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n\ncheck_df(df)\n\n##################### Columns #####################\nIndex(['Avg. Session Length', 'Time on App', 'Time on Website',\n       'Length of Membership', 'Yearly Amount Spent'],\n      dtype='object')\n##################### Shape #####################\n(500, 5)\n##################### Types #####################\nAvg. Session Length     float64\nTime on App             float64\nTime on Website         float64\nLength of Membership    float64\nYearly Amount Spent     float64\ndtype: object\n##################### Head #####################\n   Avg. Session Length  Time on App  Time on Website  Length of Membership  \\\n0                34.50        12.66            39.58                  4.08   \n1                31.93        11.11            37.27                  2.66   \n2                33.00        11.33            37.11                  4.10   \n3                34.31        13.72            36.72                  3.12   \n4                33.33        12.80            37.54                  4.45   \n\n   Yearly Amount Spent  \n0               587.95  \n1               392.20  \n2               487.55  \n3               581.85  \n4               599.41  \n##################### Tail #####################\n     Avg. Session Length  Time on App  Time on Website  Length of Membership  \\\n495                33.24        13.57            36.42                  3.75   \n496                34.70        11.70            37.19                  3.58   \n497                32.65        11.50            38.33                  4.96   \n498                33.32        12.39            36.84                  2.34   \n499                33.72        12.42            35.77                  2.74   \n\n     Yearly Amount Spent  \n495               573.85  \n496               529.05  \n497               551.62  \n498               456.47  \n499               497.78  \n##################### NA #####################\nAvg. Session Length     0\nTime on App             0\nTime on Website         0\nLength of Membership    0\nYearly Amount Spent     0\ndtype: int64\n##################### Quantiles #####################\n                      count   mean   std    min     0%     5%    50%    95%  \\\nAvg. Session Length  500.00  33.05  0.99  29.53  29.53  31.45  33.08  34.59   \nTime on App          500.00  12.05  0.99   8.51   8.51  10.53  11.98  13.67   \nTime on Website      500.00  37.06  1.01  33.91  33.91  35.46  37.07  38.78   \nLength of Membership 500.00   3.53  1.00   0.27   0.27   1.81   3.53   5.08   \nYearly Amount Spent  500.00 499.31 79.31 256.67 256.67 376.29 498.89 628.15   \n\n                        99%   100%    max  \nAvg. Session Length   35.43  36.14  36.14  \nTime on App           14.22  15.13  15.13  \nTime on Website       39.25  40.01  40.01  \nLength of Membership   5.84   6.92   6.92  \nYearly Amount Spent  701.00 765.52 765.52  \n\n\nA scatterplot to observe the relationship between the variables was created.\n\nsns.pairplot(df, kind = \"reg\")\n\n\n\n\nAnalysis of Variable Types\nIt is necessary to determine the types of variables. Thus, we can determine the types of the variables and make them suitable for the model.\nIt gives the names of the numeric, categorical but cardinal variables in the data set.\ncat_cols: Categorical variable list\nnum_cols: Numeric variable list\ncat_but_car: Categorical but cardinal variable list\nThe function named grab_col_names helps to determine the types of variables.\n\ndef grab_col_names(dataframe, cat_th=10, car_th=20):\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() &lt; cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() &gt; car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)\n\nObservations: 500\nVariables: 5\ncat_cols: 0\nnum_cols: 5\ncat_but_car: 0\nnum_but_cat: 0\n\n\nOutlier Analysis\nValues that go far beyond the general trend in the data are called outliers. Especially in linear methods, the effects of outliers are more severe.Outliers cause bias in the data set.For all these reasons, it needs to be analyzed.\n\ndef outlier_thresholds(dataframe, col_name, q1=0.10, q3=0.90):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\ndef check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] &gt; up_limit) | (dataframe[col_name] &lt; low_limit)].any(axis=None):\n        return True\n    else:\n        return False\nfor col in num_cols:\n    print(col, check_outlier(df, col))\n\nAvg. Session Length False\nTime on App False\nTime on Website False\nLength of Membership False\nYearly Amount Spent False\n\n\nAvg. Session Length False Time on App False Time on Website False Length of Membership False Yearly Amount Spent False\nAnalysis Of Missing Values\nMissing values may cause problems while setting up the model. It must be detected and necessary actions must be taken.\nNo missing values were found for the relevant data.\n\ndef missing_values_table(dataframe, na_name=False):\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() &gt; 0]\n\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[na_columns].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df, end=\"\\n\")\n\n    if na_name:\n        return na_columns\n\nmissing_values_table(df)\n\nEmpty DataFrame\nColumns: [n_miss, ratio]\nIndex: []\n\n\nEmpty DataFrame Columns: [n_miss, ratio] Index: []\nCorrelation Analysis\nValues with high correlation affect the target variable to a similar extent. Therefore, we can eliminate one of the variables with high correlation between two variables and use the other.\nWhen the data was examined, no variable with a high correlation of more than 90% was found.\n\ndef high_correlated_cols(dataframe, plot=False, corr_th=0.90):\n    corr = dataframe.corr()\n    cor_matrix = corr.abs()\n    upper_triangle_matrix = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(bool))\n    drop_list = [col for col in upper_triangle_matrix.columns if any(upper_triangle_matrix[col] &gt; corr_th)]\n    if plot:\n        import seaborn as sns\n        import matplotlib.pyplot as plt\n        sns.set(rc={'figure.figsize': (10, 5)})\n        sns.heatmap(corr, cmap=\"RdBu\", annot=True)\n        plt.show(block=True)\n    return drop_list\n\n\nhigh_correlated_cols(df,plot=True)\n\n\n\n\n[]\n\n\n\n\n\n\nLinear regression models the relationship between dependent and independent variable/variables linearly.\nIn order to create the model, dependent and independent variables were defined.\n\nX = df.drop('Yearly Amount Spent', axis=1) \n\ny = df[[\"Yearly Amount Spent\"]]\n\nBuilding the Model\nBuilding the Model A train and test set by dividing the data into two is created . By training the model with one part and testing the model with the other part, it can be determined how successful the model is.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n\nreg_model = LinearRegression().fit(X_train, y_train)\n\n# constant (b - bias)\nprint(reg_model.intercept_)\n\n# coefficients (w - weights)\nprint(reg_model.coef_)\n\n[-1047.73920526]\n[[25.78854257 38.85150472  0.25638467 61.49204989]]\n\n\nPrediction of dependent variable\n\ny_pred = reg_model.predict(X_test)\n\n\n\n\n!(rmse.png)\n\nnp.sqrt(mean_squared_error(y_test, y_pred))\n\n8.84848631350032\n\n\nThe ratio of independent variables description of dependent variable\n\nreg_model.score(X_test, y_test)\n\n0.9892888134002329\n\n\n\n\n\nFinally, the actual values corresponding to the predicted values of the model are shown in the graph.\n\ny_pred = pd.DataFrame(y_pred)\ny_test = y_test.reset_index(drop=True)\ndf_ = pd.concat([y_test,y_pred], axis=1)\ndf_.columns = [\"y_test\",\"y_pred\"]\nplt.figure(figsize=(15,10))\nplt.plot(df_)\nplt.legend([\"ACTUAL VALUES\" , \"MODEL PREDICTION\"])\n\n&lt;matplotlib.legend.Legend at 0x178c42088b0&gt;"
  },
  {
    "objectID": "posts/linregression/index.html#import-library-and-dataset",
    "href": "posts/linregression/index.html#import-library-and-dataset",
    "title": "Linear Regression",
    "section": "Import Library and Dataset",
    "text": "Import Library and Dataset\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\n\n# Import library\nimport pandas  as pd #Data manipulation\nimport numpy as np #Data manipulation\nimport matplotlib.pyplot as plt # Visualization\nimport seaborn as sns #Visualization\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# Import dataset\ndf = pd.read_csv('R:/Blog/posts/linregression/insurance.csv')\nprint('\\nNumber of rows and columns in the data set: ',df.shape)\nprint('')\ndf.head()\n\n\nNumber of rows and columns in the data set:  (1338, 7)\n\n\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.92400\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.55230\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.46200\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.47061\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.85520\n\n\n\n\n\n\n\n\n\"\"\" for our visualization purpose will fit line using seaborn library only for bmi as independent variable \nand charges as dependent variable\"\"\"\n\nsns.lmplot(x='bmi',y='charges',data=df,aspect=1.5,  height=4)\nplt.xlabel('Boby Mass Index$(kg/m^2)$: as Independent variable')\nplt.ylabel('Insurance Charges: as Dependent variable')\nplt.title('Charge Vs BMI');\n\n\n\n\n\ndf.describe()\n\n\n\n\n\n\n\n\nage\nbmi\nchildren\ncharges\n\n\n\n\ncount\n1338.000000\n1338.000000\n1338.000000\n1338.000000\n\n\nmean\n39.207025\n30.663397\n1.094918\n13270.422265\n\n\nstd\n14.049960\n6.098187\n1.205493\n12110.011237\n\n\nmin\n18.000000\n15.960000\n0.000000\n1121.873900\n\n\n25%\n27.000000\n26.296250\n0.000000\n4740.287150\n\n\n50%\n39.000000\n30.400000\n1.000000\n9382.033000\n\n\n75%\n51.000000\n34.693750\n2.000000\n16639.912515\n\n\nmax\n64.000000\n53.130000\n5.000000\n63770.428010\n\n\n\n\n\n\n\n\nCheck for missing value\n\n# Check for missing values in the DataFrame\nmissing_values = df.isnull().sum()\nprint(\"Missing values count per column:\")\nprint(missing_values)\n\nMissing values count per column:\nage         0\nsex         0\nbmi         0\nchildren    0\nsmoker      0\nregion      0\ncharges     0\ndtype: int64\n\n\n\nf= plt.figure(figsize=(9,4))\n\nax=f.add_subplot(121)\nsns.histplot(df['charges'],bins=50,color='r',ax=ax)\nax.set_title('Distribution of insurance charges')\n\nax=f.add_subplot(122)\nsns.histplot(np.log10(df['charges']),bins=40,color='b',ax=ax)\nax.set_title('Distribution of insurance charges in $log$ sacle')\nax.set_xscale('log');\n\n\n\n\n\nf = plt.figure(figsize=(9,6))\nax = f.add_subplot(121)\nsns.violinplot(x='sex', y='charges',data=df,palette='Wistia',ax=ax)\nax.set_title('Violin plot of Charges vs sex')\n\nax = f.add_subplot(122)\nsns.violinplot(x='smoker', y='charges',data=df,palette='magma',ax=ax)\nax.set_title('Violin plot of Charges vs smoker');\n\n\n\n\nFrom left plot the insurance charge for male and female is approximatley in same range,it is average around 5000 bucks. In right plot the insurance charge for smokers is much wide range compare to non smokers, the average charges for non smoker is approximately 5000 bucks. For smoker the minimum insurance charge is itself 5000 bucks.\n\nplt.figure(figsize=(9,6))\nsns.boxplot(x='children', y='charges',hue='sex',data=df,palette='rainbow')\nplt.title('Box plot of charges vs children');\n\n\n\n\n\nplt.figure(figsize=(9,6))\nsns.violinplot(x='region', y='charges',hue='sex',data=df,palette='rainbow',split=True)\nplt.title('Violin plot of charges vs children');\n\n\n\n\n\nf = plt.figure(figsize=(9,6))\nax = f.add_subplot(121)\nsns.scatterplot(x='age',y='charges',data=df,palette='magma',hue='smoker',ax=ax)\nax.set_title('Scatter plot of Charges vs age')\n\nax = f.add_subplot(122)\nsns.scatterplot(x='bmi',y='charges',data=df,palette='viridis',hue='smoker')\nax.set_title('Scatter plot of Charges vs bmi')\nplt.savefig('sc.png');"
  },
  {
    "objectID": "posts/linregression/index.html#data-preprocessing",
    "href": "posts/linregression/index.html#data-preprocessing",
    "title": "Linear Regression",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nData preprocessing in machine learning involves encoding categorical data into numerical form, as machine learning algorithms typically require numerical input. There are several techniques for this:\n\nLabel Encoding: This method involves converting categorical labels into numerical values to enable algorithms to work with them.\nOne-Hot Encoding: One-hot encoding represents categorical variables as binary vectors, making the data more expressive. First, the categorical values are mapped to integer values (label encoding), and then each integer is converted into a binary vector with all zeros except for the index of the integer, which is marked with a 1.\nDummy Variable Trap: This situation occurs when independent variables are multicollinear, meaning that two or more variables are highly correlated, making it possible to predict one variable from the others.\n\nTo simplify this process, the pandas library offers a convenient function called get_dummies. This function allows us to perform all three steps in a single line of code. We can use it to create dummy variables for features like ‘sex,’ ‘children,’ ‘smoker,’ and ‘region.’ By setting the drop_first=True parameter, we can automatically eliminate the dummy variable trap by dropping one variable and retaining the original variable. This makes data preprocessing more straightforward and efficient.\n\n# Dummy variable\ncategorical_columns = ['sex','children', 'smoker', 'region']\ndf_encode = pd.get_dummies(data = df, prefix = 'OHE', prefix_sep='_',\n               columns = categorical_columns,\n               drop_first =True,\n              dtype='int8')\n\n\n# Lets verify the dummay variable process\nprint('Columns in original data frame:\\n',df.columns.values)\nprint('\\nNumber of rows and columns in the dataset:',df.shape)\nprint('\\nColumns in data frame after encoding dummy variable:\\n',df_encode.columns.values)\nprint('\\nNumber of rows and columns in the dataset:',df_encode.shape)\n\nColumns in original data frame:\n ['age' 'sex' 'bmi' 'children' 'smoker' 'region' 'charges']\n\nNumber of rows and columns in the dataset: (1338, 7)\n\nColumns in data frame after encoding dummy variable:\n ['age' 'bmi' 'charges' 'OHE_male' 'OHE_1' 'OHE_2' 'OHE_3' 'OHE_4' 'OHE_5'\n 'OHE_yes' 'OHE_northwest' 'OHE_southeast' 'OHE_southwest']\n\nNumber of rows and columns in the dataset: (1338, 13)\n\n\n\nfrom scipy.stats import boxcox\ny_bc,lam, ci= boxcox(df_encode['charges'],alpha=0.05)\n\n#df['charges'] = y_bc  \n# it did not perform better for this model, so log transform is used\nci,lam\n## Log transform\ndf_encode['charges'] = np.log(df_encode['charges'])\n\nThe original categorical variable are remove and also one of the one hot encode varible column for perticular categorical variable is droped from the column. So we completed all three encoding step by using get dummies function."
  },
  {
    "objectID": "posts/linregression/index.html#train-test-split",
    "href": "posts/linregression/index.html#train-test-split",
    "title": "Linear Regression",
    "section": "Train Test split",
    "text": "Train Test split\n\nfrom sklearn.model_selection import train_test_split\nX = df_encode.drop('charges',axis=1) # Independet variable\ny = df_encode['charges'] # dependent variable\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=23)\n\n\n# Step 1: add x0 =1 to dataset\nX_train_0 = np.c_[np.ones((X_train.shape[0],1)),X_train]\nX_test_0 = np.c_[np.ones((X_test.shape[0],1)),X_test]\n\n# Step2: build model\ntheta = np.matmul(np.linalg.inv( np.matmul(X_train_0.T,X_train_0) ), np.matmul(X_train_0.T,y_train)) \n\n\n# The parameters for linear regression model\nparameter = ['theta_'+str(i) for i in range(X_train_0.shape[1])]\ncolumns = ['intersect:x_0=1'] + list(X.columns.values)\nparameter_df = pd.DataFrame({'Parameter':parameter,'Columns':columns,'theta':theta})\n\n\n# Scikit Learn module\nfrom sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(X_train,y_train) # Note: x_0 =1 is no need to add, sklearn will take care of it.\n\n#Parameter\nsk_theta = [lin_reg.intercept_]+list(lin_reg.coef_)\nparameter_df = parameter_df.join(pd.Series(sk_theta, name='Sklearn_theta'))\n\nThe parameter obtained from both the model are same.So we successfully build our model using normal equation and verified using sklearn linear regression module. Let’s move ahead, next step is prediction and model evaluation.\n\n# Normal equation\ny_pred_norm =  np.matmul(X_test_0,theta)\n\n#Evaluvation: MSE\nJ_mse = np.sum((y_pred_norm - y_test)**2)/ X_test_0.shape[0]\n\n# R_square \nsse = np.sum((y_pred_norm - y_test)**2)\nsst = np.sum((y_test - y_test.mean())**2)\nR_square = 1 - (sse/sst)\nprint('The Mean Square Error(MSE) or J(theta) is: ',J_mse)\nprint('R square obtain for normal equation method is :',R_square)\n\nThe Mean Square Error(MSE) or J(theta) is:  0.18729622322981948\nR square obtain for normal equation method is : 0.7795687545055312\n\n\n\n# sklearn regression module\ny_pred_sk = lin_reg.predict(X_test)\n\n#Evaluvation: MSE\nfrom sklearn.metrics import mean_squared_error\nJ_mse_sk = mean_squared_error(y_pred_sk, y_test)\n\n# R_square\nR_square_sk = lin_reg.score(X_test,y_test)\nprint('The Mean Square Error(MSE) or J(theta) is: ',J_mse_sk)\nprint('R square obtain for scikit learn library is :',R_square_sk)\n\nThe Mean Square Error(MSE) or J(theta) is:  0.18729622322981898\nR square obtain for scikit learn library is : 0.7795687545055318\n\n\nModel validation is a crucial step in assessing the performance of a linear regression model, and it involves checking various assumptions. The key assumptions for a linear regression model are as follows:\n\nLinear Relationship: Linear regression assumes that the relationship between the dependent and independent variables is linear. You can verify this assumption by creating a scatter plot of actual values against predicted values.\nNormality of Residuals: The residual errors should follow a normal distribution. This can be checked by examining the distribution of the residuals.\nMean of Residuals: The mean of the residual errors should ideally be close to 0.\nMultivariate Normality: Linear regression assumes that all variables are multivariate normally distributed. This assumption can be assessed using a Q-Q plot.\nMulticollinearity: Linear regression assumes minimal multicollinearity, meaning that independent variables are not highly correlated with each other. The variance inflation factor (VIF) can help identify and measure the strength of such correlations. A VIF greater than 1 but less than 5 indicates moderate correlation, while a VIF less than 5 suggests a critical level of multicollinearity.\nHomoscedasticity: The data should exhibit homoscedasticity, which means that the residuals are roughly equal across the regression line. You can assess this by creating a scatter plot of residuals against the fitted values. If the plot shows a funnel-shaped pattern, it indicates heteroscedasticity.\n\nEnsuring these assumptions are met is essential to build a reliable linear regression model.\n\n# Check for Linearity\nf = plt.figure(figsize=(9,5))\nax = f.add_subplot(121)\n#sns.scatterplot(data=df, y_test, y_pred_sk)\nsns.scatterplot(x=y_test,y=y_pred_sk,ax=ax,color='r')\nax.set_title('Check for Linearity:\\n Actual Vs Predicted value')\n\n# Check for Residual normality & mean\nax = f.add_subplot(122)\nsns.histplot((y_test - y_pred_sk),ax=ax,color='b')\nax.axvline((y_test - y_pred_sk).mean(),color='k',linestyle='--')\nax.set_title('Check for Residual normality & mean: \\n Residual eror');\n\n\n\n\n\n# Check for Multivariate Normality\n# Quantile-Quantile plot \nf,ax = plt.subplots(1,2,figsize=(9,6))\nimport scipy as sp\n_,(_,_,r)= sp.stats.probplot((y_test - y_pred_sk),fit=True,plot=ax[0])\nax[0].set_title('Check for Multivariate Normality: \\nQ-Q Plot')\n\n#Check for Homoscedasticity\nsns.scatterplot(y = (y_test - y_pred_sk), x= y_pred_sk, ax = ax[1],color='r') \nax[1].set_title('Check for Homoscedasticity: \\nResidual Vs Predicted');\n\n\n\n\n\n# Check for Multicollinearity\n#Variance Inflation Factor\nVIF = 1/(1- R_square_sk)\nVIF\n\n4.536561945911135\n\n\nHere are the model assumptions for linear regression, along with their assessment:\n\nThe actual vs. predicted plot doesn’t form a linear pattern, indicating a failure of the linear assumption.\nThe mean of the residuals is close to zero, and the residual error plot is skewed to the right.\nThe Q-Q plot shows that values greater than 1.5 tend to increase, suggesting a departure from multivariate normality.\nThe plot exhibits heteroscedasticity, with errors increasing after a certain point.\nThe variance inflation factor is less than 5, indicating the absence of multicollinearity."
  },
  {
    "objectID": "posts/nlinregression/index.html",
    "href": "posts/nlinregression/index.html",
    "title": "Nonlinear Regression",
    "section": "",
    "text": "Non-linear regression in machine learning is a powerful tool for modeling complex relationships between variables, where these relationships cannot be adequately described using a straight line. In machine learning, non-linear regression is used to predict outcomes based on non-linear interactions of predictor variables. It’s particularly useful in scenarios where the underlying data patterns are inherently non-linear."
  },
  {
    "objectID": "posts/nlinregression/index.html#example-of-nonlinear-regression",
    "href": "posts/nlinregression/index.html#example-of-nonlinear-regression",
    "title": "Nonlinear Regression",
    "section": "Example of Nonlinear Regression",
    "text": "Example of Nonlinear Regression\nI.Introduction\nWhen you’re looking at data that seems to form curves or non-straight patterns, using linear regression might not give you the most accurate results. Linear regression is like a tool designed for straight-line relationships, so when your data behaves in a curvy way, it doesn’t quite fit the tool’s assumptions. That’s where non-linear regression steps in. It’s like having a set of tools that can handle curves and bends in the data. These tools, like polynomial, exponential, or logarithmic regression, can adjust to the data’s curves and complexities, giving you more precise predictions and a better match to the real nature of the data."
  },
  {
    "objectID": "posts/nlinregression/index.html#linear",
    "href": "posts/nlinregression/index.html#linear",
    "title": "Nonlinear Regression",
    "section": "1. Linear",
    "text": "1. Linear\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the linear function (e.g., L(x) = ax + b)\na, b = 2, 1\n\n# Generate x values\nx = np.linspace(0, 5, 100)\n\n# Calculate the corresponding y values using the linear function\ny = a * x + b\n\n# Generate random data points with noise\nnp.random.seed(0)\nnoise = np.random.normal(0, 0.5, len(x))\ndata_y = y + noise\n\n# Plot the linear function and data points\nplt.plot(x, y, label='Linear Function', color='blue')\nplt.scatter(x, data_y, label='Data Points', color='red', alpha=0.5)\nplt.xlabel('x')\nplt.ylabel('L(x)')\nplt.title('Linear Function')\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/nlinregression/index.html#polynomial",
    "href": "posts/nlinregression/index.html#polynomial",
    "title": "Nonlinear Regression",
    "section": "2. Polynomial",
    "text": "2. Polynomial\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define coefficients of the polynomial equation (e.g., P(x) = ax^3 + bx^2 + cx + d)\na, b, c, d = 1, -3, 3, -1\n\n# Generate x values\nx = np.linspace(-2, 2, 100)\n\n# Calculate the corresponding y values using the polynomial equation\ny = a * x**3 + b * x**2 + c * x + d\n\n# Generate random data points with noise\nnp.random.seed(0)\nnoise = np.random.normal(0, 0.2, len(x))\ndata_y = y + noise\n\n# Plot the polynomial function and data points\nplt.plot(x, y, label='Polynomial Function', color='blue')\nplt.scatter(x, data_y, label='Data Points', color='red', alpha=0.5)\nplt.xlabel('x')\nplt.ylabel('P(x)')\nplt.title('Polynomial Function')\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/nlinregression/index.html#quadratic",
    "href": "posts/nlinregression/index.html#quadratic",
    "title": "Nonlinear Regression",
    "section": "3. Quadratic",
    "text": "3. Quadratic\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define coefficients of the quadratic equation (e.g., Q(x) = ax^2 + bx + c)\na, b, c = 1, -2, 1\n\n# Generate x values\nx = np.linspace(-2, 3, 100)\n\n# Calculate the corresponding y values using the quadratic equation\ny = a * x**2 + b * x + c\n\n# Generate random data points with noise\nnp.random.seed(0)\nnoise = np.random.normal(0, 0.2, len(x))\ndata_y = y + noise\n\n# Plot the quadratic function and data points\nplt.plot(x, y, label='Quadratic Function', color='blue')\nplt.scatter(x, data_y, label='Data Points', color='red', alpha=0.5)\nplt.xlabel('x')\nplt.ylabel('Q(x)')\nplt.title('Quadratic Function')\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/nlinregression/index.html#exponential",
    "href": "posts/nlinregression/index.html#exponential",
    "title": "Nonlinear Regression",
    "section": "4. Exponential",
    "text": "4. Exponential\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the exponential function (e.g., E(x) = a * e^(bx))\na, b = 1, 0.5\n\n# Generate x values\nx = np.linspace(0, 5, 100)\n\n# Calculate the corresponding y values using the exponential function\ny = a * np.exp(b * x)\n\n# Generate random data points with noise\nnp.random.seed(0)\nnoise = np.random.normal(0, 0.2, len(x))\ndata_y = y + noise\n\n# Plot the exponential function and data points\nplt.plot(x, y, label='Exponential Function', color='blue')\nplt.scatter(x, data_y, label='Data Points', color='red', alpha=0.5)\nplt.xlabel('x')\nplt.ylabel('E(x)')\nplt.title('Exponential Function')\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/nlinregression/index.html#logarithmic",
    "href": "posts/nlinregression/index.html#logarithmic",
    "title": "Nonlinear Regression",
    "section": "5. Logarithmic",
    "text": "5. Logarithmic\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the logarithmic function (e.g., L(x) = a * ln(bx))\na, b = 1, 2\n\n# Generate x values\nx = np.linspace(0.1, 5, 100)\n\n# Calculate the corresponding y values using the logarithmic function\ny = a * np.log(b * x)\n\n# Generate random data points with noise\nnp.random.seed(0)\nnoise = np.random.normal(0, 0.2, len(x))\ndata_y = y + noise\n\n# Plot the logarithmic function and data points\nplt.plot(x, y, label='Logarithmic Function', color='blue')\nplt.scatter(x, data_y, label='Data Points', color='red', alpha=0.5)\nplt.xlabel('x')\nplt.ylabel('L(x)')\nplt.title('Logarithmic Function')\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/nlinregression/index.html#sigmoidallogistic",
    "href": "posts/nlinregression/index.html#sigmoidallogistic",
    "title": "Nonlinear Regression",
    "section": "6. Sigmoidal/Logistic",
    "text": "6. Sigmoidal/Logistic\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the sigmoidal/logistic function (e.g., S(x) = 1 / (1 + e^(-x)))\nx = np.linspace(-5, 5, 100)\n\n# Calculate the corresponding y values using the sigmoidal/logistic function\ny = 1 / (1 + np.exp(-x))\n\n# Generate random data points with noise\nnp.random.seed(0)\nnoise = np.random.normal(0, 0.05, len(x))\ndata_y = y + noise\n\n# Plot the sigmoidal/logistic function and data points\nplt.plot(x, y, label='Sigmoidal/Logistic Function', color='blue')\nplt.scatter(x, data_y, label='Data Points', color='red', alpha=0.5)\nplt.xlabel('x')\nplt.ylabel('S(x)')\nplt.title('Sigmoidal/Logistic')\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/nlinregression/index.html#non-linear-regression-example-with-dataset",
    "href": "posts/nlinregression/index.html#non-linear-regression-example-with-dataset",
    "title": "Nonlinear Regression",
    "section": "Non-Linear Regression example with Dataset",
    "text": "Non-Linear Regression example with Dataset\n\ndf1 = pd.read_csv('R:/Blog/posts/nlinregression/gdp.csv')\ndf1.head()\ndf = pd.read_csv('R:/Blog/posts/nlinregression/gdp1.csv')\nprint('\\nNumber of rows and columns in the data set: ',df.shape)\nprint('')\n\n#Lets look into top few rows and columns in the dataset\ndf.head()\n\n\nNumber of rows and columns in the data set:  (55, 2)\n\n\n\n\n\n\n\n\n\n\nYear\nValue\n\n\n\n\n0\n1960\n5.918412e+10\n\n\n1\n1961\n4.955705e+10\n\n\n2\n1962\n4.668518e+10\n\n\n3\n1963\n5.009730e+10\n\n\n4\n1964\n5.906225e+10\n\n\n\n\n\n\n\n\nplt.figure(figsize=(8,5))\nx_data, y_data = (df[\"Year\"].values, df[\"Value\"].values)\nplt.plot(x_data, y_data, 'ro')\nplt.ylabel('GDP')\nplt.xlabel('Year')\nplt.show()"
  },
  {
    "objectID": "posts/nlinregression/index.html#section",
    "href": "posts/nlinregression/index.html#section",
    "title": "Nonlinear Regression",
    "section": "",
    "text": "Choosing a model\nFrom an initial look at the plot, we determine that the logistic function could be a good approximation, since it has the property of starting with a slow growth, increasing growth in the middle, and then decreasing again at the end; as illustrated below:\n\nX = np.arange(-5,5.0, 0.1)\nY = 1.0 / (1.0 + np.exp(-X))\n\nplt.plot(X,Y) \nplt.ylabel('Dependent Variable')\nplt.xlabel('Indepdendent Variable')\nplt.show()\n\n\n\n\n\nBuilding The Model\nNow, let’s build our regression model and initialize its parameters.\n\ndef sigmoid(x, Beta_1, Beta_2):\n     y = 1 / (1 + np.exp(-Beta_1*(x-Beta_2)))\n     return y\n   \n   \nbeta_1 = 0.10\nbeta_2 = 1990.0\n\n#logistic function\nY_pred = sigmoid(x_data, beta_1 , beta_2)\n\n#plot initial prediction against datapoints\nplt.plot(x_data, Y_pred*15000000000000.)\nplt.plot(x_data, y_data, 'ro')\n\n\n\n\n\n# Lets normalize our data\nxdata =x_data/max(x_data)\nydata =y_data/max(y_data)\n\n\nfrom scipy.optimize import curve_fit\npopt, pcov = curve_fit(sigmoid, xdata, ydata)\nprint(\" beta_1 = %f, beta_2 = %f\" % (popt[0], popt[1]))\n\n beta_1 = 690.451712, beta_2 = 0.997207\n\n\n\nx = np.linspace(1960, 2015, 55)\nx = x/max(x)\nplt.figure(figsize=(8,5))\ny = sigmoid(x, *popt)\nplt.plot(xdata, ydata, 'ro', label='data')\nplt.plot(x,y, linewidth=3.0, label='fit')\nplt.legend(loc='best')\nplt.ylabel('GDP')\nplt.xlabel('Year')\nplt.show()\n\n\n\n\n\n# split data into train/test\nmsk = np.random.rand(len(df)) &lt; 0.8\ntrain_x = xdata[msk]\ntest_x = xdata[~msk]\ntrain_y = ydata[msk]\ntest_y = ydata[~msk]\n\n# build the model using train set\npopt, pcov = curve_fit(sigmoid, train_x, train_y)\n\n# predict using test set\ny_hat = sigmoid(test_x, *popt)\n\n# evaluation\nprint(\"Mean absolute error: %.2f\" % np.mean(np.absolute(y_hat - test_y)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((y_hat - test_y) ** 2))\nfrom sklearn.metrics import r2_score\nprint(\"R2-score: %.2f\" % r2_score(y_hat , test_y) )\n\nMean absolute error: 0.04\nResidual sum of squares (MSE): 0.00\nR2-score: 0.96"
  },
  {
    "objectID": "posts/linregression/index.html#import-libraries-and-reading-data",
    "href": "posts/linregression/index.html#import-libraries-and-reading-data",
    "title": "Linear Regression",
    "section": "Import Libraries and Reading Data",
    "text": "Import Libraries and Reading Data\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.set_option('display.float_format', lambda x: '%.2f' % x)\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\ndf = pd.read_csv(\"R:/Blog/posts/linregression/Ecommerce Customers.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nEmail\nAddress\nAvatar\nAvg. Session Length\nTime on App\nTime on Website\nLength of Membership\nYearly Amount Spent\n\n\n\n\n0\nmstephenson@fernandez.com\n835 Frank Tunnel\\nWrightmouth, MI 82180-9605\nViolet\n34.50\n12.66\n39.58\n4.08\n587.95\n\n\n1\nhduke@hotmail.com\n4547 Archer Common\\nDiazchester, CA 06566-8576\nDarkGreen\n31.93\n11.11\n37.27\n2.66\n392.20\n\n\n2\npallen@yahoo.com\n24645 Valerie Unions Suite 582\\nCobbborough, D...\nBisque\n33.00\n11.33\n37.11\n4.10\n487.55\n\n\n3\nriverarebecca@gmail.com\n1414 David Throughway\\nPort Jason, OH 22070-1220\nSaddleBrown\n34.31\n13.72\n36.72\n3.12\n581.85\n\n\n4\nmstephens@davidson-herman.com\n14023 Rodriguez Passage\\nPort Jacobville, PR 3...\nMediumAquaMarine\n33.33\n12.80\n37.54\n4.45\n599.41\n\n\n\n\n\n\n\nThe variables to be used in the data were determined.\n\ndf = df.drop(['Email',\"Address\",\"Avatar\"], axis=1)"
  },
  {
    "objectID": "posts/linregression/index.html#advanced-functional-exploratory-data-analysis",
    "href": "posts/linregression/index.html#advanced-functional-exploratory-data-analysis",
    "title": "Linear Regression",
    "section": "Advanced Functional Exploratory Data Analysis",
    "text": "Advanced Functional Exploratory Data Analysis\nGeneral structure of the data is analyzed\n\ndef check_df(dataframe, head=5):\n    print(\"##################### Columns #####################\")\n    print(dataframe.columns)\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(head))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.describe([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n\ncheck_df(df)\n\n##################### Columns #####################\nIndex(['Avg. Session Length', 'Time on App', 'Time on Website',\n       'Length of Membership', 'Yearly Amount Spent'],\n      dtype='object')\n##################### Shape #####################\n(500, 5)\n##################### Types #####################\nAvg. Session Length     float64\nTime on App             float64\nTime on Website         float64\nLength of Membership    float64\nYearly Amount Spent     float64\ndtype: object\n##################### Head #####################\n   Avg. Session Length  Time on App  Time on Website  Length of Membership  \\\n0                34.50        12.66            39.58                  4.08   \n1                31.93        11.11            37.27                  2.66   \n2                33.00        11.33            37.11                  4.10   \n3                34.31        13.72            36.72                  3.12   \n4                33.33        12.80            37.54                  4.45   \n\n   Yearly Amount Spent  \n0               587.95  \n1               392.20  \n2               487.55  \n3               581.85  \n4               599.41  \n##################### Tail #####################\n     Avg. Session Length  Time on App  Time on Website  Length of Membership  \\\n495                33.24        13.57            36.42                  3.75   \n496                34.70        11.70            37.19                  3.58   \n497                32.65        11.50            38.33                  4.96   \n498                33.32        12.39            36.84                  2.34   \n499                33.72        12.42            35.77                  2.74   \n\n     Yearly Amount Spent  \n495               573.85  \n496               529.05  \n497               551.62  \n498               456.47  \n499               497.78  \n##################### NA #####################\nAvg. Session Length     0\nTime on App             0\nTime on Website         0\nLength of Membership    0\nYearly Amount Spent     0\ndtype: int64\n##################### Quantiles #####################\n                      count   mean   std    min     0%     5%    50%    95%  \\\nAvg. Session Length  500.00  33.05  0.99  29.53  29.53  31.45  33.08  34.59   \nTime on App          500.00  12.05  0.99   8.51   8.51  10.53  11.98  13.67   \nTime on Website      500.00  37.06  1.01  33.91  33.91  35.46  37.07  38.78   \nLength of Membership 500.00   3.53  1.00   0.27   0.27   1.81   3.53   5.08   \nYearly Amount Spent  500.00 499.31 79.31 256.67 256.67 376.29 498.89 628.15   \n\n                        99%   100%    max  \nAvg. Session Length   35.43  36.14  36.14  \nTime on App           14.22  15.13  15.13  \nTime on Website       39.25  40.01  40.01  \nLength of Membership   5.84   6.92   6.92  \nYearly Amount Spent  701.00 765.52 765.52  \n\n\nA scatterplot to observe the relationship between the variables was created.\n\nsns.pairplot(df, kind = \"reg\")"
  },
  {
    "objectID": "posts/linregression/index.html#analysis-of-variable-types",
    "href": "posts/linregression/index.html#analysis-of-variable-types",
    "title": "Linear Regression",
    "section": "Analysis of Variable Types",
    "text": "Analysis of Variable Types\nIt is necessary to determine the types of variables. Thus, we can determine the types of the variables and make them suitable for the model.\nIt gives the names of the numeric, categorical but cardinal variables in the data set.\ncat_cols: Categorical variable list\nnum_cols: Numeric variable list\ncat_but_car: Categorical but cardinal variable list\nThe function named grab_col_names helps to determine the types of variables.\n\ndef grab_col_names(dataframe, cat_th=10, car_th=20):\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() &lt; cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() &gt; car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)\n\nObservations: 500\nVariables: 5\ncat_cols: 0\nnum_cols: 5\ncat_but_car: 0\nnum_but_cat: 0"
  },
  {
    "objectID": "posts/linregression/index.html#outlier-analysis",
    "href": "posts/linregression/index.html#outlier-analysis",
    "title": "Linear Regression",
    "section": "Outlier Analysis",
    "text": "Outlier Analysis\nValues that go far beyond the general trend in the data are called outliers. Especially in linear methods, the effects of outliers are more severe.Outliers cause bias in the data set.For all these reasons, it needs to be analyzed.\n\ndef outlier_thresholds(dataframe, col_name, q1=0.10, q3=0.90):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\ndef check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] &gt; up_limit) | (dataframe[col_name] &lt; low_limit)].any(axis=None):\n        return True\n    else:\n        return False\nfor col in num_cols:\n    print(col, check_outlier(df, col))\n\nAvg. Session Length False\nTime on App False\nTime on Website False\nLength of Membership False\nYearly Amount Spent False\n\n\nAvg. Session Length False Time on App False Time on Website False Length of Membership False Yearly Amount Spent False"
  },
  {
    "objectID": "posts/linregression/index.html#analysis-of-missing-values",
    "href": "posts/linregression/index.html#analysis-of-missing-values",
    "title": "Linear Regression",
    "section": "Analysis Of Missing Values",
    "text": "Analysis Of Missing Values\nMissing values may cause problems while setting up the model. It must be detected and necessary actions must be taken.\nNo missing values were found for the relevant data.\n\ndef missing_values_table(dataframe, na_name=False):\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() &gt; 0]\n\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[na_columns].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df, end=\"\\n\")\n\n    if na_name:\n        return na_columns\n\nmissing_values_table(df)\n\nEmpty DataFrame\nColumns: [n_miss, ratio]\nIndex: []\n\n\nEmpty DataFrame Columns: [n_miss, ratio] Index: []"
  },
  {
    "objectID": "posts/linregression/index.html#correlation-analysis",
    "href": "posts/linregression/index.html#correlation-analysis",
    "title": "Linear Regression",
    "section": "Correlation Analysis",
    "text": "Correlation Analysis\nValues with high correlation affect the target variable to a similar extent. Therefore, we can eliminate one of the variables with high correlation between two variables and use the other.\nWhen the data was examined, no variable with a high correlation of more than 90% was found.\n\ndef high_correlated_cols(dataframe, plot=False, corr_th=0.90):\n    corr = dataframe.corr()\n    cor_matrix = corr.abs()\n    upper_triangle_matrix = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(bool))\n    drop_list = [col for col in upper_triangle_matrix.columns if any(upper_triangle_matrix[col] &gt; corr_th)]\n    if plot:\n        import seaborn as sns\n        import matplotlib.pyplot as plt\n        sns.set(rc={'figure.figsize': (10, 5)})\n        sns.heatmap(corr, cmap=\"RdBu\", annot=True)\n        plt.show(block=True)\n    return drop_list\n\n\nhigh_correlated_cols(df,plot=True)\n\n\n\n\n[]"
  },
  {
    "objectID": "posts/linregression/index.html#linear-regression-1",
    "href": "posts/linregression/index.html#linear-regression-1",
    "title": "Linear Regression",
    "section": "",
    "text": "Linear regression models the relationship between dependent and independent variable/variables linearly.\nIn order to create the model, dependent and independent variables were defined.\n\nX = df.drop('Yearly Amount Spent', axis=1) \n\ny = df[[\"Yearly Amount Spent\"]]\n\nBuilding the Model\nBuilding the Model A train and test set by dividing the data into two is created . By training the model with one part and testing the model with the other part, it can be determined how successful the model is.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n\nreg_model = LinearRegression().fit(X_train, y_train)\n\n# constant (b - bias)\nprint(reg_model.intercept_)\n\n# coefficients (w - weights)\nprint(reg_model.coef_)\n\n[-1047.73920526]\n[[25.78854257 38.85150472  0.25638467 61.49204989]]\n\n\nPrediction of dependent variable\n\ny_pred = reg_model.predict(X_test)"
  },
  {
    "objectID": "posts/linregression/index.html#building-the-model",
    "href": "posts/linregression/index.html#building-the-model",
    "title": "Linear Regression",
    "section": "Building the Model",
    "text": "Building the Model\nBuilding the Model A train and test set by dividing the data into two is created . By training the model with one part and testing the model with the other part, it can be determined how successful the model is.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n\nreg_model = LinearRegression().fit(X_train, y_train)\n\n# constant (b - bias)\nprint(reg_model.intercept_)\n\n# coefficients (w - weights)\nprint(reg_model.coef_)\n\n[-1047.73920526]\n[[25.78854257 38.85150472  0.25638467 61.49204989]]\n\n\nPrediction of dependent variable\n\ny_pred = reg_model.predict(X_test)"
  },
  {
    "objectID": "posts/linregression/index.html#evaluating-forecast-success",
    "href": "posts/linregression/index.html#evaluating-forecast-success",
    "title": "Linear Regression",
    "section": "Evaluating Forecast Success",
    "text": "Evaluating Forecast Success\n!(rmse.png)\n\nnp.sqrt(mean_squared_error(y_test, y_pred))\n\n8.84848631350032\n\n\nThe ratio of independent variables description of dependent variable\n\nreg_model.score(X_test, y_test)\n\n0.9892888134002329"
  },
  {
    "objectID": "posts/linregression/index.html#visualization-of-the-model",
    "href": "posts/linregression/index.html#visualization-of-the-model",
    "title": "Linear Regression",
    "section": "Visualization of the Model",
    "text": "Visualization of the Model\nFinally, the actual values corresponding to the predicted values of the model are shown in the graph.\n\ny_pred = pd.DataFrame(y_pred)\ny_test = y_test.reset_index(drop=True)\ndf_ = pd.concat([y_test,y_pred], axis=1)\ndf_.columns = [\"y_test\",\"y_pred\"]\nplt.figure(figsize=(15,10))\nplt.plot(df_)\nplt.legend([\"ACTUAL VALUES\" , \"MODEL PREDICTION\"])\n\n&lt;matplotlib.legend.Legend at 0x178df7dcdc0&gt;"
  },
  {
    "objectID": "posts/outlier-detection/index.html#importing-libraries-and-loading-data",
    "href": "posts/outlier-detection/index.html#importing-libraries-and-loading-data",
    "title": "Anomaly/Outlier detection",
    "section": "Importing libraries and loading data",
    "text": "Importing libraries and loading data\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\n\n\ndf = pd.read_csv(\"R:/Blog/posts/outlier-detection/weight-height.csv\")\nFEATURE_COLUMNS = ['Height', 'Weight']\nprint(df.shape)\ndf.sample(5).T\n\n(10000, 3)\n\n\n\n\n\n\n\n\n\n1137\n5337\n3663\n8935\n9240\n\n\n\n\nGender\nMale\nFemale\nMale\nFemale\nFemale\n\n\nHeight\n69.515659\n63.564994\n67.618846\n66.16168\n58.954923\n\n\nWeight\n185.348837\n134.111374\n190.740873\n141.558942\n99.178984"
  },
  {
    "objectID": "posts/outlier-detection/index.html#interquartile-range-iqr",
    "href": "posts/outlier-detection/index.html#interquartile-range-iqr",
    "title": "Anomaly/Outlier detection",
    "section": "Interquartile range (IQR)",
    "text": "Interquartile range (IQR)\n\ndef plot_results(df, df0, method_str=\"IQR\"):\n    # Plotting the data\n    plt.scatter(df['Height'], df['Weight'], label='Data', alpha=0.25)\n    plt.scatter(df0['Height'], df0['Weight'], color='r', alpha=1, label=f'Outliers, {method_str}')\n    plt.xlabel('Height')\n    plt.ylabel('Weight')\n    plt.title(f'Outlier Detection using {method_str}')\n    plt.legend()\n    plt.show();\n    return None\n\n\ndef detect_outliers_iqr(dataframe, column):\n    '''Uses Interquartile range (IQR) method to detect outliers'''\n    # Calculate the first quartile (Q1)\n    Q1 = dataframe[column].quantile(0.25)\n    # Calculate the third quartile (Q3)\n    Q3 = dataframe[column].quantile(0.75)\n    # Calculate the interquartile range (IQR)\n    IQR = Q3 - Q1\n    # Define the lower and upper bounds for outliers\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    # Find the outliers\n    dataframe[f'outliers_iqr_{column}'] = ((dataframe[column] &lt; lower_bound) | (dataframe[column] &gt; upper_bound)).astype(int)\n    return dataframe\n\n\nfor col in ['Weight', 'Height']:\n    df = detect_outliers_iqr(df, col)\ndf0 = df[df['outliers_iqr_Height']+df['outliers_iqr_Weight']&gt;0].copy()\nprint(df0.shape)\nplot_results(df, df0, method_str=\"IQR\")\n\n(8, 5)"
  },
  {
    "objectID": "posts/outlier-detection/index.html#isolation-forest",
    "href": "posts/outlier-detection/index.html#isolation-forest",
    "title": "Anomaly/Outlier detection",
    "section": "Isolation forest",
    "text": "Isolation forest\n\ndef detect_outliers_isolation_forest(dataframe):\n    # Create an Isolation Forest instance\n    clf = IsolationForest(contamination=0.01, n_estimators=100, bootstrap=False, random_state=42)\n    # Fit the model\n    clf.fit(dataframe[FEATURE_COLUMNS])\n    # Predict outliers\n    outliers = clf.predict(dataframe[FEATURE_COLUMNS])\n    # Create a boolean mask for outliers\n    outliers_mask = outliers == -1\n    # Mark the outliers\n    dataframe[f'outliers_iforest'] = (outliers_mask).astype(int)\n    return dataframe\n\n# Detect outliers using the IF method on column\ndf = detect_outliers_isolation_forest(df)\n\nprint(\"Outliers:\")\n    \n# Plotting the data\ndf0 = df[df['outliers_iforest']&gt;0].copy()\nprint(df0.shape)\nplot_results(df, df0, method_str=\"Isolation Forest\")\ndf.describe()\n\nOutliers:\n(100, 6)\n\n\n\n\n\n\n\n\n\n\n\n\nHeight\nWeight\noutliers_iqr_Weight\noutliers_iqr_Height\noutliers_iforest\n\n\n\n\ncount\n10000.000000\n10000.000000\n10000.0000\n10000.000000\n10000.000000\n\n\nmean\n66.367560\n161.440357\n0.0001\n0.000800\n0.010000\n\n\nstd\n3.847528\n32.108439\n0.0100\n0.028274\n0.099504\n\n\nmin\n54.263133\n64.700127\n0.0000\n0.000000\n0.000000\n\n\n25%\n63.505620\n135.818051\n0.0000\n0.000000\n0.000000\n\n\n50%\n66.318070\n161.212928\n0.0000\n0.000000\n0.000000\n\n\n75%\n69.174262\n187.169525\n0.0000\n0.000000\n0.000000\n\n\nmax\n78.998742\n269.989699\n1.0000\n1.000000\n1.000000"
  },
  {
    "objectID": "posts/outlier-detection/index.html#local-outlier-factor",
    "href": "posts/outlier-detection/index.html#local-outlier-factor",
    "title": "Anomaly/Outlier detection",
    "section": "Local Outlier Factor",
    "text": "Local Outlier Factor\n\ndef detect_outliers_lof(dataframe):\n    # Create an LOF instance\n    lof = LocalOutlierFactor(n_neighbors=20, contamination='auto')\n    # Fit the model and predict outlier scores\n    outlier_scores = lof.fit_predict(dataframe[FEATURE_COLUMNS])\n    # Create a boolean mask for outliers\n    outliers_mask = outlier_scores == -1\n    # Mark the outliers\n    dataframe[f'outliers_lof'] = (outliers_mask).astype(int)\n    return dataframe\n\n# Detect outliers using the LOF method on\ndf = detect_outliers_lof(df)\n\nprint(\"Outliers:\")\n    \n# Plotting the data\ndf0 = df[df['outliers_lof']+df['outliers_lof']&gt;0].copy()\nprint(df0.shape)\nplot_results(df, df0, method_str=\"LocalOutlierFactor\")\n\ndf.describe()\n\nOutliers:\n(132, 7)\n\n\n\n\n\n\n\n\n\n\n\n\nHeight\nWeight\noutliers_iqr_Weight\noutliers_iqr_Height\noutliers_iforest\noutliers_lof\n\n\n\n\ncount\n10000.000000\n10000.000000\n10000.0000\n10000.000000\n10000.000000\n10000.000000\n\n\nmean\n66.367560\n161.440357\n0.0001\n0.000800\n0.010000\n0.013200\n\n\nstd\n3.847528\n32.108439\n0.0100\n0.028274\n0.099504\n0.114136\n\n\nmin\n54.263133\n64.700127\n0.0000\n0.000000\n0.000000\n0.000000\n\n\n25%\n63.505620\n135.818051\n0.0000\n0.000000\n0.000000\n0.000000\n\n\n50%\n66.318070\n161.212928\n0.0000\n0.000000\n0.000000\n0.000000\n\n\n75%\n69.174262\n187.169525\n0.0000\n0.000000\n0.000000\n0.000000\n\n\nmax\n78.998742\n269.989699\n1.0000\n1.000000\n1.000000\n1.000000"
  },
  {
    "objectID": "posts/outlier-detection/index.html#one-class-svm",
    "href": "posts/outlier-detection/index.html#one-class-svm",
    "title": "Anomaly/Outlier detection",
    "section": "One-class SVM",
    "text": "One-class SVM\n\ndef detect_outliers_svm(dataframe):\n    # Create a One-Class SVM instance\n    svm = OneClassSVM(nu=0.01)\n    # Fit the model\n    svm.fit(dataframe[FEATURE_COLUMNS])\n    # Predict outlier scores\n    outlier_scores = svm.decision_function(dataframe[FEATURE_COLUMNS])\n    # Create a boolean mask for outliers\n    outliers_mask = outlier_scores &lt; 0\n    # Mark the outliers\n    dataframe[f'outliers_svm'] = (outliers_mask).astype(int)\n    return dataframe\n\n# Detect outliers using the SVM method\ndf = detect_outliers_svm(df)\n\nprint(\"Outliers:\")\n    \n# Plotting the data\ndf0 = df[df['outliers_svm']&gt;0].copy()\nprint(df0.shape)\nplot_results(df, df0, method_str=\"One-class SVM\")\ndf.describe()\n\nOutliers:\n(101, 8)\n\n\n\n\n\n\n\n\n\n\n\n\nHeight\nWeight\noutliers_iqr_Weight\noutliers_iqr_Height\noutliers_iforest\noutliers_lof\noutliers_svm\n\n\n\n\ncount\n10000.000000\n10000.000000\n10000.0000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n\n\nmean\n66.367560\n161.440357\n0.0001\n0.000800\n0.010000\n0.013200\n0.010100\n\n\nstd\n3.847528\n32.108439\n0.0100\n0.028274\n0.099504\n0.114136\n0.099995\n\n\nmin\n54.263133\n64.700127\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n63.505620\n135.818051\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n66.318070\n161.212928\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n75%\n69.174262\n187.169525\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nmax\n78.998742\n269.989699\n1.0000\n1.000000\n1.000000\n1.000000\n1.000000"
  },
  {
    "objectID": "posts/outlier-detection/index.html#autoencoder",
    "href": "posts/outlier-detection/index.html#autoencoder",
    "title": "Anomaly/Outlier detection",
    "section": "Autoencoder",
    "text": "Autoencoder\n\nclass Autoencoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super(Autoencoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, 2*hidden_dim),\n            nn.ReLU(),\n            nn.Linear(2*hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(hidden_dim, 2*hidden_dim),\n            nn.ReLU(),\n            nn.Linear(2*hidden_dim, input_dim),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n\nclass OutlierDataset(Dataset):\n    def __init__(self, data):\n        self.data = torch.tensor(data, dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\ndef detect_outliers_autoencoder(dataframe, hidden_dim=16, num_epochs=10, batch_size=32):\n    # Convert dataframe to standard scaled numpy array\n    scaler = StandardScaler()\n    data = scaler.fit_transform(dataframe[FEATURE_COLUMNS])\n\n    # Create an outlier dataset\n    dataset = OutlierDataset(data)\n\n    # Split data into training and validation sets\n    val_split = int(0.2 * len(dataset))\n    train_set, val_set = torch.utils.data.random_split(dataset, [len(dataset) - val_split, val_split])\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_set, batch_size=batch_size)\n\n    # Initialize the autoencoder model\n    input_dim = data.shape[1]\n    model = Autoencoder(input_dim, hidden_dim)\n\n    # Set the loss function and optimizer\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=3e-2, weight_decay=1e-8)\n\n    # Train the autoencoder\n    for epoch in range(num_epochs):\n        train_loss = 0.0\n        val_loss = 0.0\n\n        # Training\n        model.train()\n        for batch in train_loader:\n            # Zero the gradients\n            optimizer.zero_grad()\n            # Forward pass\n            outputs = model(batch)\n            # Compute the reconstruction loss\n            loss = criterion(outputs, batch)\n            # Backward pass and optimization\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * batch.size(0)\n\n        # Validation\n        model.eval()\n        with torch.no_grad():\n            for batch in val_loader:\n                outputs = model(batch)\n                loss = criterion(outputs, batch)\n                val_loss += loss.item() * batch.size(0)\n\n        train_loss /= len(train_set)\n        val_loss /= len(val_set)\n\n\n        print(f'Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n        \n    # Calculate reconstruction error for each data point\n    reconstructed = model(dataset.data)\n    mse_loss = nn.MSELoss(reduction='none')\n    error = torch.mean(mse_loss(reconstructed, dataset.data), dim=1)\n\n    \n    # Define a threshold for outlier detection\n    threshold = np.percentile(error.detach().numpy(), 99)\n\n    # Create a boolean mask for outliers\n    outliers_mask = (error &gt; threshold)\n\n    # Mark the outliers\n    dataframe[f'outliers_autoenc'] = (outliers_mask)\n    dataframe[f'outliers_autoenc'] = dataframe[f'outliers_autoenc'].astype(int)\n    return dataframe\n\n# Detect outliers using the autoencoder method\ndf = detect_outliers_autoencoder(df)\n\nprint(\"Outliers:\")\n    \n# Plotting the data\ndf0 = df[df['outliers_autoenc']&gt;0].copy()\nprint(df0.shape)\nplot_results(df, df0, method_str=\"One-class Autoencoders\")\n\ndf.describe()\n\nEpoch 1/10: Train Loss: 0.5812, Val Loss: 0.5335\nEpoch 2/10: Train Loss: 0.5608, Val Loss: 0.5325\nEpoch 3/10: Train Loss: 0.5601, Val Loss: 0.5323\nEpoch 4/10: Train Loss: 0.5599, Val Loss: 0.5323\nEpoch 5/10: Train Loss: 0.5600, Val Loss: 0.5322\nEpoch 6/10: Train Loss: 0.5599, Val Loss: 0.5324\nEpoch 7/10: Train Loss: 0.5604, Val Loss: 0.5325\nEpoch 8/10: Train Loss: 0.5600, Val Loss: 0.5322\nEpoch 9/10: Train Loss: 0.5597, Val Loss: 0.5321\nEpoch 10/10: Train Loss: 0.5597, Val Loss: 0.5322\nOutliers:\n(100, 9)\n\n\n\n\n\n\n\n\n\n\n\n\nHeight\nWeight\noutliers_iqr_Weight\noutliers_iqr_Height\noutliers_iforest\noutliers_lof\noutliers_svm\noutliers_autoenc\n\n\n\n\ncount\n10000.000000\n10000.000000\n10000.0000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n\n\nmean\n66.367560\n161.440357\n0.0001\n0.000800\n0.010000\n0.013200\n0.010100\n0.010000\n\n\nstd\n3.847528\n32.108439\n0.0100\n0.028274\n0.099504\n0.114136\n0.099995\n0.099504\n\n\nmin\n54.263133\n64.700127\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n63.505620\n135.818051\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n66.318070\n161.212928\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n75%\n69.174262\n187.169525\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nmax\n78.998742\n269.989699\n1.0000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000"
  },
  {
    "objectID": "posts/classification/index.html#definition-and-overview-of-classification",
    "href": "posts/classification/index.html#definition-and-overview-of-classification",
    "title": "Classification",
    "section": "Definition and Overview of Classification",
    "text": "Definition and Overview of Classification\nClassification in machine learning and statistics is a supervised learning approach where the objective is to categorize data into predefined classes. In simpler terms, it involves deciding which category or class a new observation belongs to, based on a training set of data containing observations whose category membership is known."
  },
  {
    "objectID": "posts/classification/index.html#types-of-classification",
    "href": "posts/classification/index.html#types-of-classification",
    "title": "Classification",
    "section": "Types of Classification",
    "text": "Types of Classification\nBinary Classification: Involves two classes. Common examples include spam detection (spam or not spam) and medical diagnoses (sick or healthy).\nMulticlass Classification: Involves more than two classes, but each instance is assigned to only one class. An example would be classifying types of fruits.\nMultilabel Classification: Each instance may be assigned to multiple classes. For example, a news article might be categorized into multiple genres like sports, politics, and finance."
  },
  {
    "objectID": "posts/classification/index.html#applications",
    "href": "posts/classification/index.html#applications",
    "title": "Classification",
    "section": "Applications",
    "text": "Applications\nMedical Diagnosis: Identifying diseases based on symptoms and test results.\nSpam Filtering: Categorizing emails as spam or non-spam.\nSentiment Analysis: Classifying the sentiment of text data (positive, negative, neutral).\nImage Recognition: Categorizing images into various classes like animals, objects, etc. Credit Scoring: Assessing creditworthiness as high-risk or low-risk.\nOnce trained, the algorithm can classify new, unseen data by assessing its similarity to the patterns it has learned. It predicts the likelihood of the new data point falling into one of the predefined categories. This process is akin to your email provider recognizing whether an incoming email is spam or not based on past experiences."
  },
  {
    "objectID": "posts/classification/index.html#classification-algorithms",
    "href": "posts/classification/index.html#classification-algorithms",
    "title": "Classification",
    "section": "Classification Algorithms",
    "text": "Classification Algorithms\n\nLogistic Regression:\nOverview: Logistic Regression is used for binary classification problems. It models the probability that each input belongs to a particular category. Mechanism: This algorithm uses a logistic function to squeeze the output of a linear equation between 0 and 1. The result is the probability that the given input point belongs to a certain class.\n\n\n\nImage Source: https://editor.analyticsvidhya.com/uploads/82109Webp.net-resize.jpg\n\n\nPros: Simple and efficient. Provides a probability score for observations. Low variance, avoiding overfitting.\nCons: Struggles with non-linear data. Assumes no missing values and that predictors are independent.\nApplications: Commonly used in fields like credit scoring, medical fields for disease diagnosis, and predictive analytics.\n\n\nNaive Bayes:\nOverview: Based on Bayes’ Theorem, it assumes independence among predictors. Mechanism: It calculates the probability of each class and the conditional probability of each class given each input value. These probabilities are then used to classify the data.\n\n\n\nImage Source: https://av-eks-blogoptimized.s3.amazonaws.com/Bayes_rule-300x172-300x172-111664.png\n\n\nPros: Fast and efficient. Performs well with a smaller amount of data. Handles multi-class prediction problems well.\nCons: The assumption of independent features is often unrealistic. Can be outperformed by more complex models.\nApplications: Widely used in spam filtering, text analysis, and sentiment analysis.\n\n\nK-Nearest Neighbors (KNN):\nOverview: A non-parametric method used for classification and regression. Mechanism: Classifies data based on how its neighbors are classified. It finds the K nearest points to the new data point and classifies it based on the majority class of these points.\n\n\n\nImage Source: https://static.javatpoint.com/tutorial/machine-learning/images/k-nearest-neighbor-algorithm-for-machine-learning2.png\n\n\nPros: Simple and intuitive. No need to build a model or tune parameters. Flexible to feature/distance choices.\nCons: Slows significantly as data size increases. Sensitive to irrelevant or redundant features.\nApplications: Used in recommendation systems, image recognition, and more.\n\n\nSupport Vector Machine (SVM):\nOverview: Effective in high dimensional spaces and best suited for binary classification. Mechanism: Constructs a hyperplane in a multidimensional space to separate different classes. SVM finds the best margin (distance between the line and the support vectors) to separate the classes.\n\n\n\nImage Source: https://static.javatpoint.com/tutorial/machine-learning/images/support-vector-machine-algorithm5.png\n\n\nPros: Effective in high-dimensional spaces. Uses a subset of training points (support vectors), so it’s memory efficient.\nCons: Not suitable for larger datasets. Does not perform well with noisy data.\nApplications: Used in face detection, text and hypertext categorization, classification of images.\n\n\nDecision Tree:\nOverview: A tree-structure algorithm, where each node represents a feature, each branch a decision rule, and each leaf a class. Mechanism: Splits the data into subsets based on feature values. This process is repeated recursively, resulting in a tree with decision nodes and leaf nodes.\n\n\n\nImage Source: https://lh4.googleusercontent.com/v9UQUwaQTAXVH90b-Ugyw2_61_uErfYvTBtG-RNRNB_eHUFq9AmAN_2IOdfOETnbXImnQVN-wPC7_YzDgf7urCeyhyx5UZmuSwV8BVsV8VnHxl1KtgpuxDifJ4pLE23ooYXLlnc\n\n\nPros: Easy to interpret and explain. Requires little data preparation. Can handle both numerical and categorical data.\nCons: Prone to overfitting, especially with complex trees. Small changes in data can lead to a different tree.\nApplications: Used in customer segmentation, fraud detection, and risk assessment."
  },
  {
    "objectID": "posts/classification/index.html#key-components",
    "href": "posts/classification/index.html#key-components",
    "title": "Classification",
    "section": "Key Components",
    "text": "Key Components\nClasses or Categories: These are the distinct groups or categories that data points are classified into. For instance, in a binary classification, there are two classes, while in multi-class classification, there could be three or more.\nFeatures: These are individual independent variables that act as the input for the process. Each feature contributes to determining the output class.\nLabels: In the training dataset, each data point is tagged with the correct label, which the algorithm then learns to predict."
  },
  {
    "objectID": "posts/classification/index.html#naive-bayes",
    "href": "posts/classification/index.html#naive-bayes",
    "title": "Classification",
    "section": "1. Naive Bayes",
    "text": "1. Naive Bayes\nOverview: Based on Bayes’ Theorem, it assumes independence among predictors. Mechanism: It calculates the probability of each class and the conditional probability of each class given each input value. These probabilities are then used to classify the data.\n\n\n\nImage Source: https://av-eks-blogoptimized.s3.amazonaws.com/Bayes_rule-300x172-300x172-111664.png\n\n\nPros: Fast and efficient. Performs well with a smaller amount of data. Handles multi-class prediction problems well.\nCons: The assumption of independent features is often unrealistic. Can be outperformed by more complex models.\nApplications: Widely used in spam filtering, text analysis, and sentiment analysis.\n\nK-Nearest Neighbors (KNN) Overview: A non-parametric method used for classification and regression. Mechanism: Classifies data based on how its neighbors are classified. It finds the K nearest points to the new data point and classifies it based on the majority class of these points.\n\n\n\n\nImage Source: https://static.javatpoint.com/tutorial/machine-learning/images/k-nearest-neighbor-algorithm-for-machine-learning2.png\n\n\nPros: Simple and intuitive. No need to build a model or tune parameters. Flexible to feature/distance choices.\nCons: Slows significantly as data size increases. Sensitive to irrelevant or redundant features.\nApplications: Used in recommendation systems, image recognition, and more.\n\nSupport Vector Machine (SVM) Overview: Effective in high dimensional spaces and best suited for binary classification. Mechanism: Constructs a hyperplane in a multidimensional space to separate different classes. SVM finds the best margin (distance between the line and the support vectors) to separate the classes.\n\n\n\n\nImage Source: https://static.javatpoint.com/tutorial/machine-learning/images/support-vector-machine-algorithm5.png\n\n\nPros: Effective in high-dimensional spaces. Uses a subset of training points (support vectors), so it’s memory efficient.\nCons: Not suitable for larger datasets. Does not perform well with noisy data.\nApplications: Used in face detection, text and hypertext categorization, classification of images.\n\nDecision Tree Overview: A tree-structure algorithm, where each node represents a feature, each branch a decision rule, and each leaf a class. Mechanism: Splits the data into subsets based on feature values. This process is repeated recursively, resulting in a tree with decision nodes and leaf nodes.\n\n\n\n\nImage Source: https://lh4.googleusercontent.com/v9UQUwaQTAXVH90b-Ugyw2_61_uErfYvTBtG-RNRNB_eHUFq9AmAN_2IOdfOETnbXImnQVN-wPC7_YzDgf7urCeyhyx5UZmuSwV8BVsV8VnHxl1KtgpuxDifJ4pLE23ooYXLlnc\n\n\nPros: Easy to interpret and explain. Requires little data preparation. Can handle both numerical and categorical data.\nCons: Prone to overfitting, especially with complex trees. Small changes in data can lead to a different tree.\nApplications: Used in customer segmentation, fraud detection, and risk assessment."
  },
  {
    "objectID": "posts/outlier-detection/index.html#anomalyoutlier-detection-an-overview",
    "href": "posts/outlier-detection/index.html#anomalyoutlier-detection-an-overview",
    "title": "Anomaly/Outlier detection",
    "section": "Anomaly/Outlier Detection: An Overview",
    "text": "Anomaly/Outlier Detection: An Overview\nAnomaly detection, also known as outlier detection, is a process in data analysis where unusual patterns, items, or events in a dataset are identified. These anomalies are often referred to as outliers—data points that deviate significantly from the majority of the data. Anomaly detection is crucial as these outliers can indicate critical incidents, such as bank fraud, structural defects, system faults, or errors in text data."
  },
  {
    "objectID": "posts/outlier-detection/index.html#key-concepts",
    "href": "posts/outlier-detection/index.html#key-concepts",
    "title": "Anomaly/Outlier detection",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nAnomalies:\nA data point or a pattern that does not conform to the expected behavior. Anomalies can be:\nPoint Anomalies: Individual data points that are significantly different from the rest of the data. Contextual Anomalies: Anomalies that are context-specific. These might not be outliers in a different context. Collective Anomalies: A group of data points that collectively deviate from the overall data pattern but might not be anomalies when considered individually.\n\n\nOutlier:\nOften used interchangeably with anomalies, outliers are data points that differ drastically from the rest of the dataset."
  },
  {
    "objectID": "posts/outlier-detection/index.html#techniques-in-anomaly-detection",
    "href": "posts/outlier-detection/index.html#techniques-in-anomaly-detection",
    "title": "Anomaly/Outlier detection",
    "section": "Techniques in Anomaly Detection",
    "text": "Techniques in Anomaly Detection\nAnomaly detection in machine learning can be approached in various ways:\n\nStatistical Methods:\n\nSimple statistical metrics like mean, median, standard deviation, and interquartile ranges are used to identify outliers.\nMethods like Z-score and Grubbs’ test can flag data points that deviate from the expected distribution.\n\n\n\nMachine Learning Methods:\n\nSupervised Learning: This requires a dataset with labeled anomalies. Algorithms like logistic regression, support vector machines, or neural networks can be trained to recognize and predict anomalies.\nUnsupervised Learning: Useful when you don’t have labeled data. Algorithms like k-means clustering, DBSCAN, or autoencoders can detect anomalies by understanding the data’s inherent structure and distribution.\nSemi-Supervised Learning: Involves training on a primarily normal dataset to understand the pattern of normality, against which anomalies can be detected.\n\n\n\nProximity-Based Methods:\nThese methods identify anomalies based on the distance or similarity between data points. For example, k-nearest neighbors (KNN) can be used to detect points that are far from their neighbors.\n\n\nDensity-Based Methods:\nTechniques like Local Outlier Factor (LOF) focus on the density of the area around a data point. Anomalies are typically located in low-density regions.\n\n\nClustering-Based Methods:\nAlgorithms like K-means or Hierarchical Clustering can group similar data together. Points that do not fit well into any cluster may be considered anomalies."
  },
  {
    "objectID": "posts/outlier-detection/index.html#applications-of-anomaly-detection",
    "href": "posts/outlier-detection/index.html#applications-of-anomaly-detection",
    "title": "Anomaly/Outlier detection",
    "section": "Applications of Anomaly Detection",
    "text": "Applications of Anomaly Detection\nFraud Detection: In banking and finance, detecting unusual transactions that could indicate fraud.\nIntrusion Detection: In cybersecurity, identifying unusual patterns that could signify a security breach.\nFault Detection: In industrial settings, detecting irregularities in machine or system operations to preempt failures.\nHealth Monitoring: In healthcare, identifying unusual patterns in patient data that could indicate medical issues.\nQuality Control: In manufacturing, detecting products that don’t meet quality standards."
  },
  {
    "objectID": "posts/outlier-detection/index.html#anomaly-detection-method-with-weight-and-height-dataset",
    "href": "posts/outlier-detection/index.html#anomaly-detection-method-with-weight-and-height-dataset",
    "title": "Anomaly/Outlier detection",
    "section": "Anomaly Detection Method with Weight and Height Dataset",
    "text": "Anomaly Detection Method with Weight and Height Dataset\nImporting libraries and loading data\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\n\n\ndf = pd.read_csv(\"R:/Blog/posts/outlier-detection/weight-height.csv\")\nFEATURE_COLUMNS = ['Height', 'Weight']\nprint(df.shape)\ndf.sample(5).T\n\n(10000, 3)\n\n\n\n\n\n\n\n\n\n6632\n8938\n5752\n1552\n5076\n\n\n\n\nGender\nFemale\nFemale\nFemale\nMale\nFemale\n\n\nHeight\n68.780123\n69.470987\n61.822956\n70.837159\n65.065145\n\n\nWeight\n176.450818\n178.981464\n151.621748\n201.403402\n156.163127"
  },
  {
    "objectID": "posts/linregression/index.html#definition",
    "href": "posts/linregression/index.html#definition",
    "title": "Linear Regression",
    "section": "Definition:",
    "text": "Definition:\n\nLinear regression is a statistical method used in data science and machine learning for predictive analysis.\nIt establishes a linear relationship between an independent variable (predictor) and a dependent variable (outcome) for prediction.\nThe method is suitable for continuous or numeric variables like sales, salary, age, etc."
  },
  {
    "objectID": "posts/linregression/index.html#importance-in-various-fields",
    "href": "posts/linregression/index.html#importance-in-various-fields",
    "title": "Linear Regression",
    "section": "Importance in Various Fields:",
    "text": "Importance in Various Fields:\n\nUsed in stock market forecasting, portfolio management, scientific analysis, and more.\nA simple representation is a sloped straight line in a graph, depicting the best fit line for a set of data."
  },
  {
    "objectID": "posts/linregression/index.html#benefits-of-linear-regression",
    "href": "posts/linregression/index.html#benefits-of-linear-regression",
    "title": "Linear Regression",
    "section": "Benefits of Linear Regression:",
    "text": "Benefits of Linear Regression:\n\nEasy to implement and interpret.\nScalable and optimal for online settings due to its computational efficiency."
  },
  {
    "objectID": "posts/linregression/index.html#linear-regression-equation",
    "href": "posts/linregression/index.html#linear-regression-equation",
    "title": "Linear Regression",
    "section": "Linear Regression Equation:",
    "text": "Linear Regression Equation:\nThe equation Y = mX + b, where ‘m’ is the slope and ‘b’ is the intercept, describes the relationship. In machine learning, it’s often expressed as y(x) = p0 + p1  x, where p0 and p1 are parameters to be determined."
  },
  {
    "objectID": "posts/linregression/index.html#types-of-linear-regression",
    "href": "posts/linregression/index.html#types-of-linear-regression",
    "title": "Linear Regression",
    "section": "Types of Linear Regression:",
    "text": "Types of Linear Regression:\n\nSimple Linear Regression:\nDefinition: Simple Linear Regression is a statistical method that allows us to summarize and study relationships between two continuous (quantitative) variables. This method assumes a linear relationship between the dependent variable and the independent variable.\nEquation: Y = β0 + β1 * X + ε\n\nY: Dependent variable\nX: Independent variable\nβ0: Intercept of the regression line\nβ1: Slope of the regression line\nε: Error term\n\nApplication: For example, predicting the price of a house (dependent variable) based on its size (independent variable)."
  },
  {
    "objectID": "posts/linregression/index.html#multiple-linear-regression",
    "href": "posts/linregression/index.html#multiple-linear-regression",
    "title": "Linear Regression",
    "section": "Multiple Linear Regression:",
    "text": "Multiple Linear Regression:\nDefinition: Multiple Linear Regression is an extension of simple linear regression and is used to predict the outcome of a dependent variable based on the value of two or more independent variables. This method helps in understanding how changes in independent variables are associated with changes in the dependent variable.\nEquation: Y = β0 + β1 * X1 + β2 * X2 + … + βn * Xn + ε\n\nY: Dependent variable\nX1, X2, …, Xn: Independent variables\nβ0: Intercept\nβ1, β2, …, βn: Slopes for each independent variable\nε: Error term\n\nApplication: Predicting a person’s weight based on their height, age, and diet (three independent variables)."
  },
  {
    "objectID": "posts/linregression/index.html#logistic-regression",
    "href": "posts/linregression/index.html#logistic-regression",
    "title": "Linear Regression",
    "section": "Logistic Regression:",
    "text": "Logistic Regression:\nDefinition: Logistic Regression is a statistical method for analyzing a dataset in which there are one or more independent variables that determine an outcome. The outcome is measured with a dichotomous variable (in which there are only two possible outcomes). It is used for predicting the probability of a binary outcome based on one or more predictor variables.\nEquation: log(p/(1-p)) = β0 + β1 * X1 + … + βn * Xn\n\np: Probability of the dependent event occurring\nX1, …, Xn: Predictor variables\nβ0: Intercept\nβ1, …, βn: Coefficients for each predictor\n\nApplication: Determining whether an email is spam or not spam, based on features like the email’s content, sender, etc."
  },
  {
    "objectID": "posts/linregression/index.html#ordinal-regression",
    "href": "posts/linregression/index.html#ordinal-regression",
    "title": "Linear Regression",
    "section": "Ordinal Regression:",
    "text": "Ordinal Regression:\nDefinition: Ordinal Regression is used when the dependent variable is ordinal, meaning it reflects a scale of magnitude. It models the relationship between a set of predictor variables and an ordinal scale dependent variable, where the categories have a natural order, but the intervals between them are not assumed to be equidistant.\nCharacteristics: The categories have a ranked order, but the intervals between the ranks are not necessarily equal.\nApplication: Rating a movie as poor, fair, good, very good, and excellent. Here, the ratings have a natural order but the difference between each category is not quantified."
  },
  {
    "objectID": "posts/linregression/index.html#multinomial-logistic-regression",
    "href": "posts/linregression/index.html#multinomial-logistic-regression",
    "title": "Linear Regression",
    "section": "Multinomial Logistic Regression:",
    "text": "Multinomial Logistic Regression:\nDefinition: Multinomial Logistic Regression is a classification method that generalizes logistic regression to multiclass problems, i.e., where the dependent variable can have more than two possible nominal (unordered) outcomes. It is used when the outcome involves more than two categories.\nCharacteristics: Similar to logistic regression, but suitable for more than two classes.\nApplication: Predicting the choice of transportation (like car, bus, train, bike) based on factors like distance, cost, and time."
  },
  {
    "objectID": "posts/linregression/index.html#example-yearly-amount-spent-prediction",
    "href": "posts/linregression/index.html#example-yearly-amount-spent-prediction",
    "title": "Linear Regression",
    "section": "Example: Yearly Amount Spent Prediction",
    "text": "Example: Yearly Amount Spent Prediction\nImport Libraries and Reading Data\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.set_option('display.float_format', lambda x: '%.2f' % x)\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\ndf = pd.read_csv(\"R:/Blog/posts/linregression/Ecommerce Customers.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nEmail\nAddress\nAvatar\nAvg. Session Length\nTime on App\nTime on Website\nLength of Membership\nYearly Amount Spent\n\n\n\n\n0\nmstephenson@fernandez.com\n835 Frank Tunnel\\nWrightmouth, MI 82180-9605\nViolet\n34.50\n12.66\n39.58\n4.08\n587.95\n\n\n1\nhduke@hotmail.com\n4547 Archer Common\\nDiazchester, CA 06566-8576\nDarkGreen\n31.93\n11.11\n37.27\n2.66\n392.20\n\n\n2\npallen@yahoo.com\n24645 Valerie Unions Suite 582\\nCobbborough, D...\nBisque\n33.00\n11.33\n37.11\n4.10\n487.55\n\n\n3\nriverarebecca@gmail.com\n1414 David Throughway\\nPort Jason, OH 22070-1220\nSaddleBrown\n34.31\n13.72\n36.72\n3.12\n581.85\n\n\n4\nmstephens@davidson-herman.com\n14023 Rodriguez Passage\\nPort Jacobville, PR 3...\nMediumAquaMarine\n33.33\n12.80\n37.54\n4.45\n599.41\n\n\n\n\n\n\n\nThe variables to be used in the data were determined.\n\ndf = df.drop(['Email',\"Address\",\"Avatar\"], axis=1) \n\nAdvanced Functional Exploratory Data Analysis\nGeneral structure of the data is analyzed\n\ndef check_df(dataframe, head=5):\n    print(\"##################### Columns #####################\")\n    print(dataframe.columns)\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(head))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.describe([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n\ncheck_df(df)\n\n##################### Columns #####################\nIndex(['Avg. Session Length', 'Time on App', 'Time on Website',\n       'Length of Membership', 'Yearly Amount Spent'],\n      dtype='object')\n##################### Shape #####################\n(500, 5)\n##################### Types #####################\nAvg. Session Length     float64\nTime on App             float64\nTime on Website         float64\nLength of Membership    float64\nYearly Amount Spent     float64\ndtype: object\n##################### Head #####################\n   Avg. Session Length  Time on App  Time on Website  Length of Membership  \\\n0                34.50        12.66            39.58                  4.08   \n1                31.93        11.11            37.27                  2.66   \n2                33.00        11.33            37.11                  4.10   \n3                34.31        13.72            36.72                  3.12   \n4                33.33        12.80            37.54                  4.45   \n\n   Yearly Amount Spent  \n0               587.95  \n1               392.20  \n2               487.55  \n3               581.85  \n4               599.41  \n##################### Tail #####################\n     Avg. Session Length  Time on App  Time on Website  Length of Membership  \\\n495                33.24        13.57            36.42                  3.75   \n496                34.70        11.70            37.19                  3.58   \n497                32.65        11.50            38.33                  4.96   \n498                33.32        12.39            36.84                  2.34   \n499                33.72        12.42            35.77                  2.74   \n\n     Yearly Amount Spent  \n495               573.85  \n496               529.05  \n497               551.62  \n498               456.47  \n499               497.78  \n##################### NA #####################\nAvg. Session Length     0\nTime on App             0\nTime on Website         0\nLength of Membership    0\nYearly Amount Spent     0\ndtype: int64\n##################### Quantiles #####################\n                      count   mean   std    min     0%     5%    50%    95%  \\\nAvg. Session Length  500.00  33.05  0.99  29.53  29.53  31.45  33.08  34.59   \nTime on App          500.00  12.05  0.99   8.51   8.51  10.53  11.98  13.67   \nTime on Website      500.00  37.06  1.01  33.91  33.91  35.46  37.07  38.78   \nLength of Membership 500.00   3.53  1.00   0.27   0.27   1.81   3.53   5.08   \nYearly Amount Spent  500.00 499.31 79.31 256.67 256.67 376.29 498.89 628.15   \n\n                        99%   100%    max  \nAvg. Session Length   35.43  36.14  36.14  \nTime on App           14.22  15.13  15.13  \nTime on Website       39.25  40.01  40.01  \nLength of Membership   5.84   6.92   6.92  \nYearly Amount Spent  701.00 765.52 765.52  \n\n\nA scatterplot to observe the relationship between the variables was created.\n\nsns.pairplot(df, kind = \"reg\")\n\n\n\n\nAnalysis of Variable Types\nIt is necessary to determine the types of variables. Thus, we can determine the types of the variables and make them suitable for the model.\nIt gives the names of the numeric, categorical but cardinal variables in the data set.\ncat_cols: Categorical variable list\nnum_cols: Numeric variable list\ncat_but_car: Categorical but cardinal variable list\nThe function named grab_col_names helps to determine the types of variables.\n\ndef grab_col_names(dataframe, cat_th=10, car_th=20):\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() &lt; cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() &gt; car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)\n\nObservations: 500\nVariables: 5\ncat_cols: 0\nnum_cols: 5\ncat_but_car: 0\nnum_but_cat: 0\n\n\nOutlier Analysis\nValues that go far beyond the general trend in the data are called outliers. Especially in linear methods, the effects of outliers are more severe.Outliers cause bias in the data set.For all these reasons, it needs to be analyzed.\n\ndef outlier_thresholds(dataframe, col_name, q1=0.10, q3=0.90):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\ndef check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] &gt; up_limit) | (dataframe[col_name] &lt; low_limit)].any(axis=None):\n        return True\n    else:\n        return False\nfor col in num_cols:\n    print(col, check_outlier(df, col))\n\nAvg. Session Length False\nTime on App False\nTime on Website False\nLength of Membership False\nYearly Amount Spent False\n\n\nAvg. Session Length False Time on App False Time on Website False Length of Membership False Yearly Amount Spent False\nAnalysis Of Missing Values\nMissing values may cause problems while setting up the model. It must be detected and necessary actions must be taken.\nNo missing values were found for the relevant data.\n\ndef missing_values_table(dataframe, na_name=False):\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() &gt; 0]\n\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[na_columns].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df, end=\"\\n\")\n\n    if na_name:\n        return na_columns\n\nmissing_values_table(df)\n\nEmpty DataFrame\nColumns: [n_miss, ratio]\nIndex: []\n\n\nEmpty DataFrame Columns: [n_miss, ratio] Index: []\nCorrelation Analysis\nValues with high correlation affect the target variable to a similar extent. Therefore, we can eliminate one of the variables with high correlation between two variables and use the other.\nWhen the data was examined, no variable with a high correlation of more than 90% was found.\n\ndef high_correlated_cols(dataframe, plot=False, corr_th=0.90):\n    corr = dataframe.corr()\n    cor_matrix = corr.abs()\n    upper_triangle_matrix = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(bool))\n    drop_list = [col for col in upper_triangle_matrix.columns if any(upper_triangle_matrix[col] &gt; corr_th)]\n    if plot:\n        import seaborn as sns\n        import matplotlib.pyplot as plt\n        sns.set(rc={'figure.figsize': (10, 5)})\n        sns.heatmap(corr, cmap=\"RdBu\", annot=True)\n        plt.show(block=True)\n    return drop_list\n\n\nhigh_correlated_cols(df,plot=True)\n\n\n\n\n[]"
  },
  {
    "objectID": "posts/linregression/index.html#linear-regression",
    "href": "posts/linregression/index.html#linear-regression",
    "title": "Linear Regression",
    "section": "LINEAR REGRESSION",
    "text": "LINEAR REGRESSION\n\nLinear regression models the relationship between dependent and independent variable/variables linearly.\nIn order to create the model, dependent and independent variables were defined.\n\nX = df.drop('Yearly Amount Spent', axis=1) \n\ny = df[[\"Yearly Amount Spent\"]]\n\nBuilding the Model\nBuilding the Model A train and test set by dividing the data into two is created . By training the model with one part and testing the model with the other part, it can be determined how successful the model is.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n\nreg_model = LinearRegression().fit(X_train, y_train)\n\n# constant (b - bias)\nprint(reg_model.intercept_)\n\n# coefficients (w - weights)\nprint(reg_model.coef_)\n\n[-1047.73920526]\n[[25.78854257 38.85150472  0.25638467 61.49204989]]\n\n\nPrediction of dependent variable\n\ny_pred = reg_model.predict(X_test)"
  },
  {
    "objectID": "posts/nlinregression/index.html#definition",
    "href": "posts/nlinregression/index.html#definition",
    "title": "Nonlinear Regression",
    "section": "Definition",
    "text": "Definition\nNon-linear regression in machine learning is a powerful tool for modeling complex relationships between variables, where these relationships cannot be adequately described using a straight line. In machine learning, non-linear regression is used to predict outcomes based on non-linear interactions of predictor variables. It’s particularly useful in scenarios where the underlying data patterns are inherently non-linear."
  },
  {
    "objectID": "posts/nlinregression/index.html#mathematical-formulation",
    "href": "posts/nlinregression/index.html#mathematical-formulation",
    "title": "Nonlinear Regression",
    "section": "Mathematical Formulation",
    "text": "Mathematical Formulation\nThe general form of a non-linear regression model is: Y = f(X, β) + ε\n\nY is the dependent variable.\nX is the independent variable.\nβ represents the parameters of the model.\nf is a non-linear function.\nε is the error term."
  },
  {
    "objectID": "posts/nlinregression/index.html#characteristics",
    "href": "posts/nlinregression/index.html#characteristics",
    "title": "Nonlinear Regression",
    "section": "Characteristics",
    "text": "Characteristics\nNature of Relationship: Unlike linear regression, where the relationship between variables is a straight line, non-linear regression deals with data where the relationship is curvilinear. This means the graph of the relationship forms a curve, not a straight line.\nTypes of Non-Linear Relationships: Relationships can be exponential, logarithmic, power, or more complex types. For instance, a common non-linear model is the exponential growth model, represented as:\n\nY = a * e^(bX)\nHere, e is the base of the natural logarithm, a and b are model parameters, Y is the dependent variable, and X is the independent variable.\n\nFlexibility: Non-linear regression can model more complex relationships and patterns in data compared to linear models. It’s more flexible in fitting data curves, but this comes with the cost of increased complexity in calculation and interpretation."
  },
  {
    "objectID": "posts/nlinregression/index.html#types-of-non-linear-models",
    "href": "posts/nlinregression/index.html#types-of-non-linear-models",
    "title": "Nonlinear Regression",
    "section": "Types of Non-linear Models:",
    "text": "Types of Non-linear Models:\nPolynomial Regression: Extends linear models by adding polynomial terms, making the model curve.\nDecision Trees and Random Forests: Can model non-linear relationships by splitting the data into branches based on decision rules.\nNeural Networks: Highly capable of capturing non-linear relationships through layers of neurons with non-linear activation functions.\nSupport Vector Machines with Non-linear Kernels: Use kernel functions (like RBF) to project data into higher dimensions where it is linearly separable."
  },
  {
    "objectID": "posts/nlinregression/index.html#applications",
    "href": "posts/nlinregression/index.html#applications",
    "title": "Nonlinear Regression",
    "section": "Applications",
    "text": "Applications\nScientific Data: Often used in scientific data where the rate of change of a variable accelerates or decelerates rapidly, or where the effect of an independent variable is not proportional over its range.\nGrowth Curves: Common in biological systems, such as modeling population growth or the spread of diseases.\nEconomic Data: Used in economics for functions like utility curves, supply/demand curves."
  },
  {
    "objectID": "posts/clustering/index.html#what-is-clustering",
    "href": "posts/clustering/index.html#what-is-clustering",
    "title": "Clustering",
    "section": "",
    "text": "Clustering in machine learning is a type of unsupervised learning technique that involves grouping similar data points together into clusters. In this context, “unsupervised” means that the learning process is conducted without any predefined labels or categories; the algorithm itself discovers the inherent groupings in the data."
  },
  {
    "objectID": "posts/clustering/index.html#key-aspects-of-clustering-in-machine-learning",
    "href": "posts/clustering/index.html#key-aspects-of-clustering-in-machine-learning",
    "title": "Clustering",
    "section": "Key Aspects of Clustering in Machine Learning",
    "text": "Key Aspects of Clustering in Machine Learning\n\nObjective:\nThe primary goal is to divide data into distinct groups where members of each group are more similar to each other than to those in other groups. This similarity is often based on features or attributes of the data.\nTypes of clustering algorithms:\nK-Means Clustering: This algorithm partitions the data into K clusters. It starts with random cluster centers and iteratively refines them by minimizing the variance within each cluster. Hierarchical Clustering: This creates a tree of clusters. It can be either agglomerative (merging small clusters into larger ones) or divisive (splitting clusters). DBSCAN (Density-Based Spatial Clustering of Applications with Noise): Focuses on the density of data points. It can find arbitrarily shaped clusters and is good at identifying outliers. Feature Selection and Scaling: In clustering, feature selection and normalization are crucial because the algorithm’s performance can significantly vary based on the input data’s scale and relevance of features.\nChoosing the Number of Clusters: In methods like K-Means, deciding the number of clusters (K) is critical. Techniques like the Elbow Method or the Silhouette Coefficient are used to estimate an optimal K value.\nApplications:\nCustomer Segmentation: Grouping customers based on purchasing behavior, interests, demographics, etc. Anomaly Detection: Identifying unusual data points which can be useful in fraud detection. Pattern Recognition: In areas like bioinformatics, speech recognition, and image analysis. Challenges:\nHandling Different Data Types: Clustering algorithms might struggle with varying data types (categorical, numerical, etc.). High Dimensionality: Clustering in high-dimensional spaces can be challenging due to the curse of dimensionality. Sensitivity to Initialization and Noise: Some algorithms are sensitive to the initial starting conditions or the presence of outliers. Evaluation Metrics:\nInternal Metrics: Measure based on the data itself, e.g., within-cluster sum of squares. External Metrics: Compare the clustering results to an externally known result, like a ground truth. Visualization: Techniques like Principal Component Analysis (PCA) or t-SNE are often used to visualize high-dimensional clustered data in two or three dimensions. ## KModes Clustering using Cardio Data\n\nfrom numpy import unique, where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import KMeans, DBSCAN\n\n# Initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# Define the KMeans model\nkmeans_model = KMeans(n_clusters=2)\n\n# Assign each data point to a cluster using KMeans\nkmeans_result = kmeans_model.fit_predict(training_data)\n\n# Define the DBSCAN model\ndbscan_model = DBSCAN(eps=0.3, min_samples=9)  # Adjust 'eps' and 'min_samples' as needed\n\n# Assign each data point to a cluster using DBSCAN\ndbscan_result = dbscan_model.fit_predict(training_data)\n\n# Get all of the unique clusters from DBSCAN results\ndbscan_clusters = unique(dbscan_result)\n\n# Plot the DBSCAN clusters\nfor dbscan_cluster in dbscan_clusters:\n    # Get data points that fall in this cluster\n    index = where(dbscan_result == dbscan_cluster)\n    # Make the plot\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n\n# Show the DBSCAN plot\npyplot.show()\n\nC:\\Users\\rahul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning"
  },
  {
    "objectID": "posts/clustering/index.html#types-of-clustering-algorithms",
    "href": "posts/clustering/index.html#types-of-clustering-algorithms",
    "title": "Clustering",
    "section": "Types of Clustering Algorithms",
    "text": "Types of Clustering Algorithms\nClustering algorithms come in various forms, each suited to handle different kinds of data and objectives:\nDensity-Based Clustering: These algorithms identify clusters as areas of high density separated by areas of low density. They excel in finding clusters of arbitrary shapes and typically do not assign outliers to any cluster.\nDistribution-Based Clustering: In this approach, clusters are formed based on the likelihood of data points belonging to a certain distribution. A typical model is the Gaussian distribution, where the probability of belonging to a cluster decreases as the distance from the cluster’s center increases.\nCentroid-Based Clustering: This is a widely used approach where clusters are represented by a central vector or a centroid. Each data point is assigned to the cluster with the nearest centroid. K-means is a popular example of centroid-based clustering.\nHierarchical-Based Clustering: These algorithms create a tree of clusters, which is particularly useful for hierarchical or nested data structures. It can be either divisive (top-down approach) or agglomerative (bottom-up approach)."
  },
  {
    "objectID": "posts/clustering/index.html#common-clustering-algorithms",
    "href": "posts/clustering/index.html#common-clustering-algorithms",
    "title": "Clustering",
    "section": "Common Clustering algorithms:",
    "text": "Common Clustering algorithms:\n\nK-Means Clustering algorithm:\nK-means clustering, the most commonly used clustering algorithm, partitions a dataset into K distinct clusters. It does this by assigning each data point to the nearest of K centroids, which are iteratively recalculated as the mean of the points assigned to them. This process repeats until the centroids stabilize, effectively minimizing the variance within each cluster. Widely appreciated for its simplicity and efficiency, this method assumes spherical clusters and can be sensitive to the initial placement of centroids and the presence of outliers.\nImplementation\n\nfrom numpy import unique, where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import KMeans, DBSCAN\n\n# Initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# Define the KMeans model\nkmeans_model = KMeans(n_clusters=2)\n\n# Assign each data point to a cluster using KMeans\nkmeans_result = kmeans_model.fit_predict(training_data)\n\n# Define the DBSCAN model\ndbscan_model = DBSCAN(eps=0.3, min_samples=9)  # Adjust 'eps' and 'min_samples' as needed\n\n# Assign each data point to a cluster using DBSCAN\ndbscan_result = dbscan_model.fit_predict(training_data)\n\n# Get all of the unique clusters from DBSCAN results\ndbscan_clusters = unique(dbscan_result)\n\n# Plot the DBSCAN clusters\nfor dbscan_cluster in dbscan_clusters:\n    # Get data points that fall in this cluster\n    index = where(dbscan_result == dbscan_cluster)\n    # Make the plot\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n\n# Show the DBSCAN plot\npyplot.show()\n\nC:\\Users\\rahul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\n\n\n\n\n\nDBSCAN clustering algorithm:\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a clustering algorithm that identifies clusters based on the density of data points, without requiring the number of clusters to be predefined. It categorizes points as core points, border points, or noise, depending on the number of nearby points (based on a set distance eps) and a minimum number of points (minPts) required to form a dense region. DBSCAN excels in discovering clusters of arbitrary shapes and sizes, and effectively distinguishes outliers, making it robust in handling noisy datasets. This makes it particularly useful in applications like anomaly detection, geospatial analysis, and image processing.\nImplementation\n\nfrom numpy import unique\nfrom numpy import where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import DBSCAN\n\n# initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# define the model\ndbscan_model = DBSCAN(eps=0.25, min_samples=9)\n\n# train the model\ndbscan_model.fit(training_data)\n\n# assign each data point to a cluster\ndbscan_result = dbscan_model.labels_\n\n# get all of the unique clusters\ndbscan_clusters = unique(dbscan_result)\n\n# plot the DBSCAN clusters\nfor cluster in dbscan_clusters:\n    # get data points that fall in this cluster\n    index = where(dbscan_result == cluster)\n    # make the plot\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n\n# show the DBSCAN plot\npyplot.show()\n\n\n\n\n\n\nHierarchical Clustering:\nThis creates a tree of clusters. It can be either agglomerative (merging small clusters into larger ones) or divisive (splitting clusters).\nAgglomerative Hierarchy clustering algorithm: This bottom-up approach starts with each data point as an individual cluster and iteratively merges the closest pairs of clusters until all points are merged into a single cluster or a specified number of clusters is reached. The process of choosing which clusters to merge at each step is based on a distance metric and a linkage criterion (like single, complete, or average linkage). Agglomerative clustering is particularly useful for small to medium-sized datasets and is often visualized using a dendrogram, which shows the hierarchical relationship between clusters.\nImplementation\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# Define the model\nagglomerative_model = AgglomerativeClustering(n_clusters=2)\n\n# Assign each data point to a cluster\nagglomerative_result = agglomerative_model.fit_predict(training_data)\n\n# Get all unique clusters\nagglomerative_clusters = np.unique(agglomerative_result)\n\n# Plot the clusters\nfor cluster_label in agglomerative_clusters:\n    # Get data points that belong to this cluster\n    cluster_points = training_data[agglomerative_result == cluster_label]\n    # Make the plot\n    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {cluster_label}')\n\n# Show the Agglomerative Hierarchy plot\nplt.legend()\nplt.show()\n\n\n\n\nDivisive Hierarchy clustering algorithm: Conversely, Divisive clustering is a top-down approach. It begins with all data points in a single cluster and recursively splits the most heterogeneous cluster until each cluster contains only one data point or until a specified number of clusters is achieved. This method often involves analyzing the data to determine the best way to perform the splits, typically based on a measure of dissimilarity between data points.\nImplementation\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\n\n# Initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# Define the model for divisive clustering\ndef divisive_clustering(data, stopping_condition):\n    clusters = [data]  # Start with all data points in one cluster\n\n    while len(clusters) &lt; stopping_condition:\n        # Select the cluster to split (you can define your splitting criterion here)\n        cluster_to_split = select_cluster_to_split(clusters)\n\n        # Split the selected cluster into two subclusters\n        subcluster1, subcluster2 = split_cluster(cluster_to_split)\n\n        # Remove the original cluster and add the two subclusters\n        clusters.remove(cluster_to_split)\n        clusters.append(subcluster1)\n        clusters.append(subcluster2)\n\n    return clusters\n\n# Define your custom splitting criterion (e.g., based on variance, dissimilarity, etc.)\ndef select_cluster_to_split(clusters):\n    # For simplicity, we'll select the largest cluster to split.\n    return max(clusters, key=len)\n\n# Define a function to split a cluster (you can implement your splitting logic)\ndef split_cluster(cluster):\n    midpoint = len(cluster) // 2\n    subcluster1 = cluster[:midpoint]\n    subcluster2 = cluster[midpoint:]\n    return subcluster1, subcluster2\n\n# Perform divisive clustering with a specified stopping condition\nclusters = divisive_clustering(training_data.tolist(), stopping_condition=4)\n\n# Plot the divisive clusters\nfor i, cluster in enumerate(clusters):\n    cluster = np.array(cluster)  # Convert back to NumPy array\n    plt.scatter(cluster[:, 0], cluster[:, 1], label=f'Cluster {i+1}')\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\nGaussian Mixture Model algorithm:\nThe Gaussian Mixture Model (GMM) algorithm is a probabilistic approach used for clustering and density estimation, overcoming K-means’ circular data limitation. GMM assumes data is generated from a mixture of Gaussian distributions, making it suitable for various data shapes. It iteratively estimates Gaussian parameters (mean, covariance, mixing coefficients) to fit data, providing soft cluster assignments based on probabilities. GMMs are versatile but sensitive to initializations, requiring a predefined cluster count.\nImplementation\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.mixture import GaussianMixture\n\n# Initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# Define the model\ngaussian_model = GaussianMixture(n_components=2)\n\n# Train the model\ngaussian_model.fit(training_data)\n\n# Assign each data point to a cluster\ngaussian_result = gaussian_model.predict(training_data)\n\n# Get all unique clusters\ngaussian_clusters = np.unique(gaussian_result)\n\n# Plot the Gaussian Mixture clusters\nfor cluster_label in gaussian_clusters:\n    # Get data points that belong to this cluster\n    cluster_points = training_data[gaussian_result == cluster_label]\n    # Make the plot\n    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {cluster_label}')\n\n# Show the Gaussian Mixture plot\nplt.legend()\nplt.show()\n\n\n\n\n\n\nMean-Shift clustering algorithm:\nThe Mean-Shift clustering algorithm is a density-based method used to find clusters in data without specifying the number of clusters in advance. It estimates the density of data points, iteratively shifts them towards denser regions, and identifies clusters where points converge. Mean-Shift is versatile and suitable for various cluster shapes and sizes, commonly used in applications like image segmentation and object tracking.\nImplementation\n\nfrom numpy import unique\nfrom numpy import where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import MeanShift\n\n# initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# define the model\nmean_model = MeanShift()\n\n# assign each data point to a cluster\nmean_result = mean_model.fit_predict(training_data)\n\n# get all of the unique clusters\nmean_clusters = unique(mean_result)\n\n# plot Mean-Shift the clusters\nfor mean_cluster in mean_clusters:\n    # get data points that fall in this cluster\n    index = where(mean_result == mean_cluster)\n    # make the plot\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n\n# show the Mean-Shift plot\npyplot.show()\n\n\n\n\n\n\nBIRCH algorithm:\nThe BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies) algorithm is a memory-efficient hierarchical clustering technique designed for large datasets, particularly numeric data. It builds a tree-like structure by recursively splitting data points into compact summaries that preserve distribution information, allowing scalable clustering. BIRCH is commonly combined with other clustering techniques and is well-suited for high-dimensional data. However, it requires data transformations for handling categorical values. This approach is frequently employed in exploratory data analysis and data compression, making it a valuable tool for various applications.\nImplementation\n\nfrom numpy import unique\nfrom numpy import where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import Birch\n\n# initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# define the model\nbirch_model = Birch(threshold=0.03, n_clusters=2)\n\n# train the model\nbirch_model.fit(training_data)\n\n# assign each data point to a cluster\nbirch_result = birch_model.predict(training_data)\n\n# get all of the unique clusters\nbirch_clusters = unique(birch_result)\n\n# plot the BIRCH clusters\nfor birch_cluster in birch_clusters:\n    # get data points that fall in this cluster\n    index = where(birch_result == birch_cluster)\n    # make the plot\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n\n# show the BIRCH plot\npyplot.show()\n\n\n\n\n\n\nOPTICS algorithm:\nOPTICS (Ordering Points To Identify the Clustering Structure) is a density-based clustering algorithm that excels in identifying clusters in data with varying density. It orders data points to detect different density clusters efficiently, similar to DBSCAN, and assigns specific cluster membership distances to each point. It doesn’t require specifying the number of clusters in advance, making it suitable for various dataset shapes and sizes\nImplementation\n\nfrom numpy import unique\nfrom numpy import where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import OPTICS\n\n# initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# define the model\noptics_model = OPTICS(eps=0.75, min_samples=10)\n\n# assign each data point to a cluster\noptics_result = optics_model.fit_predict(training_data)\n\n# get all of the unique clusters\noptics_clusters = unique(optics_result)\n\n# plot the OPTICS clusters\nfor optics_cluster in optics_clusters:\n    # get data points that fall in this cluster\n    index = where(optics_result == optics_cluster)\n    # make the plot\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n\n# show the OPTICS plot\npyplot.show()\n\n\n\n\n\n\nAffinity Propagation clustering algorithm:\nAffinity Propagation (AP) is a unique clustering algorithm that doesn’t require specifying the number of clusters beforehand. It uses data point communication to discover clusters, and exemplars are found as a consensus among data points. AP is ideal for scenarios where the number of clusters is uncertain, such as computer vision problems. This clustering algorithm models data points as nodes in a network and finds exemplar data points that represent clusters through message exchange. While effective for high-dimensional data and various cluster shapes and sizes, AP can be sensitive to the initial selection of exemplars and may not perform well on large datasets.\nImplementation\n\nfrom numpy import unique\nfrom numpy import where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import AffinityPropagation\n\n# initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# define the model\nmodel = AffinityPropagation(damping=0.7)\n\n# train the model\nmodel.fit(training_data)\n\n# assign each data point to a cluster\nresult = model.predict(training_data)\n\n# get all of the unique clusters\nclusters = unique(result)\n\n# plot the clusters\nfor cluster in clusters:\n    # get data points that fall in this cluster\n    index = where(result == cluster)\n    # make the plot\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n\n# show the plot\npyplot.show()"
  },
  {
    "objectID": "posts/clustering/index.html#applications",
    "href": "posts/clustering/index.html#applications",
    "title": "Clustering",
    "section": "Applications:",
    "text": "Applications:\nCustomer Segmentation: Grouping customers based on purchasing behavior, interests, demographics, etc.\nAnomaly Detection: Identifying unusual data points which can be useful in fraud detection.\nPattern Recognition: In areas like bioinformatics, speech recognition, and image analysis."
  },
  {
    "objectID": "posts/clustering/index.html#agglomerative-hierarchy-clustering-algorithm",
    "href": "posts/clustering/index.html#agglomerative-hierarchy-clustering-algorithm",
    "title": "Clustering",
    "section": "Agglomerative Hierarchy clustering algorithm:",
    "text": "Agglomerative Hierarchy clustering algorithm:\nThis bottom-up approach starts with each data point as an individual cluster and iteratively merges the closest pairs of clusters until all points are merged into a single cluster or a specified number of clusters is reached. The process of choosing which clusters to merge at each step is based on a distance metric and a linkage criterion (like single, complete, or average linkage). Agglomerative clustering is particularly useful for small to medium-sized datasets and is often visualized using a dendrogram, which shows the hierarchical relationship between clusters.\nImplementation\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# Define the model\nagglomerative_model = AgglomerativeClustering(n_clusters=2)\n\n# Assign each data point to a cluster\nagglomerative_result = agglomerative_model.fit_predict(training_data)\n\n# Get all unique clusters\nagglomerative_clusters = np.unique(agglomerative_result)\n\n# Plot the clusters\nfor cluster_label in agglomerative_clusters:\n    # Get data points that belong to this cluster\n    cluster_points = training_data[agglomerative_result == cluster_label]\n    # Make the plot\n    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {cluster_label}')\n\n# Show the Agglomerative Hierarchy plot\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/clustering/index.html#divisive-hierarchy-clustering-algorithm",
    "href": "posts/clustering/index.html#divisive-hierarchy-clustering-algorithm",
    "title": "Clustering",
    "section": "Divisive Hierarchy clustering algorithm:",
    "text": "Divisive Hierarchy clustering algorithm:\nConversely, Divisive clustering is a top-down approach. It begins with all data points in a single cluster and recursively splits the most heterogeneous cluster until each cluster contains only one data point or until a specified number of clusters is achieved. This method often involves analyzing the data to determine the best way to perform the splits, typically based on a measure of dissimilarity between data points.\nImplementation\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\n\n# Initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# Define the model for divisive clustering\ndef divisive_clustering(data, stopping_condition):\n    clusters = [data]  # Start with all data points in one cluster\n\n    while len(clusters) &lt; stopping_condition:\n        # Select the cluster to split (you can define your splitting criterion here)\n        cluster_to_split = select_cluster_to_split(clusters)\n\n        # Split the selected cluster into two subclusters\n        subcluster1, subcluster2 = split_cluster(cluster_to_split)\n\n        # Remove the original cluster and add the two subclusters\n        clusters.remove(cluster_to_split)\n        clusters.append(subcluster1)\n        clusters.append(subcluster2)\n\n    return clusters\n\n# Define your custom splitting criterion (e.g., based on variance, dissimilarity, etc.)\ndef select_cluster_to_split(clusters):\n    # For simplicity, we'll select the largest cluster to split.\n    return max(clusters, key=len)\n\n# Define a function to split a cluster (you can implement your splitting logic)\ndef split_cluster(cluster):\n    midpoint = len(cluster) // 2\n    subcluster1 = cluster[:midpoint]\n    subcluster2 = cluster[midpoint:]\n    return subcluster1, subcluster2\n\n# Perform divisive clustering with a specified stopping condition\nclusters = divisive_clustering(training_data.tolist(), stopping_condition=4)\n\n# Plot the divisive clusters\nfor i, cluster in enumerate(clusters):\n    cluster = np.array(cluster)  # Convert back to NumPy array\n    plt.scatter(cluster[:, 0], cluster[:, 1], label=f'Cluster {i+1}')\n\nplt.legend()\nplt.show()\n\n\n\n\n\nGaussian Mixture Model algorithm:\nThe Gaussian Mixture Model (GMM) algorithm is a probabilistic approach used for clustering and density estimation, overcoming K-means’ circular data limitation. GMM assumes data is generated from a mixture of Gaussian distributions, making it suitable for various data shapes. It iteratively estimates Gaussian parameters (mean, covariance, mixing coefficients) to fit data, providing soft cluster assignments based on probabilities. GMMs are versatile but sensitive to initializations, requiring a predefined cluster count.\nImplementation\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.mixture import GaussianMixture\n\n# Initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# Define the model\ngaussian_model = GaussianMixture(n_components=2)\n\n# Train the model\ngaussian_model.fit(training_data)\n\n# Assign each data point to a cluster\ngaussian_result = gaussian_model.predict(training_data)\n\n# Get all unique clusters\ngaussian_clusters = np.unique(gaussian_result)\n\n# Plot the Gaussian Mixture clusters\nfor cluster_label in gaussian_clusters:\n    # Get data points that belong to this cluster\n    cluster_points = training_data[gaussian_result == cluster_label]\n    # Make the plot\n    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {cluster_label}')\n\n# Show the Gaussian Mixture plot\nplt.legend()\nplt.show()\n\n\n\n\n\n\nMean-Shift clustering algorithm:\nThe Mean-Shift clustering algorithm is a density-based method used to find clusters in data without specifying the number of clusters in advance. It estimates the density of data points, iteratively shifts them towards denser regions, and identifies clusters where points converge. Mean-Shift is versatile and suitable for various cluster shapes and sizes, commonly used in applications like image segmentation and object tracking.\nImplementation\n\nfrom numpy import unique\nfrom numpy import where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import MeanShift\n\n# initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# define the model\nmean_model = MeanShift()\n\n# assign each data point to a cluster\nmean_result = mean_model.fit_predict(training_data)\n\n# get all of the unique clusters\nmean_clusters = unique(mean_result)\n\n# plot Mean-Shift the clusters\nfor mean_cluster in mean_clusters:\n    # get data points that fall in this cluster\n    index = where(mean_result == mean_cluster)\n    # make the plot\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n\n# show the Mean-Shift plot\npyplot.show()\n\n\n\n\n\n\nBIRCH algorithm:\nThe BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies) algorithm is a memory-efficient hierarchical clustering technique designed for large datasets, particularly numeric data. It builds a tree-like structure by recursively splitting data points into compact summaries that preserve distribution information, allowing scalable clustering. BIRCH is commonly combined with other clustering techniques and is well-suited for high-dimensional data. However, it requires data transformations for handling categorical values. This approach is frequently employed in exploratory data analysis and data compression, making it a valuable tool for various applications.\nImplementation\n\nfrom numpy import unique\nfrom numpy import where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import Birch\n\n# initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# define the model\nbirch_model = Birch(threshold=0.03, n_clusters=2)\n\n# train the model\nbirch_model.fit(training_data)\n\n# assign each data point to a cluster\nbirch_result = birch_model.predict(training_data)\n\n# get all of the unique clusters\nbirch_clusters = unique(birch_result)\n\n# plot the BIRCH clusters\nfor birch_cluster in birch_clusters:\n    # get data points that fall in this cluster\n    index = where(birch_result == birch_cluster)\n    # make the plot\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n\n# show the BIRCH plot\npyplot.show()\n\n\n\n\n\n\nOPTICS algorithm:\nOPTICS (Ordering Points To Identify the Clustering Structure) is a density-based clustering algorithm that excels in identifying clusters in data with varying density. It orders data points to detect different density clusters efficiently, similar to DBSCAN, and assigns specific cluster membership distances to each point. It doesn’t require specifying the number of clusters in advance, making it suitable for various dataset shapes and sizes\nImplementation\n\nfrom numpy import unique\nfrom numpy import where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import OPTICS\n\n# initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# define the model\noptics_model = OPTICS(eps=0.75, min_samples=10)\n\n# assign each data point to a cluster\noptics_result = optics_model.fit_predict(training_data)\n\n# get all of the unique clusters\noptics_clusters = unique(optics_result)\n\n# plot the OPTICS clusters\nfor optics_cluster in optics_clusters:\n    # get data points that fall in this cluster\n    index = where(optics_result == optics_cluster)\n    # make the plot\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n\n# show the OPTICS plot\npyplot.show()\n\n\n\n\n\n\nAffinity Propagation clustering algorithm:\nAffinity Propagation (AP) is a unique clustering algorithm that doesn’t require specifying the number of clusters beforehand. It uses data point communication to discover clusters, and exemplars are found as a consensus among data points. AP is ideal for scenarios where the number of clusters is uncertain, such as computer vision problems. This clustering algorithm models data points as nodes in a network and finds exemplar data points that represent clusters through message exchange. While effective for high-dimensional data and various cluster shapes and sizes, AP can be sensitive to the initial selection of exemplars and may not perform well on large datasets.\nImplementation\n\nfrom numpy import unique\nfrom numpy import where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import AffinityPropagation\n\n# initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# define the model\nmodel = AffinityPropagation(damping=0.7)\n\n# train the model\nmodel.fit(training_data)\n\n# assign each data point to a cluster\nresult = model.predict(training_data)\n\n# get all of the unique clusters\nclusters = unique(result)\n\n# plot the clusters\nfor cluster in clusters:\n    # get data points that fall in this cluster\n    index = where(result == cluster)\n    # make the plot\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n\n# show the plot\npyplot.show()"
  },
  {
    "objectID": "posts/probability/index.html#definition",
    "href": "posts/probability/index.html#definition",
    "title": "Probability and Random Variables",
    "section": "Definition",
    "text": "Definition\nProbability theory is the mathematical study of uncertainty. It plays a central role in machine learning, as the design of learning algorithms often relies on probabilistic assumption of the data. This set of notes attempts to cover some basic probability theory that serves as a background for the class."
  },
  {
    "objectID": "posts/probability/index.html#probability-space",
    "href": "posts/probability/index.html#probability-space",
    "title": "Probability and Random Variables",
    "section": "Probability Space",
    "text": "Probability Space\nWhen we speak about probability, we often refer to the probability of an event of uncertain nature taking place. For example, we speak about the probability of rain next Tuesday. Therefore, in order to discuss probability theory formally, we must first clarify what the possible events are to which we would like to attach probability.\nFormally, a probability space is defined by the triple (Ω, F, P), where\n\n• Ω is the space of possible outcomes (or outcome space),\n• F ⊆ 2^Ω (the power set of Ω) is the space of (measurable) events (or event space),\n• P is the probability measure (or probability distribution) that maps an event E ∈ F to a real value between 0 and 1 (think of P as a function).\n\nGiven the outcome space Ω, there is some restrictions as to what subset of 2^Ω can be considered an event space F:\n\n• The trivial event Ω and the empty event ∅ is in F.\n• The event space F is closed under (countable) union, i.e., if α, β ∈ F, then α ∪ β ∈ F.\n• The even space F is closed under complement, i.e., if α ∈ F, then (Ω  α) ∈ F.\n\nExample 1. Suppose we throw a (six-sided) dice. The space of possible outcomes Ω = {1, 2, 3, 4, 5, 6}. We may decide that the events of interest is whether the dice throw is odd or even. This event space will be given by F = {∅, {1, 3, 5}, {2, 4, 6}, Ω}.\nNote that when the outcome space Ω is finite, as in the previous example, we often take the event space F to be 2Ω. This treatment is not fully general, but it is often sufficient for practical purposes. However, when the outcome space is infinite, we must be careful to define what the event space is.\nGiven an event space F, the probability measure P must satisfy certain axioms.\n• (non-negativity) For all α ∈ F, P(α) ≥ 0.\n• (trivial event) P(Ω) = 1.\n• (additivity) For all α, β ∈ F and α ∩ β = ∅, P(α ∪ β) = P(α) + P(β).\nExample 2. Returning to our dice example, suppose we now take the event space F to be 2Ω. Further, we define a probability distribution P over F such that\nP({1}) = P({2}) = · · · = P({6}) = 1/6\nthen this distribution P completely specifies the probability of any given event happening (through the additivity axiom). For example, the probability of an even dice throw will be\nP({2, 4, 6}) = P({2}) + P({4}) + P({6}) = 1/6 + 1/6 + 1/6 = 1/2\nsince each of these events are disjoint."
  },
  {
    "objectID": "posts/probability/index.html#random-variables",
    "href": "posts/probability/index.html#random-variables",
    "title": "Probability and Random Variables",
    "section": "Random Variables",
    "text": "Random Variables\nRandom variables play an important role in probability theory. The most important fact about random variables is that they are not variables. They are actually functions that map outcomes (in the outcome space) to real values. In terms of notation, we usually denote random variables by a capital letter. Let’s see an example.\nExample 3. Again, consider the process of throwing a dice. Let X be a random variable that depends on the outcome of the throw. A natural choice for X would be to map the outcome i to the value i, i.e., mapping the event of throwing an “one” to the value of 1. Note that we could have chosen some strange mappings too. For example, we could have a random variable Y that maps all outcomes to 0, which would be a very boring function, or a random variable Z that maps the outcome i to the value of 2^i if i is odd and the value of −i if i is even, which would be quite strange indeed.\nIn a sense, random variables allow us to abstract away from the formal notion of event space, as we can define random variables that capture the appropriate events. For example, consider the event space of odd or even dice throw in Example 1. We could have defined a random variable that takes on value 1 if outcome i is odd and 0 otherwise. These type of binary random variables are very common in practice, and are known as indicator variables, taking its name from its use to indicate whether a certain event has happened. So why did we introduce event space? That is because when one studies probability theory (more 2 rigorously) using measure theory, the distinction between outcome space and event space will be very important. This topic is too advanced to be covered in this short review note.In any case, it is good to keep in mind that event space is not always simply the power set of the outcome space.\nFrom here onwards, we will talk mostly about probability with respect to random variables. While some probability concepts can be defined meaningfully without using them, random variables allow us to provide a more uniform treatment of probability theory. For notations, the probability of a random variable X taking on the value of a will be denoted by either\n                 P(X = a) or Px(a)\nWe will also denote the range of a random variable X by V al(X)."
  },
  {
    "objectID": "posts/probability/index.html#probability-distribution",
    "href": "posts/probability/index.html#probability-distribution",
    "title": "Probability and Random Variables",
    "section": "Probability Distribution",
    "text": "Probability Distribution\nProbability distributions are fundamental to understanding random variables in statistics and probability theory. They provide a systematic way to describe the likelihood of different outcomes from a random process. Let’s explore this concept in detail for both discrete and continuous random variables:"
  },
  {
    "objectID": "posts/probability/index.html#probability-distributions-for-discrete-random-variables",
    "href": "posts/probability/index.html#probability-distributions-for-discrete-random-variables",
    "title": "Probability and Random Variables",
    "section": "Probability Distributions for Discrete Random Variables",
    "text": "Probability Distributions for Discrete Random Variables\n\nProbability Mass Function (PMF):\nThe PMF is a function that gives the probability that a discrete random variable is exactly equal to some value. It satisfies two conditions: The sum of probabilities for all possible outcomes equals 1, and the probability for each individual outcome is between 0 and 1.\nKey Discrete Distributions:\nBinomial Distribution: Models the number of successes in a fixed number of independent Bernoulli trials (like flipping a coin several times). It is characterized by two parameters: the number of trials (n) and the probability of success (p) in each trial.\n\nimport numpy as np\nfrom scipy.stats import binom\n\n# Define parameters\nn = 10  # Number of trials\np = 0.5  # Probability of success in each trial\n\n# Generate binomial random variables\nrv = binom(n, p)\n\n# Probability Mass Function (PMF)\nx = np.arange(0, n+1)\npmf = rv.pmf(x)\n\n# Plot the PMF\nimport matplotlib.pyplot as plt\nplt.bar(x, pmf)\nplt.xlabel('Number of Successes')\nplt.ylabel('Probability')\nplt.title('Binomial Distribution PMF')\nplt.show()\n\n\n\n\nPoisson Distribution: Used for counting the number of events that occur in a fixed interval of time or space. It is characterized by its rate parameter (λ), which is the average number of events in the given interval.\n\nimport numpy as np\nfrom scipy.stats import poisson\n\n# Define rate parameter (average events in the interval)\nlambda_ = 3.0\n\n# Generate Poisson random variables\nrv = poisson(mu=lambda_)\n\n# Probability Mass Function (PMF)\nx = np.arange(0, 11)  # Example: Counting events from 0 to 10\npmf = rv.pmf(x)\n\n# Plot the PMF\nimport matplotlib.pyplot as plt\nplt.bar(x, pmf)\nplt.xlabel('Number of Events')\nplt.ylabel('Probability')\nplt.title('Poisson Distribution PMF')\nplt.show()\n\n\n\n\nGeometric Distribution: Describes the number of Bernoulli trials needed to get one success. Its key parameter is the probability of success (p) in each trial.\n\nimport numpy as np\nfrom scipy.stats import geom\n\n# Define probability of success in each trial\np = 0.2\n\n# Generate Geometric random variables\nrv = geom(p)\n\n# Probability Mass Function (PMF)\nx = np.arange(1, 11)  # Number of trials needed (1 to 10)\npmf = rv.pmf(x)\n\n# Plot the PMF\nimport matplotlib.pyplot as plt\nplt.bar(x, pmf)\nplt.xlabel('Number of Trials Needed')\nplt.ylabel('Probability')\nplt.title('Geometric Distribution PMF')\nplt.show()"
  },
  {
    "objectID": "posts/probability/index.html#probability-distributions-for-continuous-random-variables",
    "href": "posts/probability/index.html#probability-distributions-for-continuous-random-variables",
    "title": "Probability and Random Variables",
    "section": "Probability Distributions for Continuous Random Variables",
    "text": "Probability Distributions for Continuous Random Variables\n\n**Probability Density Function (PDF):\nThe PDF is used to specify the probability of the random variable falling within a particular range of values, as opposed to taking on any one specific value. The probability for any single point is zero for a continuous distribution. Instead, the area under the PDF curve within a range of values indicates the probability of falling within that range.\nKey Continuous Distributions:\nNormal (Gaussian) Distribution: One of the most important probability distributions, known for its bell-shaped curve. It is characterized by two parameters: the mean (μ), which indicates the center of the distribution, and the standard deviation (σ), which measures the spread.\n\nimport numpy as np\nfrom scipy.stats import norm\n\n# Define parameters\nmu = 0  # Mean\nsigma = 1  # Standard Deviation\n\n# Generate normal random variables\nrv = norm(loc=mu, scale=sigma)\n\n# Probability Density Function (PDF)\nx = np.linspace(-3, 3, 100)  # Range of values\npdf = rv.pdf(x)\n\n# Plot the PDF\nimport matplotlib.pyplot as plt\nplt.plot(x, pdf)\nplt.xlabel('Value')\nplt.ylabel('Probability Density')\nplt.title('Normal Distribution PDF')\nplt.show()\n\n\n\n\nExponential Distribution: Used to model the time elapsed between events in a process with a constant average rate (e.g., radioactive decay). It is characterized by its rate parameter (λ).\n\nimport numpy as np\nfrom scipy.stats import expon\n\n# Define rate parameter (λ)\nlambda_ = 0.5\n\n# Generate Exponential random variables\nrv = expon(scale=1/lambda_)\n\n# Probability Density Function (PDF)\nx = np.linspace(0, 10, 100)  # Range of values\npdf = rv.pdf(x)\n\n# Plot the PDF\nimport matplotlib.pyplot as plt\nplt.plot(x, pdf)\nplt.xlabel('Time')\nplt.ylabel('Probability Density')\nplt.title('Exponential Distribution PDF')\nplt.show()\n\n\n\n\nUniform Distribution: Describes a situation where all outcomes are equally likely. In its continuous form, it’s defined by two parameters, a and b, which are the minimum and maximum values of the distribution.\n\nimport numpy as np\nfrom scipy.stats import uniform\n\n# Define parameters\na = 0  # Minimum value\nb = 1  # Maximum value\n\n# Generate Uniform random variables\nrv = uniform(loc=a, scale=b-a)\n\n# Probability Density Function (PDF)\nx = np.linspace(a-0.1, b+0.1, 100)  # Range of values\npdf = rv.pdf(x)\n\n# Plot the PDF\nimport matplotlib.pyplot as plt\nplt.plot(x, pdf)\nplt.xlabel('Value')\nplt.ylabel('Probability Density')\nplt.title('Uniform Distribution PDF')\nplt.show()"
  },
  {
    "objectID": "posts/nonlinearRegression/index.html#definition",
    "href": "posts/nonlinearRegression/index.html#definition",
    "title": "Nonlinear Regression",
    "section": "Definition",
    "text": "Definition\nNon-linear regression in machine learning is a powerful tool for modeling complex relationships between variables, where these relationships cannot be adequately described using a straight line. In machine learning, non-linear regression is used to predict outcomes based on non-linear interactions of predictor variables. It’s particularly useful in scenarios where the underlying data patterns are inherently non-linear."
  },
  {
    "objectID": "posts/nonlinearRegression/index.html#mathematical-formulation",
    "href": "posts/nonlinearRegression/index.html#mathematical-formulation",
    "title": "Nonlinear Regression",
    "section": "Mathematical Formulation",
    "text": "Mathematical Formulation\nThe general form of a non-linear regression model is: Y = f(X, β) + ε\n\nY is the dependent variable.\nX is the independent variable.\nβ represents the parameters of the model.\nf is a non-linear function.\nε is the error term."
  },
  {
    "objectID": "posts/nonlinearRegression/index.html#characteristics",
    "href": "posts/nonlinearRegression/index.html#characteristics",
    "title": "Nonlinear Regression",
    "section": "Characteristics",
    "text": "Characteristics\nNature of Relationship: Unlike linear regression, where the relationship between variables is a straight line, non-linear regression deals with data where the relationship is curvilinear. This means the graph of the relationship forms a curve, not a straight line.\nTypes of Non-Linear Relationships: Relationships can be exponential, logarithmic, power, or more complex types. For instance, a common non-linear model is the exponential growth model, represented as:\n\nY = a * e^(bX)\nHere, e is the base of the natural logarithm, a and b are model parameters, Y is the dependent variable, and X is the independent variable.\n\nFlexibility: Non-linear regression can model more complex relationships and patterns in data compared to linear models. It’s more flexible in fitting data curves, but this comes with the cost of increased complexity in calculation and interpretation."
  },
  {
    "objectID": "posts/nonlinearRegression/index.html#types-of-non-linear-models",
    "href": "posts/nonlinearRegression/index.html#types-of-non-linear-models",
    "title": "Nonlinear Regression",
    "section": "Types of Non-linear Models:",
    "text": "Types of Non-linear Models:\nPolynomial Regression: Extends linear models by adding polynomial terms, making the model curve.\nDecision Trees and Random Forests: Can model non-linear relationships by splitting the data into branches based on decision rules.\nNeural Networks: Highly capable of capturing non-linear relationships through layers of neurons with non-linear activation functions.\nSupport Vector Machines with Non-linear Kernels: Use kernel functions (like RBF) to project data into higher dimensions where it is linearly separable."
  },
  {
    "objectID": "posts/nonlinearRegression/index.html#applications",
    "href": "posts/nonlinearRegression/index.html#applications",
    "title": "Nonlinear Regression",
    "section": "Applications",
    "text": "Applications\nScientific Data: Often used in scientific data where the rate of change of a variable accelerates or decelerates rapidly, or where the effect of an independent variable is not proportional over its range.\nGrowth Curves: Common in biological systems, such as modeling population growth or the spread of diseases.\nEconomic Data: Used in economics for functions like utility curves, supply/demand curves."
  },
  {
    "objectID": "posts/nonlinearRegression/index.html#example-of-nonlinear-regression",
    "href": "posts/nonlinearRegression/index.html#example-of-nonlinear-regression",
    "title": "Nonlinear Regression",
    "section": "Example of Nonlinear Regression",
    "text": "Example of Nonlinear Regression\nI.Introduction\nWhen you’re looking at data that seems to form curves or non-straight patterns, using linear regression might not give you the most accurate results. Linear regression is like a tool designed for straight-line relationships, so when your data behaves in a curvy way, it doesn’t quite fit the tool’s assumptions. That’s where non-linear regression steps in. It’s like having a set of tools that can handle curves and bends in the data. These tools, like polynomial, exponential, or logarithmic regression, can adjust to the data’s curves and complexities, giving you more precise predictions and a better match to the real nature of the data."
  },
  {
    "objectID": "posts/nonlinearRegression/index.html#linear",
    "href": "posts/nonlinearRegression/index.html#linear",
    "title": "Nonlinear Regression",
    "section": "1. Linear",
    "text": "1. Linear\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the linear function (e.g., L(x) = ax + b)\na, b = 2, 1\n\n# Generate x values\nx = np.linspace(0, 5, 100)\n\n# Calculate the corresponding y values using the linear function\ny = a * x + b\n\n# Generate random data points with noise\nnp.random.seed(0)\nnoise = np.random.normal(0, 0.5, len(x))\ndata_y = y + noise\n\n# Plot the linear function and data points\nplt.plot(x, y, label='Linear Function', color='blue')\nplt.scatter(x, data_y, label='Data Points', color='red', alpha=0.5)\nplt.xlabel('x')\nplt.ylabel('L(x)')\nplt.title('Linear Function')\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/nonlinearRegression/index.html#polynomial",
    "href": "posts/nonlinearRegression/index.html#polynomial",
    "title": "Nonlinear Regression",
    "section": "2. Polynomial",
    "text": "2. Polynomial\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define coefficients of the polynomial equation (e.g., P(x) = ax^3 + bx^2 + cx + d)\na, b, c, d = 1, -3, 3, -1\n\n# Generate x values\nx = np.linspace(-2, 2, 100)\n\n# Calculate the corresponding y values using the polynomial equation\ny = a * x**3 + b * x**2 + c * x + d\n\n# Generate random data points with noise\nnp.random.seed(0)\nnoise = np.random.normal(0, 0.2, len(x))\ndata_y = y + noise\n\n# Plot the polynomial function and data points\nplt.plot(x, y, label='Polynomial Function', color='blue')\nplt.scatter(x, data_y, label='Data Points', color='red', alpha=0.5)\nplt.xlabel('x')\nplt.ylabel('P(x)')\nplt.title('Polynomial Function')\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/nonlinearRegression/index.html#quadratic",
    "href": "posts/nonlinearRegression/index.html#quadratic",
    "title": "Nonlinear Regression",
    "section": "3. Quadratic",
    "text": "3. Quadratic\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define coefficients of the quadratic equation (e.g., Q(x) = ax^2 + bx + c)\na, b, c = 1, -2, 1\n\n# Generate x values\nx = np.linspace(-2, 3, 100)\n\n# Calculate the corresponding y values using the quadratic equation\ny = a * x**2 + b * x + c\n\n# Generate random data points with noise\nnp.random.seed(0)\nnoise = np.random.normal(0, 0.2, len(x))\ndata_y = y + noise\n\n# Plot the quadratic function and data points\nplt.plot(x, y, label='Quadratic Function', color='blue')\nplt.scatter(x, data_y, label='Data Points', color='red', alpha=0.5)\nplt.xlabel('x')\nplt.ylabel('Q(x)')\nplt.title('Quadratic Function')\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/nonlinearRegression/index.html#exponential",
    "href": "posts/nonlinearRegression/index.html#exponential",
    "title": "Nonlinear Regression",
    "section": "4. Exponential",
    "text": "4. Exponential\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the exponential function (e.g., E(x) = a * e^(bx))\na, b = 1, 0.5\n\n# Generate x values\nx = np.linspace(0, 5, 100)\n\n# Calculate the corresponding y values using the exponential function\ny = a * np.exp(b * x)\n\n# Generate random data points with noise\nnp.random.seed(0)\nnoise = np.random.normal(0, 0.2, len(x))\ndata_y = y + noise\n\n# Plot the exponential function and data points\nplt.plot(x, y, label='Exponential Function', color='blue')\nplt.scatter(x, data_y, label='Data Points', color='red', alpha=0.5)\nplt.xlabel('x')\nplt.ylabel('E(x)')\nplt.title('Exponential Function')\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/nonlinearRegression/index.html#logarithmic",
    "href": "posts/nonlinearRegression/index.html#logarithmic",
    "title": "Nonlinear Regression",
    "section": "5. Logarithmic",
    "text": "5. Logarithmic\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the logarithmic function (e.g., L(x) = a * ln(bx))\na, b = 1, 2\n\n# Generate x values\nx = np.linspace(0.1, 5, 100)\n\n# Calculate the corresponding y values using the logarithmic function\ny = a * np.log(b * x)\n\n# Generate random data points with noise\nnp.random.seed(0)\nnoise = np.random.normal(0, 0.2, len(x))\ndata_y = y + noise\n\n# Plot the logarithmic function and data points\nplt.plot(x, y, label='Logarithmic Function', color='blue')\nplt.scatter(x, data_y, label='Data Points', color='red', alpha=0.5)\nplt.xlabel('x')\nplt.ylabel('L(x)')\nplt.title('Logarithmic Function')\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/nonlinearRegression/index.html#sigmoidallogistic",
    "href": "posts/nonlinearRegression/index.html#sigmoidallogistic",
    "title": "Nonlinear Regression",
    "section": "6. Sigmoidal/Logistic",
    "text": "6. Sigmoidal/Logistic\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the sigmoidal/logistic function (e.g., S(x) = 1 / (1 + e^(-x)))\nx = np.linspace(-5, 5, 100)\n\n# Calculate the corresponding y values using the sigmoidal/logistic function\ny = 1 / (1 + np.exp(-x))\n\n# Generate random data points with noise\nnp.random.seed(0)\nnoise = np.random.normal(0, 0.05, len(x))\ndata_y = y + noise\n\n# Plot the sigmoidal/logistic function and data points\nplt.plot(x, y, label='Sigmoidal/Logistic Function', color='blue')\nplt.scatter(x, data_y, label='Data Points', color='red', alpha=0.5)\nplt.xlabel('x')\nplt.ylabel('S(x)')\nplt.title('Sigmoidal/Logistic')\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/anomaly detecter/index.html",
    "href": "posts/anomaly detecter/index.html",
    "title": "Anomaly/Outlier detection",
    "section": "",
    "text": "Image Source: https://miro.medium.com/v2/resize:fit:720/format:webp/1*BaOiTGWAoFi-3_-E-rJGdg.jpeg"
  },
  {
    "objectID": "posts/anomaly detecter/index.html#anomalyoutlier-detection-an-overview",
    "href": "posts/anomaly detecter/index.html#anomalyoutlier-detection-an-overview",
    "title": "Anomaly/Outlier detection",
    "section": "Anomaly/Outlier Detection: An Overview",
    "text": "Anomaly/Outlier Detection: An Overview\nAnomaly detection, also known as outlier detection, is a process in data analysis where unusual patterns, items, or events in a dataset are identified. These anomalies are often referred to as outliers—data points that deviate significantly from the majority of the data. Anomaly detection is crucial as these outliers can indicate critical incidents, such as bank fraud, structural defects, system faults, or errors in text data."
  },
  {
    "objectID": "posts/anomaly detecter/index.html#key-concepts",
    "href": "posts/anomaly detecter/index.html#key-concepts",
    "title": "Anomaly/Outlier detection",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nAnomalies:\nA data point or a pattern that does not conform to the expected behavior. Anomalies can be:\nPoint Anomalies: Individual data points that are significantly different from the rest of the data. Contextual Anomalies: Anomalies that are context-specific. These might not be outliers in a different context. Collective Anomalies: A group of data points that collectively deviate from the overall data pattern but might not be anomalies when considered individually.\n\n\nOutlier:\nOften used interchangeably with anomalies, outliers are data points that differ drastically from the rest of the dataset."
  },
  {
    "objectID": "posts/anomaly detecter/index.html#techniques-in-anomaly-detection",
    "href": "posts/anomaly detecter/index.html#techniques-in-anomaly-detection",
    "title": "Anomaly/Outlier detection",
    "section": "Techniques in Anomaly Detection",
    "text": "Techniques in Anomaly Detection\nAnomaly detection in machine learning can be approached in various ways:\n\nStatistical Methods:\n\nSimple statistical metrics like mean, median, standard deviation, and interquartile ranges are used to identify outliers.\nMethods like Z-score and Grubbs’ test can flag data points that deviate from the expected distribution.\n\n\n\nMachine Learning Methods:\n\nSupervised Learning: This requires a dataset with labeled anomalies. Algorithms like logistic regression, support vector machines, or neural networks can be trained to recognize and predict anomalies.\nUnsupervised Learning: Useful when you don’t have labeled data. Algorithms like k-means clustering, DBSCAN, or autoencoders can detect anomalies by understanding the data’s inherent structure and distribution.\nSemi-Supervised Learning: Involves training on a primarily normal dataset to understand the pattern of normality, against which anomalies can be detected.\n\n\n\nProximity-Based Methods:\nThese methods identify anomalies based on the distance or similarity between data points. For example, k-nearest neighbors (KNN) can be used to detect points that are far from their neighbors.\n\n\nDensity-Based Methods:\nTechniques like Local Outlier Factor (LOF) focus on the density of the area around a data point. Anomalies are typically located in low-density regions.\n\n\nClustering-Based Methods:\nAlgorithms like K-means or Hierarchical Clustering can group similar data together. Points that do not fit well into any cluster may be considered anomalies."
  },
  {
    "objectID": "posts/anomaly detecter/index.html#applications-of-anomaly-detection",
    "href": "posts/anomaly detecter/index.html#applications-of-anomaly-detection",
    "title": "Anomaly/Outlier detection",
    "section": "Applications of Anomaly Detection",
    "text": "Applications of Anomaly Detection\nFraud Detection: In banking and finance, detecting unusual transactions that could indicate fraud.\nIntrusion Detection: In cybersecurity, identifying unusual patterns that could signify a security breach.\nFault Detection: In industrial settings, detecting irregularities in machine or system operations to preempt failures.\nHealth Monitoring: In healthcare, identifying unusual patterns in patient data that could indicate medical issues.\nQuality Control: In manufacturing, detecting products that don’t meet quality standards."
  },
  {
    "objectID": "posts/anomaly detecter/index.html#anomaly-detection-method-with-weight-and-height-dataset",
    "href": "posts/anomaly detecter/index.html#anomaly-detection-method-with-weight-and-height-dataset",
    "title": "Anomaly/Outlier detection",
    "section": "Anomaly Detection Method with Weight and Height Dataset",
    "text": "Anomaly Detection Method with Weight and Height Dataset\nImporting libraries and loading data\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\n\n\ndf = pd.read_csv(\"R:/Blog/posts/anomaly detecter/weight-height.csv\")\nFEATURE_COLUMNS = ['Height', 'Weight']\nprint(df.shape)\ndf.sample(5).T\n\n(10000, 3)\n\n\n\n\n\n\n\n\n\n2260\n6017\n9120\n4282\n9845\n\n\n\n\nGender\nMale\nFemale\nFemale\nMale\nFemale\n\n\nHeight\n68.235886\n64.537048\n65.191183\n64.815009\n62.12748\n\n\nWeight\n186.742654\n130.67026\n130.516731\n159.940417\n136.783022"
  },
  {
    "objectID": "posts/anomaly detecter/index.html#interquartile-range-iqr",
    "href": "posts/anomaly detecter/index.html#interquartile-range-iqr",
    "title": "Anomaly/Outlier detection",
    "section": "Interquartile range (IQR)",
    "text": "Interquartile range (IQR)\n\ndef plot_results(df, df0, method_str=\"IQR\"):\n    # Plotting the data\n    plt.scatter(df['Height'], df['Weight'], label='Data', alpha=0.25)\n    plt.scatter(df0['Height'], df0['Weight'], color='r', alpha=1, label=f'Outliers, {method_str}')\n    plt.xlabel('Height')\n    plt.ylabel('Weight')\n    plt.title(f'Outlier Detection using {method_str}')\n    plt.legend()\n    plt.show();\n    return None\n\n\ndef detect_outliers_iqr(dataframe, column):\n    '''Uses Interquartile range (IQR) method to detect outliers'''\n    # Calculate the first quartile (Q1)\n    Q1 = dataframe[column].quantile(0.25)\n    # Calculate the third quartile (Q3)\n    Q3 = dataframe[column].quantile(0.75)\n    # Calculate the interquartile range (IQR)\n    IQR = Q3 - Q1\n    # Define the lower and upper bounds for outliers\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    # Find the outliers\n    dataframe[f'outliers_iqr_{column}'] = ((dataframe[column] &lt; lower_bound) | (dataframe[column] &gt; upper_bound)).astype(int)\n    return dataframe\n\n\nfor col in ['Weight', 'Height']:\n    df = detect_outliers_iqr(df, col)\ndf0 = df[df['outliers_iqr_Height']+df['outliers_iqr_Weight']&gt;0].copy()\nprint(df0.shape)\nplot_results(df, df0, method_str=\"IQR\")\n\n(8, 5)"
  },
  {
    "objectID": "posts/anomaly detecter/index.html#isolation-forest",
    "href": "posts/anomaly detecter/index.html#isolation-forest",
    "title": "Anomaly/Outlier detection",
    "section": "Isolation forest",
    "text": "Isolation forest\n\ndef detect_outliers_isolation_forest(dataframe):\n    # Create an Isolation Forest instance\n    clf = IsolationForest(contamination=0.01, n_estimators=100, bootstrap=False, random_state=42)\n    # Fit the model\n    clf.fit(dataframe[FEATURE_COLUMNS])\n    # Predict outliers\n    outliers = clf.predict(dataframe[FEATURE_COLUMNS])\n    # Create a boolean mask for outliers\n    outliers_mask = outliers == -1\n    # Mark the outliers\n    dataframe[f'outliers_iforest'] = (outliers_mask).astype(int)\n    return dataframe\n\n# Detect outliers using the IF method on column\ndf = detect_outliers_isolation_forest(df)\n\nprint(\"Outliers:\")\n    \n# Plotting the data\ndf0 = df[df['outliers_iforest']&gt;0].copy()\nprint(df0.shape)\nplot_results(df, df0, method_str=\"Isolation Forest\")\ndf.describe()\n\nOutliers:\n(100, 6)\n\n\n\n\n\n\n\n\n\n\n\n\nHeight\nWeight\noutliers_iqr_Weight\noutliers_iqr_Height\noutliers_iforest\n\n\n\n\ncount\n10000.000000\n10000.000000\n10000.0000\n10000.000000\n10000.000000\n\n\nmean\n66.367560\n161.440357\n0.0001\n0.000800\n0.010000\n\n\nstd\n3.847528\n32.108439\n0.0100\n0.028274\n0.099504\n\n\nmin\n54.263133\n64.700127\n0.0000\n0.000000\n0.000000\n\n\n25%\n63.505620\n135.818051\n0.0000\n0.000000\n0.000000\n\n\n50%\n66.318070\n161.212928\n0.0000\n0.000000\n0.000000\n\n\n75%\n69.174262\n187.169525\n0.0000\n0.000000\n0.000000\n\n\nmax\n78.998742\n269.989699\n1.0000\n1.000000\n1.000000"
  },
  {
    "objectID": "posts/anomaly detecter/index.html#local-outlier-factor",
    "href": "posts/anomaly detecter/index.html#local-outlier-factor",
    "title": "Anomaly/Outlier detection",
    "section": "Local Outlier Factor",
    "text": "Local Outlier Factor\n\ndef detect_outliers_lof(dataframe):\n    # Create an LOF instance\n    lof = LocalOutlierFactor(n_neighbors=20, contamination='auto')\n    # Fit the model and predict outlier scores\n    outlier_scores = lof.fit_predict(dataframe[FEATURE_COLUMNS])\n    # Create a boolean mask for outliers\n    outliers_mask = outlier_scores == -1\n    # Mark the outliers\n    dataframe[f'outliers_lof'] = (outliers_mask).astype(int)\n    return dataframe\n\n# Detect outliers using the LOF method on\ndf = detect_outliers_lof(df)\n\nprint(\"Outliers:\")\n    \n# Plotting the data\ndf0 = df[df['outliers_lof']+df['outliers_lof']&gt;0].copy()\nprint(df0.shape)\nplot_results(df, df0, method_str=\"LocalOutlierFactor\")\n\ndf.describe()\n\nOutliers:\n(132, 7)\n\n\n\n\n\n\n\n\n\n\n\n\nHeight\nWeight\noutliers_iqr_Weight\noutliers_iqr_Height\noutliers_iforest\noutliers_lof\n\n\n\n\ncount\n10000.000000\n10000.000000\n10000.0000\n10000.000000\n10000.000000\n10000.000000\n\n\nmean\n66.367560\n161.440357\n0.0001\n0.000800\n0.010000\n0.013200\n\n\nstd\n3.847528\n32.108439\n0.0100\n0.028274\n0.099504\n0.114136\n\n\nmin\n54.263133\n64.700127\n0.0000\n0.000000\n0.000000\n0.000000\n\n\n25%\n63.505620\n135.818051\n0.0000\n0.000000\n0.000000\n0.000000\n\n\n50%\n66.318070\n161.212928\n0.0000\n0.000000\n0.000000\n0.000000\n\n\n75%\n69.174262\n187.169525\n0.0000\n0.000000\n0.000000\n0.000000\n\n\nmax\n78.998742\n269.989699\n1.0000\n1.000000\n1.000000\n1.000000"
  },
  {
    "objectID": "posts/anomaly detecter/index.html#one-class-svm",
    "href": "posts/anomaly detecter/index.html#one-class-svm",
    "title": "Anomaly/Outlier detection",
    "section": "One-class SVM",
    "text": "One-class SVM\n\ndef detect_outliers_svm(dataframe):\n    # Create a One-Class SVM instance\n    svm = OneClassSVM(nu=0.01)\n    # Fit the model\n    svm.fit(dataframe[FEATURE_COLUMNS])\n    # Predict outlier scores\n    outlier_scores = svm.decision_function(dataframe[FEATURE_COLUMNS])\n    # Create a boolean mask for outliers\n    outliers_mask = outlier_scores &lt; 0\n    # Mark the outliers\n    dataframe[f'outliers_svm'] = (outliers_mask).astype(int)\n    return dataframe\n\n# Detect outliers using the SVM method\ndf = detect_outliers_svm(df)\n\nprint(\"Outliers:\")\n    \n# Plotting the data\ndf0 = df[df['outliers_svm']&gt;0].copy()\nprint(df0.shape)\nplot_results(df, df0, method_str=\"One-class SVM\")\ndf.describe()\n\nOutliers:\n(101, 8)\n\n\n\n\n\n\n\n\n\n\n\n\nHeight\nWeight\noutliers_iqr_Weight\noutliers_iqr_Height\noutliers_iforest\noutliers_lof\noutliers_svm\n\n\n\n\ncount\n10000.000000\n10000.000000\n10000.0000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n\n\nmean\n66.367560\n161.440357\n0.0001\n0.000800\n0.010000\n0.013200\n0.010100\n\n\nstd\n3.847528\n32.108439\n0.0100\n0.028274\n0.099504\n0.114136\n0.099995\n\n\nmin\n54.263133\n64.700127\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n63.505620\n135.818051\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n66.318070\n161.212928\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n75%\n69.174262\n187.169525\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nmax\n78.998742\n269.989699\n1.0000\n1.000000\n1.000000\n1.000000\n1.000000"
  },
  {
    "objectID": "posts/anomaly detecter/index.html#autoencoder",
    "href": "posts/anomaly detecter/index.html#autoencoder",
    "title": "Anomaly/Outlier detection",
    "section": "Autoencoder",
    "text": "Autoencoder\n\nclass Autoencoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super(Autoencoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, 2*hidden_dim),\n            nn.ReLU(),\n            nn.Linear(2*hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(hidden_dim, 2*hidden_dim),\n            nn.ReLU(),\n            nn.Linear(2*hidden_dim, input_dim),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n\nclass OutlierDataset(Dataset):\n    def __init__(self, data):\n        self.data = torch.tensor(data, dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\ndef detect_outliers_autoencoder(dataframe, hidden_dim=16, num_epochs=10, batch_size=32):\n    # Convert dataframe to standard scaled numpy array\n    scaler = StandardScaler()\n    data = scaler.fit_transform(dataframe[FEATURE_COLUMNS])\n\n    # Create an outlier dataset\n    dataset = OutlierDataset(data)\n\n    # Split data into training and validation sets\n    val_split = int(0.2 * len(dataset))\n    train_set, val_set = torch.utils.data.random_split(dataset, [len(dataset) - val_split, val_split])\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_set, batch_size=batch_size)\n\n    # Initialize the autoencoder model\n    input_dim = data.shape[1]\n    model = Autoencoder(input_dim, hidden_dim)\n\n    # Set the loss function and optimizer\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=3e-2, weight_decay=1e-8)\n\n    # Train the autoencoder\n    for epoch in range(num_epochs):\n        train_loss = 0.0\n        val_loss = 0.0\n\n        # Training\n        model.train()\n        for batch in train_loader:\n            # Zero the gradients\n            optimizer.zero_grad()\n            # Forward pass\n            outputs = model(batch)\n            # Compute the reconstruction loss\n            loss = criterion(outputs, batch)\n            # Backward pass and optimization\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * batch.size(0)\n\n        # Validation\n        model.eval()\n        with torch.no_grad():\n            for batch in val_loader:\n                outputs = model(batch)\n                loss = criterion(outputs, batch)\n                val_loss += loss.item() * batch.size(0)\n\n        train_loss /= len(train_set)\n        val_loss /= len(val_set)\n\n\n        print(f'Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n        \n    # Calculate reconstruction error for each data point\n    reconstructed = model(dataset.data)\n    mse_loss = nn.MSELoss(reduction='none')\n    error = torch.mean(mse_loss(reconstructed, dataset.data), dim=1)\n\n    \n    # Define a threshold for outlier detection\n    threshold = np.percentile(error.detach().numpy(), 99)\n\n    # Create a boolean mask for outliers\n    outliers_mask = (error &gt; threshold)\n\n    # Mark the outliers\n    dataframe[f'outliers_autoenc'] = (outliers_mask)\n    dataframe[f'outliers_autoenc'] = dataframe[f'outliers_autoenc'].astype(int)\n    return dataframe\n\n# Detect outliers using the autoencoder method\ndf = detect_outliers_autoencoder(df)\n\nprint(\"Outliers:\")\n    \n# Plotting the data\ndf0 = df[df['outliers_autoenc']&gt;0].copy()\nprint(df0.shape)\nplot_results(df, df0, method_str=\"One-class Autoencoders\")\n\ndf.describe()\n\nEpoch 1/10: Train Loss: 0.5699, Val Loss: 0.5704\nEpoch 2/10: Train Loss: 0.5517, Val Loss: 0.5696\nEpoch 3/10: Train Loss: 0.5510, Val Loss: 0.5705\nEpoch 4/10: Train Loss: 0.5512, Val Loss: 0.5692\nEpoch 5/10: Train Loss: 0.5510, Val Loss: 0.5700\nEpoch 6/10: Train Loss: 0.5511, Val Loss: 0.5692\nEpoch 7/10: Train Loss: 0.5508, Val Loss: 0.5693\nEpoch 8/10: Train Loss: 0.5507, Val Loss: 0.5696\nEpoch 9/10: Train Loss: 0.5511, Val Loss: 0.5729\nEpoch 10/10: Train Loss: 0.5513, Val Loss: 0.5692\nOutliers:\n(100, 9)\n\n\n\n\n\n\n\n\n\n\n\n\nHeight\nWeight\noutliers_iqr_Weight\noutliers_iqr_Height\noutliers_iforest\noutliers_lof\noutliers_svm\noutliers_autoenc\n\n\n\n\ncount\n10000.000000\n10000.000000\n10000.0000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n\n\nmean\n66.367560\n161.440357\n0.0001\n0.000800\n0.010000\n0.013200\n0.010100\n0.010000\n\n\nstd\n3.847528\n32.108439\n0.0100\n0.028274\n0.099504\n0.114136\n0.099995\n0.099504\n\n\nmin\n54.263133\n64.700127\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n63.505620\n135.818051\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n66.318070\n161.212928\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n75%\n69.174262\n187.169525\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nmax\n78.998742\n269.989699\n1.0000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000"
  },
  {
    "objectID": "posts/linearRegression/index.html#definition",
    "href": "posts/linearRegression/index.html#definition",
    "title": "Linear Regression",
    "section": "Definition:",
    "text": "Definition:\n\nLinear regression is a statistical method used in data science and machine learning for predictive analysis.\nIt establishes a linear relationship between an independent variable (predictor) and a dependent variable (outcome) for prediction.\nThe method is suitable for continuous or numeric variables like sales, salary, age, etc."
  },
  {
    "objectID": "posts/linearRegression/index.html#importance-in-various-fields",
    "href": "posts/linearRegression/index.html#importance-in-various-fields",
    "title": "Linear Regression",
    "section": "Importance in Various Fields:",
    "text": "Importance in Various Fields:\n\nUsed in stock market forecasting, portfolio management, scientific analysis, and more.\nA simple representation is a sloped straight line in a graph, depicting the best fit line for a set of data."
  },
  {
    "objectID": "posts/linearRegression/index.html#benefits-of-linear-regression",
    "href": "posts/linearRegression/index.html#benefits-of-linear-regression",
    "title": "Linear Regression",
    "section": "Benefits of Linear Regression:",
    "text": "Benefits of Linear Regression:\n\nEasy to implement and interpret.\nScalable and optimal for online settings due to its computational efficiency."
  },
  {
    "objectID": "posts/linearRegression/index.html#linear-regression-equation",
    "href": "posts/linearRegression/index.html#linear-regression-equation",
    "title": "Linear Regression",
    "section": "Linear Regression Equation:",
    "text": "Linear Regression Equation:\nThe equation Y = mX + b, where ‘m’ is the slope and ‘b’ is the intercept, describes the relationship. In machine learning, it’s often expressed as y(x) = p0 + p1  x, where p0 and p1 are parameters to be determined."
  },
  {
    "objectID": "posts/linearRegression/index.html#types-of-linear-regression",
    "href": "posts/linearRegression/index.html#types-of-linear-regression",
    "title": "Linear Regression",
    "section": "Types of Linear Regression:",
    "text": "Types of Linear Regression:\n\nSimple Linear Regression:\nDefinition: Simple Linear Regression is a statistical method that allows us to summarize and study relationships between two continuous (quantitative) variables. This method assumes a linear relationship between the dependent variable and the independent variable.\nEquation: Y = β0 + β1 * X + ε\n\nY: Dependent variable\nX: Independent variable\nβ0: Intercept of the regression line\nβ1: Slope of the regression line\nε: Error term\n\nApplication: For example, predicting the price of a house (dependent variable) based on its size (independent variable)."
  },
  {
    "objectID": "posts/linearRegression/index.html#multiple-linear-regression",
    "href": "posts/linearRegression/index.html#multiple-linear-regression",
    "title": "Linear Regression",
    "section": "Multiple Linear Regression:",
    "text": "Multiple Linear Regression:\nDefinition: Multiple Linear Regression is an extension of simple linear regression and is used to predict the outcome of a dependent variable based on the value of two or more independent variables. This method helps in understanding how changes in independent variables are associated with changes in the dependent variable.\nEquation: Y = β0 + β1 * X1 + β2 * X2 + … + βn * Xn + ε\n\nY: Dependent variable\nX1, X2, …, Xn: Independent variables\nβ0: Intercept\nβ1, β2, …, βn: Slopes for each independent variable\nε: Error term\n\nApplication: Predicting a person’s weight based on their height, age, and diet (three independent variables)."
  },
  {
    "objectID": "posts/linearRegression/index.html#logistic-regression",
    "href": "posts/linearRegression/index.html#logistic-regression",
    "title": "Linear Regression",
    "section": "Logistic Regression:",
    "text": "Logistic Regression:\nDefinition: Logistic Regression is a statistical method for analyzing a dataset in which there are one or more independent variables that determine an outcome. The outcome is measured with a dichotomous variable (in which there are only two possible outcomes). It is used for predicting the probability of a binary outcome based on one or more predictor variables.\nEquation: log(p/(1-p)) = β0 + β1 * X1 + … + βn * Xn\n\np: Probability of the dependent event occurring\nX1, …, Xn: Predictor variables\nβ0: Intercept\nβ1, …, βn: Coefficients for each predictor\n\nApplication: Determining whether an email is spam or not spam, based on features like the email’s content, sender, etc."
  },
  {
    "objectID": "posts/linearRegression/index.html#ordinal-regression",
    "href": "posts/linearRegression/index.html#ordinal-regression",
    "title": "Linear Regression",
    "section": "Ordinal Regression:",
    "text": "Ordinal Regression:\nDefinition: Ordinal Regression is used when the dependent variable is ordinal, meaning it reflects a scale of magnitude. It models the relationship between a set of predictor variables and an ordinal scale dependent variable, where the categories have a natural order, but the intervals between them are not assumed to be equidistant.\nCharacteristics: The categories have a ranked order, but the intervals between the ranks are not necessarily equal.\nApplication: Rating a movie as poor, fair, good, very good, and excellent. Here, the ratings have a natural order but the difference between each category is not quantified."
  },
  {
    "objectID": "posts/linearRegression/index.html#multinomial-logistic-regression",
    "href": "posts/linearRegression/index.html#multinomial-logistic-regression",
    "title": "Linear Regression",
    "section": "Multinomial Logistic Regression:",
    "text": "Multinomial Logistic Regression:\nDefinition: Multinomial Logistic Regression is a classification method that generalizes logistic regression to multiclass problems, i.e., where the dependent variable can have more than two possible nominal (unordered) outcomes. It is used when the outcome involves more than two categories.\nCharacteristics: Similar to logistic regression, but suitable for more than two classes.\nApplication: Predicting the choice of transportation (like car, bus, train, bike) based on factors like distance, cost, and time."
  },
  {
    "objectID": "posts/linearRegression/index.html#example-yearly-amount-spent-prediction",
    "href": "posts/linearRegression/index.html#example-yearly-amount-spent-prediction",
    "title": "Linear Regression",
    "section": "Example: Yearly Amount Spent Prediction",
    "text": "Example: Yearly Amount Spent Prediction\nImport Libraries and Reading Data\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.set_option('display.float_format', lambda x: '%.2f' % x)\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\ndf = pd.read_csv(\"R:/Blog/posts/linearRegression/Ecommerce Customers.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nEmail\nAddress\nAvatar\nAvg. Session Length\nTime on App\nTime on Website\nLength of Membership\nYearly Amount Spent\n\n\n\n\n0\nmstephenson@fernandez.com\n835 Frank Tunnel\\nWrightmouth, MI 82180-9605\nViolet\n34.50\n12.66\n39.58\n4.08\n587.95\n\n\n1\nhduke@hotmail.com\n4547 Archer Common\\nDiazchester, CA 06566-8576\nDarkGreen\n31.93\n11.11\n37.27\n2.66\n392.20\n\n\n2\npallen@yahoo.com\n24645 Valerie Unions Suite 582\\nCobbborough, D...\nBisque\n33.00\n11.33\n37.11\n4.10\n487.55\n\n\n3\nriverarebecca@gmail.com\n1414 David Throughway\\nPort Jason, OH 22070-1220\nSaddleBrown\n34.31\n13.72\n36.72\n3.12\n581.85\n\n\n4\nmstephens@davidson-herman.com\n14023 Rodriguez Passage\\nPort Jacobville, PR 3...\nMediumAquaMarine\n33.33\n12.80\n37.54\n4.45\n599.41\n\n\n\n\n\n\n\nThe variables to be used in the data were determined.\n\ndf = df.drop(['Email',\"Address\",\"Avatar\"], axis=1) \n\nAdvanced Functional Exploratory Data Analysis\nGeneral structure of the data is analyzed\n\ndef check_df(dataframe, head=5):\n    print(\"##################### Columns #####################\")\n    print(dataframe.columns)\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(head))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.describe([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n\ncheck_df(df)\n\n##################### Columns #####################\nIndex(['Avg. Session Length', 'Time on App', 'Time on Website',\n       'Length of Membership', 'Yearly Amount Spent'],\n      dtype='object')\n##################### Shape #####################\n(500, 5)\n##################### Types #####################\nAvg. Session Length     float64\nTime on App             float64\nTime on Website         float64\nLength of Membership    float64\nYearly Amount Spent     float64\ndtype: object\n##################### Head #####################\n   Avg. Session Length  Time on App  Time on Website  Length of Membership  \\\n0                34.50        12.66            39.58                  4.08   \n1                31.93        11.11            37.27                  2.66   \n2                33.00        11.33            37.11                  4.10   \n3                34.31        13.72            36.72                  3.12   \n4                33.33        12.80            37.54                  4.45   \n\n   Yearly Amount Spent  \n0               587.95  \n1               392.20  \n2               487.55  \n3               581.85  \n4               599.41  \n##################### Tail #####################\n     Avg. Session Length  Time on App  Time on Website  Length of Membership  \\\n495                33.24        13.57            36.42                  3.75   \n496                34.70        11.70            37.19                  3.58   \n497                32.65        11.50            38.33                  4.96   \n498                33.32        12.39            36.84                  2.34   \n499                33.72        12.42            35.77                  2.74   \n\n     Yearly Amount Spent  \n495               573.85  \n496               529.05  \n497               551.62  \n498               456.47  \n499               497.78  \n##################### NA #####################\nAvg. Session Length     0\nTime on App             0\nTime on Website         0\nLength of Membership    0\nYearly Amount Spent     0\ndtype: int64\n##################### Quantiles #####################\n                      count   mean   std    min     0%     5%    50%    95%  \\\nAvg. Session Length  500.00  33.05  0.99  29.53  29.53  31.45  33.08  34.59   \nTime on App          500.00  12.05  0.99   8.51   8.51  10.53  11.98  13.67   \nTime on Website      500.00  37.06  1.01  33.91  33.91  35.46  37.07  38.78   \nLength of Membership 500.00   3.53  1.00   0.27   0.27   1.81   3.53   5.08   \nYearly Amount Spent  500.00 499.31 79.31 256.67 256.67 376.29 498.89 628.15   \n\n                        99%   100%    max  \nAvg. Session Length   35.43  36.14  36.14  \nTime on App           14.22  15.13  15.13  \nTime on Website       39.25  40.01  40.01  \nLength of Membership   5.84   6.92   6.92  \nYearly Amount Spent  701.00 765.52 765.52  \n\n\nA scatterplot to observe the relationship between the variables was created.\n\nsns.pairplot(df, kind = \"reg\")\n\n\n\n\nAnalysis of Variable Types\nIt is necessary to determine the types of variables. Thus, we can determine the types of the variables and make them suitable for the model.\nIt gives the names of the numeric, categorical but cardinal variables in the data set.\ncat_cols: Categorical variable list\nnum_cols: Numeric variable list\ncat_but_car: Categorical but cardinal variable list\nThe function named grab_col_names helps to determine the types of variables.\n\ndef grab_col_names(dataframe, cat_th=10, car_th=20):\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() &lt; cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() &gt; car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)\n\nObservations: 500\nVariables: 5\ncat_cols: 0\nnum_cols: 5\ncat_but_car: 0\nnum_but_cat: 0\n\n\nOutlier Analysis\nValues that go far beyond the general trend in the data are called outliers. Especially in linear methods, the effects of outliers are more severe.Outliers cause bias in the data set.For all these reasons, it needs to be analyzed.\n\ndef outlier_thresholds(dataframe, col_name, q1=0.10, q3=0.90):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\ndef check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] &gt; up_limit) | (dataframe[col_name] &lt; low_limit)].any(axis=None):\n        return True\n    else:\n        return False\nfor col in num_cols:\n    print(col, check_outlier(df, col))\n\nAvg. Session Length False\nTime on App False\nTime on Website False\nLength of Membership False\nYearly Amount Spent False\n\n\nAvg. Session Length False Time on App False Time on Website False Length of Membership False Yearly Amount Spent False\nAnalysis Of Missing Values\nMissing values may cause problems while setting up the model. It must be detected and necessary actions must be taken.\nNo missing values were found for the relevant data.\n\ndef missing_values_table(dataframe, na_name=False):\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() &gt; 0]\n\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[na_columns].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df, end=\"\\n\")\n\n    if na_name:\n        return na_columns\n\nmissing_values_table(df)\n\nEmpty DataFrame\nColumns: [n_miss, ratio]\nIndex: []\n\n\nEmpty DataFrame Columns: [n_miss, ratio] Index: []\nCorrelation Analysis\nValues with high correlation affect the target variable to a similar extent. Therefore, we can eliminate one of the variables with high correlation between two variables and use the other.\nWhen the data was examined, no variable with a high correlation of more than 90% was found.\n\ndef high_correlated_cols(dataframe, plot=False, corr_th=0.90):\n    corr = dataframe.corr()\n    cor_matrix = corr.abs()\n    upper_triangle_matrix = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(bool))\n    drop_list = [col for col in upper_triangle_matrix.columns if any(upper_triangle_matrix[col] &gt; corr_th)]\n    if plot:\n        import seaborn as sns\n        import matplotlib.pyplot as plt\n        sns.set(rc={'figure.figsize': (10, 5)})\n        sns.heatmap(corr, cmap=\"RdBu\", annot=True)\n        plt.show(block=True)\n    return drop_list\n\n\nhigh_correlated_cols(df,plot=True)\n\n\n\n\n[]"
  },
  {
    "objectID": "posts/linearRegression/index.html#linear-regression",
    "href": "posts/linearRegression/index.html#linear-regression",
    "title": "Linear Regression",
    "section": "LINEAR REGRESSION",
    "text": "LINEAR REGRESSION\n\nLinear regression models the relationship between dependent and independent variable/variables linearly.\nIn order to create the model, dependent and independent variables were defined.\n\nX = df.drop('Yearly Amount Spent', axis=1) \n\ny = df[[\"Yearly Amount Spent\"]]\n\nBuilding the Model\nBuilding the Model A train and test set by dividing the data into two is created . By training the model with one part and testing the model with the other part, it can be determined how successful the model is.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n\nreg_model = LinearRegression().fit(X_train, y_train)\n\n# constant (b - bias)\nprint(reg_model.intercept_)\n\n# coefficients (w - weights)\nprint(reg_model.coef_)\n\n[-1047.73920526]\n[[25.78854257 38.85150472  0.25638467 61.49204989]]\n\n\nPrediction of dependent variable\n\ny_pred = reg_model.predict(X_test)"
  },
  {
    "objectID": "posts/linearRegression/index.html#evaluating-forecast-success",
    "href": "posts/linearRegression/index.html#evaluating-forecast-success",
    "title": "Linear Regression",
    "section": "Evaluating Forecast Success",
    "text": "Evaluating Forecast Success\n!(rmse.png)\n\nnp.sqrt(mean_squared_error(y_test, y_pred))\n\n8.84848631350032\n\n\nThe ratio of independent variables description of dependent variable\n\nreg_model.score(X_test, y_test)\n\n0.9892888134002329"
  },
  {
    "objectID": "posts/linearRegression/index.html#visualization-of-the-model",
    "href": "posts/linearRegression/index.html#visualization-of-the-model",
    "title": "Linear Regression",
    "section": "Visualization of the Model",
    "text": "Visualization of the Model\nFinally, the actual values corresponding to the predicted values of the model are shown in the graph.\n\ny_pred = pd.DataFrame(y_pred)\ny_test = y_test.reset_index(drop=True)\ndf_ = pd.concat([y_test,y_pred], axis=1)\ndf_.columns = [\"y_test\",\"y_pred\"]\nplt.figure(figsize=(15,10))\nplt.plot(df_)\nplt.legend([\"ACTUAL VALUES\" , \"MODEL PREDICTION\"])\n\n&lt;matplotlib.legend.Legend at 0x1fe04f5f7f0&gt;"
  },
  {
    "objectID": "posts/nonlinearRegression/index.html#examples-of-nonlinear-regression",
    "href": "posts/nonlinearRegression/index.html#examples-of-nonlinear-regression",
    "title": "Nonlinear Regression",
    "section": "Examples of Nonlinear Regression",
    "text": "Examples of Nonlinear Regression\nI.Introduction\nWhen you’re looking at data that seems to form curves or non-straight patterns, using linear regression might not give you the most accurate results. Linear regression is like a tool designed for straight-line relationships, so when your data behaves in a curvy way, it doesn’t quite fit the tool’s assumptions. That’s where non-linear regression steps in. It’s like having a set of tools that can handle curves and bends in the data. These tools, like polynomial, exponential, or logarithmic regression, can adjust to the data’s curves and complexities, giving you more precise predictions and a better match to the real nature of the data."
  },
  {
    "objectID": "posts/nonlinearRegression/index.html",
    "href": "posts/nonlinearRegression/index.html",
    "title": "Definition",
    "section": "",
    "text": "Non-linear regression in machine learning is a powerful tool for modeling complex relationships between variables, where these relationships cannot be adequately described using a straight line. In machine learning, non-linear regression is used to predict outcomes based on non-linear interactions of predictor variables. It’s particularly useful in scenarios where the underlying data patterns are inherently non-linear."
  },
  {
    "objectID": "posts/anomaly detection/index.html",
    "href": "posts/anomaly detection/index.html",
    "title": "Anomaly/Outlier detection",
    "section": "",
    "text": "Image Source: https://miro.medium.com/v2/resize:fit:720/format:webp/1*BaOiTGWAoFi-3_-E-rJGdg.jpeg"
  },
  {
    "objectID": "posts/anomaly detection/index.html#anomalyoutlier-detection-an-overview",
    "href": "posts/anomaly detection/index.html#anomalyoutlier-detection-an-overview",
    "title": "Anomaly/Outlier detection",
    "section": "Anomaly/Outlier Detection: An Overview",
    "text": "Anomaly/Outlier Detection: An Overview\nAnomaly detection, also known as outlier detection, is a process in data analysis where unusual patterns, items, or events in a dataset are identified. These anomalies are often referred to as outliers—data points that deviate significantly from the majority of the data. Anomaly detection is crucial as these outliers can indicate critical incidents, such as bank fraud, structural defects, system faults, or errors in text data."
  },
  {
    "objectID": "posts/anomaly detection/index.html#key-concepts",
    "href": "posts/anomaly detection/index.html#key-concepts",
    "title": "Anomaly/Outlier detection",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nAnomalies:\nA data point or a pattern that does not conform to the expected behavior. Anomalies can be:\nPoint Anomalies: Individual data points that are significantly different from the rest of the data. Contextual Anomalies: Anomalies that are context-specific. These might not be outliers in a different context. Collective Anomalies: A group of data points that collectively deviate from the overall data pattern but might not be anomalies when considered individually.\n\n\nOutlier:\nOften used interchangeably with anomalies, outliers are data points that differ drastically from the rest of the dataset."
  },
  {
    "objectID": "posts/anomaly detection/index.html#techniques-in-anomaly-detection",
    "href": "posts/anomaly detection/index.html#techniques-in-anomaly-detection",
    "title": "Anomaly/Outlier detection",
    "section": "Techniques in Anomaly Detection",
    "text": "Techniques in Anomaly Detection\nAnomaly detection in machine learning can be approached in various ways:\n\nStatistical Methods:\n\nSimple statistical metrics like mean, median, standard deviation, and interquartile ranges are used to identify outliers.\nMethods like Z-score and Grubbs’ test can flag data points that deviate from the expected distribution.\n\n\n\nMachine Learning Methods:\n\nSupervised Learning: This requires a dataset with labeled anomalies. Algorithms like logistic regression, support vector machines, or neural networks can be trained to recognize and predict anomalies.\nUnsupervised Learning: Useful when you don’t have labeled data. Algorithms like k-means clustering, DBSCAN, or autoencoders can detect anomalies by understanding the data’s inherent structure and distribution.\nSemi-Supervised Learning: Involves training on a primarily normal dataset to understand the pattern of normality, against which anomalies can be detected.\n\n\n\nProximity-Based Methods:\nThese methods identify anomalies based on the distance or similarity between data points. For example, k-nearest neighbors (KNN) can be used to detect points that are far from their neighbors.\n\n\nDensity-Based Methods:\nTechniques like Local Outlier Factor (LOF) focus on the density of the area around a data point. Anomalies are typically located in low-density regions.\n\n\nClustering-Based Methods:\nAlgorithms like K-means or Hierarchical Clustering can group similar data together. Points that do not fit well into any cluster may be considered anomalies."
  },
  {
    "objectID": "posts/anomaly detection/index.html#applications-of-anomaly-detection",
    "href": "posts/anomaly detection/index.html#applications-of-anomaly-detection",
    "title": "Anomaly/Outlier detection",
    "section": "Applications of Anomaly Detection",
    "text": "Applications of Anomaly Detection\nFraud Detection: In banking and finance, detecting unusual transactions that could indicate fraud.\nIntrusion Detection: In cybersecurity, identifying unusual patterns that could signify a security breach.\nFault Detection: In industrial settings, detecting irregularities in machine or system operations to preempt failures.\nHealth Monitoring: In healthcare, identifying unusual patterns in patient data that could indicate medical issues.\nQuality Control: In manufacturing, detecting products that don’t meet quality standards."
  },
  {
    "objectID": "posts/anomaly detection/index.html#anomaly-detection-method-with-weight-and-height-dataset",
    "href": "posts/anomaly detection/index.html#anomaly-detection-method-with-weight-and-height-dataset",
    "title": "Anomaly/Outlier detection",
    "section": "Anomaly Detection Method with Weight and Height Dataset",
    "text": "Anomaly Detection Method with Weight and Height Dataset\nImporting libraries and loading data\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\n\n\ndf = pd.read_csv(\"R:/MLBlog/posts/anomaly detection/weight-height.csv\")\nFEATURE_COLUMNS = ['Height', 'Weight']\nprint(df.shape)\ndf.sample(5).T\n\n(10000, 3)\n\n\n\n\n\n\n\n\n\n9267\n3922\n9811\n7392\n7716\n\n\n\n\nGender\nFemale\nMale\nFemale\nFemale\nFemale\n\n\nHeight\n62.428114\n63.820819\n63.805528\n59.908545\n65.306095\n\n\nWeight\n146.435582\n155.187428\n138.15133\n115.525916\n141.379213"
  },
  {
    "objectID": "posts/anomaly detection/index.html#interquartile-range-iqr",
    "href": "posts/anomaly detection/index.html#interquartile-range-iqr",
    "title": "Anomaly/Outlier detection",
    "section": "Interquartile range (IQR)",
    "text": "Interquartile range (IQR)\n\ndef plot_results(df, df0, method_str=\"IQR\"):\n    # Plotting the data\n    plt.scatter(df['Height'], df['Weight'], label='Data', alpha=0.25)\n    plt.scatter(df0['Height'], df0['Weight'], color='r', alpha=1, label=f'Outliers, {method_str}')\n    plt.xlabel('Height')\n    plt.ylabel('Weight')\n    plt.title(f'Outlier Detection using {method_str}')\n    plt.legend()\n    plt.show();\n    return None\n\n\ndef detect_outliers_iqr(dataframe, column):\n    '''Uses Interquartile range (IQR) method to detect outliers'''\n    # Calculate the first quartile (Q1)\n    Q1 = dataframe[column].quantile(0.25)\n    # Calculate the third quartile (Q3)\n    Q3 = dataframe[column].quantile(0.75)\n    # Calculate the interquartile range (IQR)\n    IQR = Q3 - Q1\n    # Define the lower and upper bounds for outliers\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    # Find the outliers\n    dataframe[f'outliers_iqr_{column}'] = ((dataframe[column] &lt; lower_bound) | (dataframe[column] &gt; upper_bound)).astype(int)\n    return dataframe\n\n\nfor col in ['Weight', 'Height']:\n    df = detect_outliers_iqr(df, col)\ndf0 = df[df['outliers_iqr_Height']+df['outliers_iqr_Weight']&gt;0].copy()\nprint(df0.shape)\nplot_results(df, df0, method_str=\"IQR\")\n\n(8, 5)"
  },
  {
    "objectID": "posts/anomaly detection/index.html#isolation-forest",
    "href": "posts/anomaly detection/index.html#isolation-forest",
    "title": "Anomaly/Outlier detection",
    "section": "Isolation forest",
    "text": "Isolation forest\n\ndef detect_outliers_isolation_forest(dataframe):\n    # Create an Isolation Forest instance\n    clf = IsolationForest(contamination=0.01, n_estimators=100, bootstrap=False, random_state=42)\n    # Fit the model\n    clf.fit(dataframe[FEATURE_COLUMNS])\n    # Predict outliers\n    outliers = clf.predict(dataframe[FEATURE_COLUMNS])\n    # Create a boolean mask for outliers\n    outliers_mask = outliers == -1\n    # Mark the outliers\n    dataframe[f'outliers_iforest'] = (outliers_mask).astype(int)\n    return dataframe\n\n# Detect outliers using the IF method on column\ndf = detect_outliers_isolation_forest(df)\n\nprint(\"Outliers:\")\n    \n# Plotting the data\ndf0 = df[df['outliers_iforest']&gt;0].copy()\nprint(df0.shape)\nplot_results(df, df0, method_str=\"Isolation Forest\")\ndf.describe()\n\nOutliers:\n(100, 6)\n\n\n\n\n\n\n\n\n\n\n\n\nHeight\nWeight\noutliers_iqr_Weight\noutliers_iqr_Height\noutliers_iforest\n\n\n\n\ncount\n10000.000000\n10000.000000\n10000.0000\n10000.000000\n10000.000000\n\n\nmean\n66.367560\n161.440357\n0.0001\n0.000800\n0.010000\n\n\nstd\n3.847528\n32.108439\n0.0100\n0.028274\n0.099504\n\n\nmin\n54.263133\n64.700127\n0.0000\n0.000000\n0.000000\n\n\n25%\n63.505620\n135.818051\n0.0000\n0.000000\n0.000000\n\n\n50%\n66.318070\n161.212928\n0.0000\n0.000000\n0.000000\n\n\n75%\n69.174262\n187.169525\n0.0000\n0.000000\n0.000000\n\n\nmax\n78.998742\n269.989699\n1.0000\n1.000000\n1.000000"
  },
  {
    "objectID": "posts/anomaly detection/index.html#local-outlier-factor",
    "href": "posts/anomaly detection/index.html#local-outlier-factor",
    "title": "Anomaly/Outlier detection",
    "section": "Local Outlier Factor",
    "text": "Local Outlier Factor\n\ndef detect_outliers_lof(dataframe):\n    # Create an LOF instance\n    lof = LocalOutlierFactor(n_neighbors=20, contamination='auto')\n    # Fit the model and predict outlier scores\n    outlier_scores = lof.fit_predict(dataframe[FEATURE_COLUMNS])\n    # Create a boolean mask for outliers\n    outliers_mask = outlier_scores == -1\n    # Mark the outliers\n    dataframe[f'outliers_lof'] = (outliers_mask).astype(int)\n    return dataframe\n\n# Detect outliers using the LOF method on\ndf = detect_outliers_lof(df)\n\nprint(\"Outliers:\")\n    \n# Plotting the data\ndf0 = df[df['outliers_lof']+df['outliers_lof']&gt;0].copy()\nprint(df0.shape)\nplot_results(df, df0, method_str=\"LocalOutlierFactor\")\n\ndf.describe()\n\nOutliers:\n(132, 7)\n\n\n\n\n\n\n\n\n\n\n\n\nHeight\nWeight\noutliers_iqr_Weight\noutliers_iqr_Height\noutliers_iforest\noutliers_lof\n\n\n\n\ncount\n10000.000000\n10000.000000\n10000.0000\n10000.000000\n10000.000000\n10000.000000\n\n\nmean\n66.367560\n161.440357\n0.0001\n0.000800\n0.010000\n0.013200\n\n\nstd\n3.847528\n32.108439\n0.0100\n0.028274\n0.099504\n0.114136\n\n\nmin\n54.263133\n64.700127\n0.0000\n0.000000\n0.000000\n0.000000\n\n\n25%\n63.505620\n135.818051\n0.0000\n0.000000\n0.000000\n0.000000\n\n\n50%\n66.318070\n161.212928\n0.0000\n0.000000\n0.000000\n0.000000\n\n\n75%\n69.174262\n187.169525\n0.0000\n0.000000\n0.000000\n0.000000\n\n\nmax\n78.998742\n269.989699\n1.0000\n1.000000\n1.000000\n1.000000"
  },
  {
    "objectID": "posts/anomaly detection/index.html#one-class-svm",
    "href": "posts/anomaly detection/index.html#one-class-svm",
    "title": "Anomaly/Outlier detection",
    "section": "One-class SVM",
    "text": "One-class SVM\n\ndef detect_outliers_svm(dataframe):\n    # Create a One-Class SVM instance\n    svm = OneClassSVM(nu=0.01)\n    # Fit the model\n    svm.fit(dataframe[FEATURE_COLUMNS])\n    # Predict outlier scores\n    outlier_scores = svm.decision_function(dataframe[FEATURE_COLUMNS])\n    # Create a boolean mask for outliers\n    outliers_mask = outlier_scores &lt; 0\n    # Mark the outliers\n    dataframe[f'outliers_svm'] = (outliers_mask).astype(int)\n    return dataframe\n\n# Detect outliers using the SVM method\ndf = detect_outliers_svm(df)\n\nprint(\"Outliers:\")\n    \n# Plotting the data\ndf0 = df[df['outliers_svm']&gt;0].copy()\nprint(df0.shape)\nplot_results(df, df0, method_str=\"One-class SVM\")\ndf.describe()\n\nOutliers:\n(101, 8)\n\n\n\n\n\n\n\n\n\n\n\n\nHeight\nWeight\noutliers_iqr_Weight\noutliers_iqr_Height\noutliers_iforest\noutliers_lof\noutliers_svm\n\n\n\n\ncount\n10000.000000\n10000.000000\n10000.0000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n\n\nmean\n66.367560\n161.440357\n0.0001\n0.000800\n0.010000\n0.013200\n0.010100\n\n\nstd\n3.847528\n32.108439\n0.0100\n0.028274\n0.099504\n0.114136\n0.099995\n\n\nmin\n54.263133\n64.700127\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n63.505620\n135.818051\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n66.318070\n161.212928\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n75%\n69.174262\n187.169525\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nmax\n78.998742\n269.989699\n1.0000\n1.000000\n1.000000\n1.000000\n1.000000"
  },
  {
    "objectID": "posts/anomaly detection/index.html#autoencoder",
    "href": "posts/anomaly detection/index.html#autoencoder",
    "title": "Anomaly/Outlier detection",
    "section": "Autoencoder",
    "text": "Autoencoder\n\nclass Autoencoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super(Autoencoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, 2*hidden_dim),\n            nn.ReLU(),\n            nn.Linear(2*hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(hidden_dim, 2*hidden_dim),\n            nn.ReLU(),\n            nn.Linear(2*hidden_dim, input_dim),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n\nclass OutlierDataset(Dataset):\n    def __init__(self, data):\n        self.data = torch.tensor(data, dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\ndef detect_outliers_autoencoder(dataframe, hidden_dim=16, num_epochs=10, batch_size=32):\n    # Convert dataframe to standard scaled numpy array\n    scaler = StandardScaler()\n    data = scaler.fit_transform(dataframe[FEATURE_COLUMNS])\n\n    # Create an outlier dataset\n    dataset = OutlierDataset(data)\n\n    # Split data into training and validation sets\n    val_split = int(0.2 * len(dataset))\n    train_set, val_set = torch.utils.data.random_split(dataset, [len(dataset) - val_split, val_split])\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_set, batch_size=batch_size)\n\n    # Initialize the autoencoder model\n    input_dim = data.shape[1]\n    model = Autoencoder(input_dim, hidden_dim)\n\n    # Set the loss function and optimizer\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=3e-2, weight_decay=1e-8)\n\n    # Train the autoencoder\n    for epoch in range(num_epochs):\n        train_loss = 0.0\n        val_loss = 0.0\n\n        # Training\n        model.train()\n        for batch in train_loader:\n            # Zero the gradients\n            optimizer.zero_grad()\n            # Forward pass\n            outputs = model(batch)\n            # Compute the reconstruction loss\n            loss = criterion(outputs, batch)\n            # Backward pass and optimization\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * batch.size(0)\n\n        # Validation\n        model.eval()\n        with torch.no_grad():\n            for batch in val_loader:\n                outputs = model(batch)\n                loss = criterion(outputs, batch)\n                val_loss += loss.item() * batch.size(0)\n\n        train_loss /= len(train_set)\n        val_loss /= len(val_set)\n\n\n        print(f'Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n        \n    # Calculate reconstruction error for each data point\n    reconstructed = model(dataset.data)\n    mse_loss = nn.MSELoss(reduction='none')\n    error = torch.mean(mse_loss(reconstructed, dataset.data), dim=1)\n\n    \n    # Define a threshold for outlier detection\n    threshold = np.percentile(error.detach().numpy(), 99)\n\n    # Create a boolean mask for outliers\n    outliers_mask = (error &gt; threshold)\n\n    # Mark the outliers\n    dataframe[f'outliers_autoenc'] = (outliers_mask)\n    dataframe[f'outliers_autoenc'] = dataframe[f'outliers_autoenc'].astype(int)\n    return dataframe\n\n# Detect outliers using the autoencoder method\ndf = detect_outliers_autoencoder(df)\n\nprint(\"Outliers:\")\n    \n# Plotting the data\ndf0 = df[df['outliers_autoenc']&gt;0].copy()\nprint(df0.shape)\nplot_results(df, df0, method_str=\"One-class Autoencoders\")\n\ndf.describe()\n\nEpoch 1/10: Train Loss: 0.5693, Val Loss: 0.5681\nEpoch 2/10: Train Loss: 0.5514, Val Loss: 0.5679\nEpoch 3/10: Train Loss: 0.5512, Val Loss: 0.5681\nEpoch 4/10: Train Loss: 0.5511, Val Loss: 0.5675\nEpoch 5/10: Train Loss: 0.5514, Val Loss: 0.5678\nEpoch 6/10: Train Loss: 0.5513, Val Loss: 0.5680\nEpoch 7/10: Train Loss: 0.5511, Val Loss: 0.5679\nEpoch 8/10: Train Loss: 0.5516, Val Loss: 0.5682\nEpoch 9/10: Train Loss: 0.5511, Val Loss: 0.5675\nEpoch 10/10: Train Loss: 0.5511, Val Loss: 0.5676\nOutliers:\n(100, 9)\n\n\n\n\n\n\n\n\n\n\n\n\nHeight\nWeight\noutliers_iqr_Weight\noutliers_iqr_Height\noutliers_iforest\noutliers_lof\noutliers_svm\noutliers_autoenc\n\n\n\n\ncount\n10000.000000\n10000.000000\n10000.0000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n\n\nmean\n66.367560\n161.440357\n0.0001\n0.000800\n0.010000\n0.013200\n0.010100\n0.010000\n\n\nstd\n3.847528\n32.108439\n0.0100\n0.028274\n0.099504\n0.114136\n0.099995\n0.099504\n\n\nmin\n54.263133\n64.700127\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n63.505620\n135.818051\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n66.318070\n161.212928\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n75%\n69.174262\n187.169525\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nmax\n78.998742\n269.989699\n1.0000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000"
  }
]