<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Rahul Pulluri">
<meta name="dcterms.date" content="2023-10-27">

<title>Machine Learning - Clustering</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Machine Learning</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">Rahul Pulluri</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/tpriya14/MLBlog" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/tspriya14" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Clustering</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Rahul Pulluri </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 27, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="what-is-clustering" class="level2">
<h2 class="anchored" data-anchor-id="what-is-clustering">What is Clustering?</h2>
<p>Clustering in machine learning is a type of unsupervised learning technique that involves grouping similar data points together into clusters. In this context, “unsupervised” means that the learning process is conducted without any predefined labels or categories; the algorithm itself discovers the inherent groupings in the data.</p>
</section>
<section id="types-of-clustering-algorithms" class="level2">
<h2 class="anchored" data-anchor-id="types-of-clustering-algorithms">Types of Clustering Algorithms</h2>
<p>Clustering algorithms come in various forms, each suited to handle different kinds of data and objectives:</p>
<p><strong>Density-Based Clustering:</strong> These algorithms identify clusters as areas of high density separated by areas of low density. They excel in finding clusters of arbitrary shapes and typically do not assign outliers to any cluster.</p>
<p><strong>Distribution-Based Clustering:</strong> In this approach, clusters are formed based on the likelihood of data points belonging to a certain distribution. A typical model is the Gaussian distribution, where the probability of belonging to a cluster decreases as the distance from the cluster’s center increases.</p>
<p><strong>Centroid-Based Clustering:</strong> This is a widely used approach where clusters are represented by a central vector or a centroid. Each data point is assigned to the cluster with the nearest centroid. K-means is a popular example of centroid-based clustering.</p>
<p><strong>Hierarchical-Based Clustering:</strong> These algorithms create a tree of clusters, which is particularly useful for hierarchical or nested data structures. It can be either divisive (top-down approach) or agglomerative (bottom-up approach).</p>
</section>
<section id="applications" class="level2">
<h2 class="anchored" data-anchor-id="applications">Applications:</h2>
<p><strong>Customer Segmentation:</strong> Grouping customers based on purchasing behavior, interests, demographics, etc.</p>
<p><strong>Anomaly Detection:</strong> Identifying unusual data points which can be useful in fraud detection.</p>
<p><strong>Pattern Recognition:</strong> In areas like bioinformatics, speech recognition, and image analysis.</p>
</section>
<section id="common-clustering-algorithms" class="level2">
<h2 class="anchored" data-anchor-id="common-clustering-algorithms">Common Clustering algorithms:</h2>
<section id="k-means-clustering-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="k-means-clustering-algorithm">K-Means Clustering algorithm:</h3>
<p>K-means clustering, the most commonly used clustering algorithm, partitions a dataset into K distinct clusters. It does this by assigning each data point to the nearest of K centroids, which are iteratively recalculated as the mean of the points assigned to them. This process repeats until the centroids stabilize, effectively minimizing the variance within each cluster. Widely appreciated for its simplicity and efficiency, this method assumes spherical clusters and can be sensitive to the initial placement of centroids and the presence of outliers.</p>
<p><strong>Implementation</strong></p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> unique, where</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans, DBSCAN</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the data set we'll work with</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>training_data, _ <span class="op">=</span> make_classification(</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    n_features<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    n_informative<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    n_redundant<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>    n_clusters_per_class<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">4</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the KMeans model</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>kmeans_model <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Assign each data point to a cluster using KMeans</span></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>kmeans_result <span class="op">=</span> kmeans_model.fit_predict(training_data)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the DBSCAN model</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>dbscan_model <span class="op">=</span> DBSCAN(eps<span class="op">=</span><span class="fl">0.3</span>, min_samples<span class="op">=</span><span class="dv">9</span>)  <span class="co"># Adjust 'eps' and 'min_samples' as needed</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Assign each data point to a cluster using DBSCAN</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>dbscan_result <span class="op">=</span> dbscan_model.fit_predict(training_data)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Get all of the unique clusters from DBSCAN results</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>dbscan_clusters <span class="op">=</span> unique(dbscan_result)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the DBSCAN clusters</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> dbscan_cluster <span class="kw">in</span> dbscan_clusters:</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get data points that fall in this cluster</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>    index <span class="op">=</span> where(dbscan_result <span class="op">==</span> dbscan_cluster)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Make the plot</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    pyplot.scatter(training_data[index, <span class="dv">0</span>], training_data[index, <span class="dv">1</span>])</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the DBSCAN plot</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>pyplot.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>C:\Users\rahul\AppData\Local\Programs\Python\Python310\lib\site-packages\sklearn\cluster\_kmeans.py:1416: FutureWarning:

The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-2-output-2.png" width="569" height="411"></p>
</div>
</div>
</section>
<section id="dbscan-clustering-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="dbscan-clustering-algorithm">DBSCAN clustering algorithm:</h3>
<p>DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a clustering algorithm that identifies clusters based on the density of data points, without requiring the number of clusters to be predefined. It categorizes points as core points, border points, or noise, depending on the number of nearby points (based on a set distance eps) and a minimum number of points (minPts) required to form a dense region. DBSCAN excels in discovering clusters of arbitrary shapes and sizes, and effectively distinguishes outliers, making it robust in handling noisy datasets. This makes it particularly useful in applications like anomaly detection, geospatial analysis, and image processing.</p>
<p><strong>Implementation</strong></p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> unique</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> where</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> DBSCAN</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize the data set we'll work with</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>training_data, _ <span class="op">=</span> make_classification(</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>    n_features<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    n_informative<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>    n_redundant<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    n_clusters_per_class<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">4</span></span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co"># define the model</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>dbscan_model <span class="op">=</span> DBSCAN(eps<span class="op">=</span><span class="fl">0.25</span>, min_samples<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="co"># train the model</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>dbscan_model.fit(training_data)</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a><span class="co"># assign each data point to a cluster</span></span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>dbscan_result <span class="op">=</span> dbscan_model.labels_</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="co"># get all of the unique clusters</span></span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>dbscan_clusters <span class="op">=</span> unique(dbscan_result)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the DBSCAN clusters</span></span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> cluster <span class="kw">in</span> dbscan_clusters:</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get data points that fall in this cluster</span></span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>    index <span class="op">=</span> where(dbscan_result <span class="op">==</span> cluster)</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># make the plot</span></span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>    pyplot.scatter(training_data[index, <span class="dv">0</span>], training_data[index, <span class="dv">1</span>])</span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a><span class="co"># show the DBSCAN plot</span></span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>pyplot.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-3-output-1.png" width="569" height="411"></p>
</div>
</div>
</section>
<section id="hierarchical-clustering" class="level3">
<h3 class="anchored" data-anchor-id="hierarchical-clustering">Hierarchical Clustering:</h3>
<p>This creates a tree of clusters. It can be either agglomerative (merging small clusters into larger ones) or divisive (splitting clusters).</p>
<p><strong>Agglomerative Hierarchy clustering algorithm:</strong> This bottom-up approach starts with each data point as an individual cluster and iteratively merges the closest pairs of clusters until all points are merged into a single cluster or a specified number of clusters is reached. The process of choosing which clusters to merge at each step is based on a distance metric and a linkage criterion (like single, complete, or average linkage). Agglomerative clustering is particularly useful for small to medium-sized datasets and is often visualized using a dendrogram, which shows the hierarchical relationship between clusters.</p>
<p><strong>Implementation</strong></p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> AgglomerativeClustering</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the data set we'll work with</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>training_data, _ <span class="op">=</span> make_classification(</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    n_features<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    n_informative<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    n_redundant<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    n_clusters_per_class<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">4</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>agglomerative_model <span class="op">=</span> AgglomerativeClustering(n_clusters<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Assign each data point to a cluster</span></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>agglomerative_result <span class="op">=</span> agglomerative_model.fit_predict(training_data)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Get all unique clusters</span></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>agglomerative_clusters <span class="op">=</span> np.unique(agglomerative_result)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the clusters</span></span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> cluster_label <span class="kw">in</span> agglomerative_clusters:</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get data points that belong to this cluster</span></span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>    cluster_points <span class="op">=</span> training_data[agglomerative_result <span class="op">==</span> cluster_label]</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Make the plot</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>    plt.scatter(cluster_points[:, <span class="dv">0</span>], cluster_points[:, <span class="dv">1</span>], label<span class="op">=</span><span class="ss">f'Cluster </span><span class="sc">{</span>cluster_label<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the Agglomerative Hierarchy plot</span></span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-4-output-1.png" width="569" height="411"></p>
</div>
</div>
<p><strong>Divisive Hierarchy clustering algorithm:</strong> Conversely, Divisive clustering is a top-down approach. It begins with all data points in a single cluster and recursively splits the most heterogeneous cluster until each cluster contains only one data point or until a specified number of clusters is achieved. This method often involves analyzing the data to determine the best way to perform the splits, typically based on a measure of dissimilarity between data points.</p>
<p><strong>Implementation</strong></p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the data set we'll work with</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>training_data, _ <span class="op">=</span> make_classification(</span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>    n_features<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a>    n_informative<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    n_redundant<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    n_clusters_per_class<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">4</span></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model for divisive clustering</span></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> divisive_clustering(data, stopping_condition):</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>    clusters <span class="op">=</span> [data]  <span class="co"># Start with all data points in one cluster</span></span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="bu">len</span>(clusters) <span class="op">&lt;</span> stopping_condition:</span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Select the cluster to split (you can define your splitting criterion here)</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a>        cluster_to_split <span class="op">=</span> select_cluster_to_split(clusters)</span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Split the selected cluster into two subclusters</span></span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a>        subcluster1, subcluster2 <span class="op">=</span> split_cluster(cluster_to_split)</span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Remove the original cluster and add the two subclusters</span></span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a>        clusters.remove(cluster_to_split)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a>        clusters.append(subcluster1)</span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a>        clusters.append(subcluster2)</span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> clusters</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a><span class="co"># Define your custom splitting criterion (e.g., based on variance, dissimilarity, etc.)</span></span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> select_cluster_to_split(clusters):</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For simplicity, we'll select the largest cluster to split.</span></span>
<span id="cb5-36"><a href="#cb5-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">max</span>(clusters, key<span class="op">=</span><span class="bu">len</span>)</span>
<span id="cb5-37"><a href="#cb5-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-38"><a href="#cb5-38" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function to split a cluster (you can implement your splitting logic)</span></span>
<span id="cb5-39"><a href="#cb5-39" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> split_cluster(cluster):</span>
<span id="cb5-40"><a href="#cb5-40" aria-hidden="true" tabindex="-1"></a>    midpoint <span class="op">=</span> <span class="bu">len</span>(cluster) <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb5-41"><a href="#cb5-41" aria-hidden="true" tabindex="-1"></a>    subcluster1 <span class="op">=</span> cluster[:midpoint]</span>
<span id="cb5-42"><a href="#cb5-42" aria-hidden="true" tabindex="-1"></a>    subcluster2 <span class="op">=</span> cluster[midpoint:]</span>
<span id="cb5-43"><a href="#cb5-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> subcluster1, subcluster2</span>
<span id="cb5-44"><a href="#cb5-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-45"><a href="#cb5-45" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform divisive clustering with a specified stopping condition</span></span>
<span id="cb5-46"><a href="#cb5-46" aria-hidden="true" tabindex="-1"></a>clusters <span class="op">=</span> divisive_clustering(training_data.tolist(), stopping_condition<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb5-47"><a href="#cb5-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-48"><a href="#cb5-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the divisive clusters</span></span>
<span id="cb5-49"><a href="#cb5-49" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, cluster <span class="kw">in</span> <span class="bu">enumerate</span>(clusters):</span>
<span id="cb5-50"><a href="#cb5-50" aria-hidden="true" tabindex="-1"></a>    cluster <span class="op">=</span> np.array(cluster)  <span class="co"># Convert back to NumPy array</span></span>
<span id="cb5-51"><a href="#cb5-51" aria-hidden="true" tabindex="-1"></a>    plt.scatter(cluster[:, <span class="dv">0</span>], cluster[:, <span class="dv">1</span>], label<span class="op">=</span><span class="ss">f'Cluster </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb5-52"><a href="#cb5-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-53"><a href="#cb5-53" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb5-54"><a href="#cb5-54" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-5-output-1.png" width="569" height="411"></p>
</div>
</div>
</section>
<section id="gaussian-mixture-model-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="gaussian-mixture-model-algorithm">Gaussian Mixture Model algorithm:</h3>
<p>The Gaussian Mixture Model (GMM) algorithm is a probabilistic approach used for clustering and density estimation, overcoming K-means’ circular data limitation. GMM assumes data is generated from a mixture of Gaussian distributions, making it suitable for various data shapes. It iteratively estimates Gaussian parameters (mean, covariance, mixing coefficients) to fit data, providing soft cluster assignments based on probabilities. GMMs are versatile but sensitive to initializations, requiring a predefined cluster count.</p>
<p><strong>Implementation</strong></p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.mixture <span class="im">import</span> GaussianMixture</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the data set we'll work with</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>training_data, _ <span class="op">=</span> make_classification(</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>    n_features<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>    n_informative<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>    n_redundant<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>    n_clusters_per_class<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">4</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>gaussian_model <span class="op">=</span> GaussianMixture(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>gaussian_model.fit(training_data)</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Assign each data point to a cluster</span></span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>gaussian_result <span class="op">=</span> gaussian_model.predict(training_data)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a><span class="co"># Get all unique clusters</span></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>gaussian_clusters <span class="op">=</span> np.unique(gaussian_result)</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the Gaussian Mixture clusters</span></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> cluster_label <span class="kw">in</span> gaussian_clusters:</span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get data points that belong to this cluster</span></span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>    cluster_points <span class="op">=</span> training_data[gaussian_result <span class="op">==</span> cluster_label]</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Make the plot</span></span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>    plt.scatter(cluster_points[:, <span class="dv">0</span>], cluster_points[:, <span class="dv">1</span>], label<span class="op">=</span><span class="ss">f'Cluster </span><span class="sc">{</span>cluster_label<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the Gaussian Mixture plot</span></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-6-output-1.png" width="569" height="411"></p>
</div>
</div>
</section>
<section id="mean-shift-clustering-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="mean-shift-clustering-algorithm">Mean-Shift clustering algorithm:</h3>
<p>The Mean-Shift clustering algorithm is a density-based method used to find clusters in data without specifying the number of clusters in advance. It estimates the density of data points, iteratively shifts them towards denser regions, and identifies clusters where points converge. Mean-Shift is versatile and suitable for various cluster shapes and sizes, commonly used in applications like image segmentation and object tracking.</p>
<p><strong>Implementation</strong></p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> unique</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> where</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> MeanShift</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize the data set we'll work with</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>training_data, _ <span class="op">=</span> make_classification(</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>    n_features<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>    n_informative<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>    n_redundant<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>    n_clusters_per_class<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">4</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="co"># define the model</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>mean_model <span class="op">=</span> MeanShift()</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="co"># assign each data point to a cluster</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>mean_result <span class="op">=</span> mean_model.fit_predict(training_data)</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a><span class="co"># get all of the unique clusters</span></span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>mean_clusters <span class="op">=</span> unique(mean_result)</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a><span class="co"># plot Mean-Shift the clusters</span></span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> mean_cluster <span class="kw">in</span> mean_clusters:</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get data points that fall in this cluster</span></span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>    index <span class="op">=</span> where(mean_result <span class="op">==</span> mean_cluster)</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># make the plot</span></span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>    pyplot.scatter(training_data[index, <span class="dv">0</span>], training_data[index, <span class="dv">1</span>])</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a><span class="co"># show the Mean-Shift plot</span></span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>pyplot.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-7-output-1.png" width="569" height="411"></p>
</div>
</div>
</section>
<section id="birch-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="birch-algorithm">BIRCH algorithm:</h3>
<p>The BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies) algorithm is a memory-efficient hierarchical clustering technique designed for large datasets, particularly numeric data. It builds a tree-like structure by recursively splitting data points into compact summaries that preserve distribution information, allowing scalable clustering. BIRCH is commonly combined with other clustering techniques and is well-suited for high-dimensional data. However, it requires data transformations for handling categorical values. This approach is frequently employed in exploratory data analysis and data compression, making it a valuable tool for various applications.</p>
<p><strong>Implementation</strong></p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> unique</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> where</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> Birch</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize the data set we'll work with</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>training_data, _ <span class="op">=</span> make_classification(</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    n_features<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    n_informative<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>    n_redundant<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    n_clusters_per_class<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">4</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a><span class="co"># define the model</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>birch_model <span class="op">=</span> Birch(threshold<span class="op">=</span><span class="fl">0.03</span>, n_clusters<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="co"># train the model</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a>birch_model.fit(training_data)</span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a><span class="co"># assign each data point to a cluster</span></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a>birch_result <span class="op">=</span> birch_model.predict(training_data)</span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a><span class="co"># get all of the unique clusters</span></span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>birch_clusters <span class="op">=</span> unique(birch_result)</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the BIRCH clusters</span></span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> birch_cluster <span class="kw">in</span> birch_clusters:</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get data points that fall in this cluster</span></span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a>    index <span class="op">=</span> where(birch_result <span class="op">==</span> birch_cluster)</span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># make the plot</span></span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>    pyplot.scatter(training_data[index, <span class="dv">0</span>], training_data[index, <span class="dv">1</span>])</span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a><span class="co"># show the BIRCH plot</span></span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>pyplot.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-8-output-1.png" width="569" height="411"></p>
</div>
</div>
</section>
<section id="optics-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="optics-algorithm">OPTICS algorithm:</h3>
<p>OPTICS (Ordering Points To Identify the Clustering Structure) is a density-based clustering algorithm that excels in identifying clusters in data with varying density. It orders data points to detect different density clusters efficiently, similar to DBSCAN, and assigns specific cluster membership distances to each point. It doesn’t require specifying the number of clusters in advance, making it suitable for various dataset shapes and sizes</p>
<p><strong>Implementation</strong></p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> unique</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> where</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> OPTICS</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize the data set we'll work with</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a>training_data, _ <span class="op">=</span> make_classification(</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>    n_features<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>    n_informative<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    n_redundant<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>    n_clusters_per_class<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">4</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="co"># define the model</span></span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>optics_model <span class="op">=</span> OPTICS(eps<span class="op">=</span><span class="fl">0.75</span>, min_samples<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="co"># assign each data point to a cluster</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>optics_result <span class="op">=</span> optics_model.fit_predict(training_data)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a><span class="co"># get all of the unique clusters</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>optics_clusters <span class="op">=</span> unique(optics_result)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the OPTICS clusters</span></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> optics_cluster <span class="kw">in</span> optics_clusters:</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get data points that fall in this cluster</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>    index <span class="op">=</span> where(optics_result <span class="op">==</span> optics_cluster)</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># make the plot</span></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>    pyplot.scatter(training_data[index, <span class="dv">0</span>], training_data[index, <span class="dv">1</span>])</span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a><span class="co"># show the OPTICS plot</span></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>pyplot.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-9-output-1.png" width="569" height="411"></p>
</div>
</div>
</section>
<section id="affinity-propagation-clustering-algorithm" class="level3">
<h3 class="anchored" data-anchor-id="affinity-propagation-clustering-algorithm">Affinity Propagation clustering algorithm:</h3>
<p>Affinity Propagation (AP) is a unique clustering algorithm that doesn’t require specifying the number of clusters beforehand. It uses data point communication to discover clusters, and exemplars are found as a consensus among data points. AP is ideal for scenarios where the number of clusters is uncertain, such as computer vision problems. This clustering algorithm models data points as nodes in a network and finds exemplar data points that represent clusters through message exchange. While effective for high-dimensional data and various cluster shapes and sizes, AP can be sensitive to the initial selection of exemplars and may not perform well on large datasets.</p>
<p><strong>Implementation</strong></p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> unique</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> where</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> AffinityPropagation</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize the data set we'll work with</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>training_data, _ <span class="op">=</span> make_classification(</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>    n_features<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    n_informative<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a>    n_redundant<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a>    n_clusters_per_class<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">4</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a><span class="co"># define the model</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AffinityPropagation(damping<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a><span class="co"># train the model</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>model.fit(training_data)</span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a><span class="co"># assign each data point to a cluster</span></span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.predict(training_data)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a><span class="co"># get all of the unique clusters</span></span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>clusters <span class="op">=</span> unique(result)</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the clusters</span></span>
<span id="cb10-30"><a href="#cb10-30" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> cluster <span class="kw">in</span> clusters:</span>
<span id="cb10-31"><a href="#cb10-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get data points that fall in this cluster</span></span>
<span id="cb10-32"><a href="#cb10-32" aria-hidden="true" tabindex="-1"></a>    index <span class="op">=</span> where(result <span class="op">==</span> cluster)</span>
<span id="cb10-33"><a href="#cb10-33" aria-hidden="true" tabindex="-1"></a>    <span class="co"># make the plot</span></span>
<span id="cb10-34"><a href="#cb10-34" aria-hidden="true" tabindex="-1"></a>    pyplot.scatter(training_data[index, <span class="dv">0</span>], training_data[index, <span class="dv">1</span>])</span>
<span id="cb10-35"><a href="#cb10-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-36"><a href="#cb10-36" aria-hidden="true" tabindex="-1"></a><span class="co"># show the plot</span></span>
<span id="cb10-37"><a href="#cb10-37" aria-hidden="true" tabindex="-1"></a>pyplot.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-10-output-1.png" width="569" height="411"></p>
</div>
</div>


<!-- -->

</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb11" data-shortcodes="false"><pre class="sourceCode markdown code-with-copy"><code class="sourceCode markdown"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a><span class="an">title:</span><span class="co"> "Clustering"</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="an">author:</span><span class="co"> "Rahul Pulluri"</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="an">date:</span><span class="co"> "2023-10-27"</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="an">image:</span><span class="co"> "cluster.jpg"</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="an">jupyter:</span><span class="co"> python3</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co">---</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="fu">## What is Clustering?</span></span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>Clustering in machine learning is a type of unsupervised learning technique that involves grouping similar data points together into clusters. In this context, "unsupervised" means that the learning process is conducted without any predefined labels or categories; the algorithm itself discovers the inherent groupings in the data.</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-13"><a href="#cb11-13" aria-hidden="true" tabindex="-1"></a><span class="fu">## Types of Clustering Algorithms</span></span>
<span id="cb11-14"><a href="#cb11-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-15"><a href="#cb11-15" aria-hidden="true" tabindex="-1"></a>Clustering algorithms come in various forms, each suited to handle different kinds of data and objectives:</span>
<span id="cb11-16"><a href="#cb11-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-17"><a href="#cb11-17" aria-hidden="true" tabindex="-1"></a>**Density-Based Clustering:** These algorithms identify clusters as areas of high density separated by areas of low density. They excel in finding clusters of arbitrary shapes and typically do not assign outliers to any cluster.</span>
<span id="cb11-18"><a href="#cb11-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-19"><a href="#cb11-19" aria-hidden="true" tabindex="-1"></a>**Distribution-Based Clustering:** In this approach, clusters are formed based on the likelihood of data points belonging to a certain distribution. A typical model is the Gaussian distribution, where the probability of belonging to a cluster decreases as the distance from the cluster's center increases.</span>
<span id="cb11-20"><a href="#cb11-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-21"><a href="#cb11-21" aria-hidden="true" tabindex="-1"></a>**Centroid-Based Clustering:** This is a widely used approach where clusters are represented by a central vector or a centroid. Each data point is assigned to the cluster with the nearest centroid. K-means is a popular example of centroid-based clustering.</span>
<span id="cb11-22"><a href="#cb11-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-23"><a href="#cb11-23" aria-hidden="true" tabindex="-1"></a>**Hierarchical-Based Clustering:** These algorithms create a tree of clusters, which is particularly useful for hierarchical or nested data structures. It can be either divisive (top-down approach) or agglomerative (bottom-up approach).</span>
<span id="cb11-24"><a href="#cb11-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-25"><a href="#cb11-25" aria-hidden="true" tabindex="-1"></a><span class="fu">## Applications:</span></span>
<span id="cb11-26"><a href="#cb11-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-27"><a href="#cb11-27" aria-hidden="true" tabindex="-1"></a>**Customer Segmentation:** Grouping customers based on purchasing behavior, interests, demographics, etc.</span>
<span id="cb11-28"><a href="#cb11-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-29"><a href="#cb11-29" aria-hidden="true" tabindex="-1"></a>**Anomaly Detection:** Identifying unusual data points which can be useful in fraud detection.</span>
<span id="cb11-30"><a href="#cb11-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-31"><a href="#cb11-31" aria-hidden="true" tabindex="-1"></a>**Pattern Recognition:** In areas like bioinformatics, speech recognition, and image analysis.</span>
<span id="cb11-32"><a href="#cb11-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-33"><a href="#cb11-33" aria-hidden="true" tabindex="-1"></a><span class="fu">## Common Clustering algorithms:</span></span>
<span id="cb11-34"><a href="#cb11-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-35"><a href="#cb11-35" aria-hidden="true" tabindex="-1"></a><span class="fu">### K-Means Clustering algorithm:</span></span>
<span id="cb11-36"><a href="#cb11-36" aria-hidden="true" tabindex="-1"></a> K-means clustering, the most commonly used clustering algorithm, partitions a dataset into K distinct clusters. It does this by assigning each data point to the nearest of K centroids, which are iteratively recalculated as the mean of the points assigned to them. This process repeats until the centroids stabilize, effectively minimizing the variance within each cluster. Widely appreciated for its simplicity and efficiency, this method assumes spherical clusters and can be sensitive to the initial placement of centroids and the presence of outliers.</span>
<span id="cb11-37"><a href="#cb11-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-38"><a href="#cb11-38" aria-hidden="true" tabindex="-1"></a>**Implementation**</span>
<span id="cb11-39"><a href="#cb11-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-42"><a href="#cb11-42" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb11-43"><a href="#cb11-43" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> unique, where</span>
<span id="cb11-44"><a href="#cb11-44" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot</span>
<span id="cb11-45"><a href="#cb11-45" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb11-46"><a href="#cb11-46" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans, DBSCAN</span>
<span id="cb11-47"><a href="#cb11-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-48"><a href="#cb11-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the data set we'll work with</span></span>
<span id="cb11-49"><a href="#cb11-49" aria-hidden="true" tabindex="-1"></a>training_data, _ <span class="op">=</span> make_classification(</span>
<span id="cb11-50"><a href="#cb11-50" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb11-51"><a href="#cb11-51" aria-hidden="true" tabindex="-1"></a>    n_features<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb11-52"><a href="#cb11-52" aria-hidden="true" tabindex="-1"></a>    n_informative<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb11-53"><a href="#cb11-53" aria-hidden="true" tabindex="-1"></a>    n_redundant<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb11-54"><a href="#cb11-54" aria-hidden="true" tabindex="-1"></a>    n_clusters_per_class<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb11-55"><a href="#cb11-55" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">4</span></span>
<span id="cb11-56"><a href="#cb11-56" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-57"><a href="#cb11-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-58"><a href="#cb11-58" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the KMeans model</span></span>
<span id="cb11-59"><a href="#cb11-59" aria-hidden="true" tabindex="-1"></a>kmeans_model <span class="op">=</span> KMeans(n_clusters<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb11-60"><a href="#cb11-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-61"><a href="#cb11-61" aria-hidden="true" tabindex="-1"></a><span class="co"># Assign each data point to a cluster using KMeans</span></span>
<span id="cb11-62"><a href="#cb11-62" aria-hidden="true" tabindex="-1"></a>kmeans_result <span class="op">=</span> kmeans_model.fit_predict(training_data)</span>
<span id="cb11-63"><a href="#cb11-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-64"><a href="#cb11-64" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the DBSCAN model</span></span>
<span id="cb11-65"><a href="#cb11-65" aria-hidden="true" tabindex="-1"></a>dbscan_model <span class="op">=</span> DBSCAN(eps<span class="op">=</span><span class="fl">0.3</span>, min_samples<span class="op">=</span><span class="dv">9</span>)  <span class="co"># Adjust 'eps' and 'min_samples' as needed</span></span>
<span id="cb11-66"><a href="#cb11-66" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-67"><a href="#cb11-67" aria-hidden="true" tabindex="-1"></a><span class="co"># Assign each data point to a cluster using DBSCAN</span></span>
<span id="cb11-68"><a href="#cb11-68" aria-hidden="true" tabindex="-1"></a>dbscan_result <span class="op">=</span> dbscan_model.fit_predict(training_data)</span>
<span id="cb11-69"><a href="#cb11-69" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-70"><a href="#cb11-70" aria-hidden="true" tabindex="-1"></a><span class="co"># Get all of the unique clusters from DBSCAN results</span></span>
<span id="cb11-71"><a href="#cb11-71" aria-hidden="true" tabindex="-1"></a>dbscan_clusters <span class="op">=</span> unique(dbscan_result)</span>
<span id="cb11-72"><a href="#cb11-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-73"><a href="#cb11-73" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the DBSCAN clusters</span></span>
<span id="cb11-74"><a href="#cb11-74" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> dbscan_cluster <span class="kw">in</span> dbscan_clusters:</span>
<span id="cb11-75"><a href="#cb11-75" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get data points that fall in this cluster</span></span>
<span id="cb11-76"><a href="#cb11-76" aria-hidden="true" tabindex="-1"></a>    index <span class="op">=</span> where(dbscan_result <span class="op">==</span> dbscan_cluster)</span>
<span id="cb11-77"><a href="#cb11-77" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Make the plot</span></span>
<span id="cb11-78"><a href="#cb11-78" aria-hidden="true" tabindex="-1"></a>    pyplot.scatter(training_data[index, <span class="dv">0</span>], training_data[index, <span class="dv">1</span>])</span>
<span id="cb11-79"><a href="#cb11-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-80"><a href="#cb11-80" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the DBSCAN plot</span></span>
<span id="cb11-81"><a href="#cb11-81" aria-hidden="true" tabindex="-1"></a>pyplot.show()</span>
<span id="cb11-82"><a href="#cb11-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-83"><a href="#cb11-83" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb11-84"><a href="#cb11-84" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-85"><a href="#cb11-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-86"><a href="#cb11-86" aria-hidden="true" tabindex="-1"></a><span class="fu">### DBSCAN clustering algorithm:</span></span>
<span id="cb11-87"><a href="#cb11-87" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb11-88"><a href="#cb11-88" aria-hidden="true" tabindex="-1"></a>DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a clustering algorithm that identifies clusters based on the density of data points, without requiring the number of clusters to be predefined. It categorizes points as core points, border points, or noise, depending on the number of nearby points (based on a set distance eps) and a minimum number of points (minPts) required to form a dense region. DBSCAN excels in discovering clusters of arbitrary shapes and sizes, and effectively distinguishes outliers, making it robust in handling noisy datasets. This makes it particularly useful in applications like anomaly detection, geospatial analysis, and image processing.</span>
<span id="cb11-89"><a href="#cb11-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-90"><a href="#cb11-90" aria-hidden="true" tabindex="-1"></a>**Implementation**</span>
<span id="cb11-91"><a href="#cb11-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-94"><a href="#cb11-94" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb11-95"><a href="#cb11-95" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> unique</span>
<span id="cb11-96"><a href="#cb11-96" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> where</span>
<span id="cb11-97"><a href="#cb11-97" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot</span>
<span id="cb11-98"><a href="#cb11-98" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb11-99"><a href="#cb11-99" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> DBSCAN</span>
<span id="cb11-100"><a href="#cb11-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-101"><a href="#cb11-101" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize the data set we'll work with</span></span>
<span id="cb11-102"><a href="#cb11-102" aria-hidden="true" tabindex="-1"></a>training_data, _ <span class="op">=</span> make_classification(</span>
<span id="cb11-103"><a href="#cb11-103" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb11-104"><a href="#cb11-104" aria-hidden="true" tabindex="-1"></a>    n_features<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb11-105"><a href="#cb11-105" aria-hidden="true" tabindex="-1"></a>    n_informative<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb11-106"><a href="#cb11-106" aria-hidden="true" tabindex="-1"></a>    n_redundant<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb11-107"><a href="#cb11-107" aria-hidden="true" tabindex="-1"></a>    n_clusters_per_class<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb11-108"><a href="#cb11-108" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">4</span></span>
<span id="cb11-109"><a href="#cb11-109" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-110"><a href="#cb11-110" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-111"><a href="#cb11-111" aria-hidden="true" tabindex="-1"></a><span class="co"># define the model</span></span>
<span id="cb11-112"><a href="#cb11-112" aria-hidden="true" tabindex="-1"></a>dbscan_model <span class="op">=</span> DBSCAN(eps<span class="op">=</span><span class="fl">0.25</span>, min_samples<span class="op">=</span><span class="dv">9</span>)</span>
<span id="cb11-113"><a href="#cb11-113" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-114"><a href="#cb11-114" aria-hidden="true" tabindex="-1"></a><span class="co"># train the model</span></span>
<span id="cb11-115"><a href="#cb11-115" aria-hidden="true" tabindex="-1"></a>dbscan_model.fit(training_data)</span>
<span id="cb11-116"><a href="#cb11-116" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-117"><a href="#cb11-117" aria-hidden="true" tabindex="-1"></a><span class="co"># assign each data point to a cluster</span></span>
<span id="cb11-118"><a href="#cb11-118" aria-hidden="true" tabindex="-1"></a>dbscan_result <span class="op">=</span> dbscan_model.labels_</span>
<span id="cb11-119"><a href="#cb11-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-120"><a href="#cb11-120" aria-hidden="true" tabindex="-1"></a><span class="co"># get all of the unique clusters</span></span>
<span id="cb11-121"><a href="#cb11-121" aria-hidden="true" tabindex="-1"></a>dbscan_clusters <span class="op">=</span> unique(dbscan_result)</span>
<span id="cb11-122"><a href="#cb11-122" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-123"><a href="#cb11-123" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the DBSCAN clusters</span></span>
<span id="cb11-124"><a href="#cb11-124" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> cluster <span class="kw">in</span> dbscan_clusters:</span>
<span id="cb11-125"><a href="#cb11-125" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get data points that fall in this cluster</span></span>
<span id="cb11-126"><a href="#cb11-126" aria-hidden="true" tabindex="-1"></a>    index <span class="op">=</span> where(dbscan_result <span class="op">==</span> cluster)</span>
<span id="cb11-127"><a href="#cb11-127" aria-hidden="true" tabindex="-1"></a>    <span class="co"># make the plot</span></span>
<span id="cb11-128"><a href="#cb11-128" aria-hidden="true" tabindex="-1"></a>    pyplot.scatter(training_data[index, <span class="dv">0</span>], training_data[index, <span class="dv">1</span>])</span>
<span id="cb11-129"><a href="#cb11-129" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-130"><a href="#cb11-130" aria-hidden="true" tabindex="-1"></a><span class="co"># show the DBSCAN plot</span></span>
<span id="cb11-131"><a href="#cb11-131" aria-hidden="true" tabindex="-1"></a>pyplot.show()</span>
<span id="cb11-132"><a href="#cb11-132" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-133"><a href="#cb11-133" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-134"><a href="#cb11-134" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb11-135"><a href="#cb11-135" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-136"><a href="#cb11-136" aria-hidden="true" tabindex="-1"></a><span class="fu">### Hierarchical Clustering:</span></span>
<span id="cb11-137"><a href="#cb11-137" aria-hidden="true" tabindex="-1"></a> This creates a tree of clusters. It can be either agglomerative (merging small clusters into larger ones) or divisive (splitting clusters).</span>
<span id="cb11-138"><a href="#cb11-138" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-139"><a href="#cb11-139" aria-hidden="true" tabindex="-1"></a>**Agglomerative Hierarchy clustering algorithm:** </span>
<span id="cb11-140"><a href="#cb11-140" aria-hidden="true" tabindex="-1"></a>This bottom-up approach starts with each data point as an individual cluster and iteratively merges the closest pairs of clusters until all points are merged into a single cluster or a specified number of clusters is reached. The process of choosing which clusters to merge at each step is based on a distance metric and a linkage criterion (like single, complete, or average linkage). Agglomerative clustering is particularly useful for small to medium-sized datasets and is often visualized using a dendrogram, which shows the hierarchical relationship between clusters.</span>
<span id="cb11-141"><a href="#cb11-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-142"><a href="#cb11-142" aria-hidden="true" tabindex="-1"></a>**Implementation**</span>
<span id="cb11-143"><a href="#cb11-143" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-146"><a href="#cb11-146" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb11-147"><a href="#cb11-147" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-148"><a href="#cb11-148" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb11-149"><a href="#cb11-149" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb11-150"><a href="#cb11-150" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> AgglomerativeClustering</span>
<span id="cb11-151"><a href="#cb11-151" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-152"><a href="#cb11-152" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the data set we'll work with</span></span>
<span id="cb11-153"><a href="#cb11-153" aria-hidden="true" tabindex="-1"></a>training_data, _ <span class="op">=</span> make_classification(</span>
<span id="cb11-154"><a href="#cb11-154" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb11-155"><a href="#cb11-155" aria-hidden="true" tabindex="-1"></a>    n_features<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb11-156"><a href="#cb11-156" aria-hidden="true" tabindex="-1"></a>    n_informative<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb11-157"><a href="#cb11-157" aria-hidden="true" tabindex="-1"></a>    n_redundant<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb11-158"><a href="#cb11-158" aria-hidden="true" tabindex="-1"></a>    n_clusters_per_class<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb11-159"><a href="#cb11-159" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">4</span></span>
<span id="cb11-160"><a href="#cb11-160" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-161"><a href="#cb11-161" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-162"><a href="#cb11-162" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model</span></span>
<span id="cb11-163"><a href="#cb11-163" aria-hidden="true" tabindex="-1"></a>agglomerative_model <span class="op">=</span> AgglomerativeClustering(n_clusters<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb11-164"><a href="#cb11-164" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-165"><a href="#cb11-165" aria-hidden="true" tabindex="-1"></a><span class="co"># Assign each data point to a cluster</span></span>
<span id="cb11-166"><a href="#cb11-166" aria-hidden="true" tabindex="-1"></a>agglomerative_result <span class="op">=</span> agglomerative_model.fit_predict(training_data)</span>
<span id="cb11-167"><a href="#cb11-167" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-168"><a href="#cb11-168" aria-hidden="true" tabindex="-1"></a><span class="co"># Get all unique clusters</span></span>
<span id="cb11-169"><a href="#cb11-169" aria-hidden="true" tabindex="-1"></a>agglomerative_clusters <span class="op">=</span> np.unique(agglomerative_result)</span>
<span id="cb11-170"><a href="#cb11-170" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-171"><a href="#cb11-171" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the clusters</span></span>
<span id="cb11-172"><a href="#cb11-172" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> cluster_label <span class="kw">in</span> agglomerative_clusters:</span>
<span id="cb11-173"><a href="#cb11-173" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get data points that belong to this cluster</span></span>
<span id="cb11-174"><a href="#cb11-174" aria-hidden="true" tabindex="-1"></a>    cluster_points <span class="op">=</span> training_data[agglomerative_result <span class="op">==</span> cluster_label]</span>
<span id="cb11-175"><a href="#cb11-175" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Make the plot</span></span>
<span id="cb11-176"><a href="#cb11-176" aria-hidden="true" tabindex="-1"></a>    plt.scatter(cluster_points[:, <span class="dv">0</span>], cluster_points[:, <span class="dv">1</span>], label<span class="op">=</span><span class="ss">f'Cluster </span><span class="sc">{</span>cluster_label<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb11-177"><a href="#cb11-177" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-178"><a href="#cb11-178" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the Agglomerative Hierarchy plot</span></span>
<span id="cb11-179"><a href="#cb11-179" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb11-180"><a href="#cb11-180" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb11-181"><a href="#cb11-181" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-182"><a href="#cb11-182" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb11-183"><a href="#cb11-183" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-184"><a href="#cb11-184" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-185"><a href="#cb11-185" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-186"><a href="#cb11-186" aria-hidden="true" tabindex="-1"></a>**Divisive Hierarchy clustering algorithm:**</span>
<span id="cb11-187"><a href="#cb11-187" aria-hidden="true" tabindex="-1"></a> Conversely, Divisive clustering is a top-down approach. It begins with all data points in a single cluster and recursively splits the most heterogeneous cluster until each cluster contains only one data point or until a specified number of clusters is achieved. This method often involves analyzing the data to determine the best way to perform the splits, typically based on a measure of dissimilarity between data points.</span>
<span id="cb11-188"><a href="#cb11-188" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-189"><a href="#cb11-189" aria-hidden="true" tabindex="-1"></a>**Implementation**</span>
<span id="cb11-190"><a href="#cb11-190" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-193"><a href="#cb11-193" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb11-194"><a href="#cb11-194" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-195"><a href="#cb11-195" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb11-196"><a href="#cb11-196" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb11-197"><a href="#cb11-197" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-198"><a href="#cb11-198" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the data set we'll work with</span></span>
<span id="cb11-199"><a href="#cb11-199" aria-hidden="true" tabindex="-1"></a>training_data, _ <span class="op">=</span> make_classification(</span>
<span id="cb11-200"><a href="#cb11-200" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb11-201"><a href="#cb11-201" aria-hidden="true" tabindex="-1"></a>    n_features<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb11-202"><a href="#cb11-202" aria-hidden="true" tabindex="-1"></a>    n_informative<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb11-203"><a href="#cb11-203" aria-hidden="true" tabindex="-1"></a>    n_redundant<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb11-204"><a href="#cb11-204" aria-hidden="true" tabindex="-1"></a>    n_clusters_per_class<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb11-205"><a href="#cb11-205" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">4</span></span>
<span id="cb11-206"><a href="#cb11-206" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-207"><a href="#cb11-207" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-208"><a href="#cb11-208" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model for divisive clustering</span></span>
<span id="cb11-209"><a href="#cb11-209" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> divisive_clustering(data, stopping_condition):</span>
<span id="cb11-210"><a href="#cb11-210" aria-hidden="true" tabindex="-1"></a>    clusters <span class="op">=</span> [data]  <span class="co"># Start with all data points in one cluster</span></span>
<span id="cb11-211"><a href="#cb11-211" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-212"><a href="#cb11-212" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="bu">len</span>(clusters) <span class="op">&lt;</span> stopping_condition:</span>
<span id="cb11-213"><a href="#cb11-213" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Select the cluster to split (you can define your splitting criterion here)</span></span>
<span id="cb11-214"><a href="#cb11-214" aria-hidden="true" tabindex="-1"></a>        cluster_to_split <span class="op">=</span> select_cluster_to_split(clusters)</span>
<span id="cb11-215"><a href="#cb11-215" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-216"><a href="#cb11-216" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Split the selected cluster into two subclusters</span></span>
<span id="cb11-217"><a href="#cb11-217" aria-hidden="true" tabindex="-1"></a>        subcluster1, subcluster2 <span class="op">=</span> split_cluster(cluster_to_split)</span>
<span id="cb11-218"><a href="#cb11-218" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-219"><a href="#cb11-219" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Remove the original cluster and add the two subclusters</span></span>
<span id="cb11-220"><a href="#cb11-220" aria-hidden="true" tabindex="-1"></a>        clusters.remove(cluster_to_split)</span>
<span id="cb11-221"><a href="#cb11-221" aria-hidden="true" tabindex="-1"></a>        clusters.append(subcluster1)</span>
<span id="cb11-222"><a href="#cb11-222" aria-hidden="true" tabindex="-1"></a>        clusters.append(subcluster2)</span>
<span id="cb11-223"><a href="#cb11-223" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-224"><a href="#cb11-224" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> clusters</span>
<span id="cb11-225"><a href="#cb11-225" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-226"><a href="#cb11-226" aria-hidden="true" tabindex="-1"></a><span class="co"># Define your custom splitting criterion (e.g., based on variance, dissimilarity, etc.)</span></span>
<span id="cb11-227"><a href="#cb11-227" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> select_cluster_to_split(clusters):</span>
<span id="cb11-228"><a href="#cb11-228" aria-hidden="true" tabindex="-1"></a>    <span class="co"># For simplicity, we'll select the largest cluster to split.</span></span>
<span id="cb11-229"><a href="#cb11-229" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">max</span>(clusters, key<span class="op">=</span><span class="bu">len</span>)</span>
<span id="cb11-230"><a href="#cb11-230" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-231"><a href="#cb11-231" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a function to split a cluster (you can implement your splitting logic)</span></span>
<span id="cb11-232"><a href="#cb11-232" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> split_cluster(cluster):</span>
<span id="cb11-233"><a href="#cb11-233" aria-hidden="true" tabindex="-1"></a>    midpoint <span class="op">=</span> <span class="bu">len</span>(cluster) <span class="op">//</span> <span class="dv">2</span></span>
<span id="cb11-234"><a href="#cb11-234" aria-hidden="true" tabindex="-1"></a>    subcluster1 <span class="op">=</span> cluster[:midpoint]</span>
<span id="cb11-235"><a href="#cb11-235" aria-hidden="true" tabindex="-1"></a>    subcluster2 <span class="op">=</span> cluster[midpoint:]</span>
<span id="cb11-236"><a href="#cb11-236" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> subcluster1, subcluster2</span>
<span id="cb11-237"><a href="#cb11-237" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-238"><a href="#cb11-238" aria-hidden="true" tabindex="-1"></a><span class="co"># Perform divisive clustering with a specified stopping condition</span></span>
<span id="cb11-239"><a href="#cb11-239" aria-hidden="true" tabindex="-1"></a>clusters <span class="op">=</span> divisive_clustering(training_data.tolist(), stopping_condition<span class="op">=</span><span class="dv">4</span>)</span>
<span id="cb11-240"><a href="#cb11-240" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-241"><a href="#cb11-241" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the divisive clusters</span></span>
<span id="cb11-242"><a href="#cb11-242" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i, cluster <span class="kw">in</span> <span class="bu">enumerate</span>(clusters):</span>
<span id="cb11-243"><a href="#cb11-243" aria-hidden="true" tabindex="-1"></a>    cluster <span class="op">=</span> np.array(cluster)  <span class="co"># Convert back to NumPy array</span></span>
<span id="cb11-244"><a href="#cb11-244" aria-hidden="true" tabindex="-1"></a>    plt.scatter(cluster[:, <span class="dv">0</span>], cluster[:, <span class="dv">1</span>], label<span class="op">=</span><span class="ss">f'Cluster </span><span class="sc">{</span>i<span class="op">+</span><span class="dv">1</span><span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb11-245"><a href="#cb11-245" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-246"><a href="#cb11-246" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb11-247"><a href="#cb11-247" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb11-248"><a href="#cb11-248" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-249"><a href="#cb11-249" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb11-250"><a href="#cb11-250" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-251"><a href="#cb11-251" aria-hidden="true" tabindex="-1"></a><span class="fu">### Gaussian Mixture Model algorithm: </span></span>
<span id="cb11-252"><a href="#cb11-252" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-253"><a href="#cb11-253" aria-hidden="true" tabindex="-1"></a>The Gaussian Mixture Model (GMM) algorithm is a probabilistic approach used for clustering and density estimation, overcoming K-means' circular data limitation. GMM assumes data is generated from a mixture of Gaussian distributions, making it suitable for various data shapes. It iteratively estimates Gaussian parameters (mean, covariance, mixing coefficients) to fit data, providing soft cluster assignments based on probabilities. GMMs are versatile but sensitive to initializations, requiring a predefined cluster count.</span>
<span id="cb11-254"><a href="#cb11-254" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-255"><a href="#cb11-255" aria-hidden="true" tabindex="-1"></a>**Implementation**</span>
<span id="cb11-256"><a href="#cb11-256" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-259"><a href="#cb11-259" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb11-260"><a href="#cb11-260" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb11-261"><a href="#cb11-261" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb11-262"><a href="#cb11-262" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb11-263"><a href="#cb11-263" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.mixture <span class="im">import</span> GaussianMixture</span>
<span id="cb11-264"><a href="#cb11-264" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-265"><a href="#cb11-265" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize the data set we'll work with</span></span>
<span id="cb11-266"><a href="#cb11-266" aria-hidden="true" tabindex="-1"></a>training_data, _ <span class="op">=</span> make_classification(</span>
<span id="cb11-267"><a href="#cb11-267" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb11-268"><a href="#cb11-268" aria-hidden="true" tabindex="-1"></a>    n_features<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb11-269"><a href="#cb11-269" aria-hidden="true" tabindex="-1"></a>    n_informative<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb11-270"><a href="#cb11-270" aria-hidden="true" tabindex="-1"></a>    n_redundant<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb11-271"><a href="#cb11-271" aria-hidden="true" tabindex="-1"></a>    n_clusters_per_class<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb11-272"><a href="#cb11-272" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">4</span></span>
<span id="cb11-273"><a href="#cb11-273" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-274"><a href="#cb11-274" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-275"><a href="#cb11-275" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the model</span></span>
<span id="cb11-276"><a href="#cb11-276" aria-hidden="true" tabindex="-1"></a>gaussian_model <span class="op">=</span> GaussianMixture(n_components<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb11-277"><a href="#cb11-277" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-278"><a href="#cb11-278" aria-hidden="true" tabindex="-1"></a><span class="co"># Train the model</span></span>
<span id="cb11-279"><a href="#cb11-279" aria-hidden="true" tabindex="-1"></a>gaussian_model.fit(training_data)</span>
<span id="cb11-280"><a href="#cb11-280" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-281"><a href="#cb11-281" aria-hidden="true" tabindex="-1"></a><span class="co"># Assign each data point to a cluster</span></span>
<span id="cb11-282"><a href="#cb11-282" aria-hidden="true" tabindex="-1"></a>gaussian_result <span class="op">=</span> gaussian_model.predict(training_data)</span>
<span id="cb11-283"><a href="#cb11-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-284"><a href="#cb11-284" aria-hidden="true" tabindex="-1"></a><span class="co"># Get all unique clusters</span></span>
<span id="cb11-285"><a href="#cb11-285" aria-hidden="true" tabindex="-1"></a>gaussian_clusters <span class="op">=</span> np.unique(gaussian_result)</span>
<span id="cb11-286"><a href="#cb11-286" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-287"><a href="#cb11-287" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot the Gaussian Mixture clusters</span></span>
<span id="cb11-288"><a href="#cb11-288" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> cluster_label <span class="kw">in</span> gaussian_clusters:</span>
<span id="cb11-289"><a href="#cb11-289" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get data points that belong to this cluster</span></span>
<span id="cb11-290"><a href="#cb11-290" aria-hidden="true" tabindex="-1"></a>    cluster_points <span class="op">=</span> training_data[gaussian_result <span class="op">==</span> cluster_label]</span>
<span id="cb11-291"><a href="#cb11-291" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Make the plot</span></span>
<span id="cb11-292"><a href="#cb11-292" aria-hidden="true" tabindex="-1"></a>    plt.scatter(cluster_points[:, <span class="dv">0</span>], cluster_points[:, <span class="dv">1</span>], label<span class="op">=</span><span class="ss">f'Cluster </span><span class="sc">{</span>cluster_label<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb11-293"><a href="#cb11-293" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-294"><a href="#cb11-294" aria-hidden="true" tabindex="-1"></a><span class="co"># Show the Gaussian Mixture plot</span></span>
<span id="cb11-295"><a href="#cb11-295" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb11-296"><a href="#cb11-296" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb11-297"><a href="#cb11-297" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-298"><a href="#cb11-298" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb11-299"><a href="#cb11-299" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-300"><a href="#cb11-300" aria-hidden="true" tabindex="-1"></a><span class="fu">### Mean-Shift clustering algorithm:</span></span>
<span id="cb11-301"><a href="#cb11-301" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-302"><a href="#cb11-302" aria-hidden="true" tabindex="-1"></a>The Mean-Shift clustering algorithm is a density-based method used to find clusters in data without specifying the number of clusters in advance. It estimates the density of data points, iteratively shifts them towards denser regions, and identifies clusters where points converge. Mean-Shift is versatile and suitable for various cluster shapes and sizes, commonly used in applications like image segmentation and object tracking.</span>
<span id="cb11-303"><a href="#cb11-303" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-304"><a href="#cb11-304" aria-hidden="true" tabindex="-1"></a>**Implementation**</span>
<span id="cb11-305"><a href="#cb11-305" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-308"><a href="#cb11-308" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb11-309"><a href="#cb11-309" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> unique</span>
<span id="cb11-310"><a href="#cb11-310" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> where</span>
<span id="cb11-311"><a href="#cb11-311" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot</span>
<span id="cb11-312"><a href="#cb11-312" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb11-313"><a href="#cb11-313" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> MeanShift</span>
<span id="cb11-314"><a href="#cb11-314" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-315"><a href="#cb11-315" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize the data set we'll work with</span></span>
<span id="cb11-316"><a href="#cb11-316" aria-hidden="true" tabindex="-1"></a>training_data, _ <span class="op">=</span> make_classification(</span>
<span id="cb11-317"><a href="#cb11-317" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb11-318"><a href="#cb11-318" aria-hidden="true" tabindex="-1"></a>    n_features<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb11-319"><a href="#cb11-319" aria-hidden="true" tabindex="-1"></a>    n_informative<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb11-320"><a href="#cb11-320" aria-hidden="true" tabindex="-1"></a>    n_redundant<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb11-321"><a href="#cb11-321" aria-hidden="true" tabindex="-1"></a>    n_clusters_per_class<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb11-322"><a href="#cb11-322" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">4</span></span>
<span id="cb11-323"><a href="#cb11-323" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-324"><a href="#cb11-324" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-325"><a href="#cb11-325" aria-hidden="true" tabindex="-1"></a><span class="co"># define the model</span></span>
<span id="cb11-326"><a href="#cb11-326" aria-hidden="true" tabindex="-1"></a>mean_model <span class="op">=</span> MeanShift()</span>
<span id="cb11-327"><a href="#cb11-327" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-328"><a href="#cb11-328" aria-hidden="true" tabindex="-1"></a><span class="co"># assign each data point to a cluster</span></span>
<span id="cb11-329"><a href="#cb11-329" aria-hidden="true" tabindex="-1"></a>mean_result <span class="op">=</span> mean_model.fit_predict(training_data)</span>
<span id="cb11-330"><a href="#cb11-330" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-331"><a href="#cb11-331" aria-hidden="true" tabindex="-1"></a><span class="co"># get all of the unique clusters</span></span>
<span id="cb11-332"><a href="#cb11-332" aria-hidden="true" tabindex="-1"></a>mean_clusters <span class="op">=</span> unique(mean_result)</span>
<span id="cb11-333"><a href="#cb11-333" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-334"><a href="#cb11-334" aria-hidden="true" tabindex="-1"></a><span class="co"># plot Mean-Shift the clusters</span></span>
<span id="cb11-335"><a href="#cb11-335" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> mean_cluster <span class="kw">in</span> mean_clusters:</span>
<span id="cb11-336"><a href="#cb11-336" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get data points that fall in this cluster</span></span>
<span id="cb11-337"><a href="#cb11-337" aria-hidden="true" tabindex="-1"></a>    index <span class="op">=</span> where(mean_result <span class="op">==</span> mean_cluster)</span>
<span id="cb11-338"><a href="#cb11-338" aria-hidden="true" tabindex="-1"></a>    <span class="co"># make the plot</span></span>
<span id="cb11-339"><a href="#cb11-339" aria-hidden="true" tabindex="-1"></a>    pyplot.scatter(training_data[index, <span class="dv">0</span>], training_data[index, <span class="dv">1</span>])</span>
<span id="cb11-340"><a href="#cb11-340" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-341"><a href="#cb11-341" aria-hidden="true" tabindex="-1"></a><span class="co"># show the Mean-Shift plot</span></span>
<span id="cb11-342"><a href="#cb11-342" aria-hidden="true" tabindex="-1"></a>pyplot.show()</span>
<span id="cb11-343"><a href="#cb11-343" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb11-344"><a href="#cb11-344" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-345"><a href="#cb11-345" aria-hidden="true" tabindex="-1"></a><span class="fu">### BIRCH algorithm:</span></span>
<span id="cb11-346"><a href="#cb11-346" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-347"><a href="#cb11-347" aria-hidden="true" tabindex="-1"></a>The BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies) algorithm is a memory-efficient hierarchical clustering technique designed for large datasets, particularly numeric data. It builds a tree-like structure by recursively splitting data points into compact summaries that preserve distribution information, allowing scalable clustering. BIRCH is commonly combined with other clustering techniques and is well-suited for high-dimensional data. However, it requires data transformations for handling categorical values. This approach is frequently employed in exploratory data analysis and data compression, making it a valuable tool for various applications.</span>
<span id="cb11-348"><a href="#cb11-348" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-349"><a href="#cb11-349" aria-hidden="true" tabindex="-1"></a>**Implementation**</span>
<span id="cb11-350"><a href="#cb11-350" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-353"><a href="#cb11-353" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb11-354"><a href="#cb11-354" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> unique</span>
<span id="cb11-355"><a href="#cb11-355" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> where</span>
<span id="cb11-356"><a href="#cb11-356" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot</span>
<span id="cb11-357"><a href="#cb11-357" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb11-358"><a href="#cb11-358" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> Birch</span>
<span id="cb11-359"><a href="#cb11-359" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-360"><a href="#cb11-360" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize the data set we'll work with</span></span>
<span id="cb11-361"><a href="#cb11-361" aria-hidden="true" tabindex="-1"></a>training_data, _ <span class="op">=</span> make_classification(</span>
<span id="cb11-362"><a href="#cb11-362" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb11-363"><a href="#cb11-363" aria-hidden="true" tabindex="-1"></a>    n_features<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb11-364"><a href="#cb11-364" aria-hidden="true" tabindex="-1"></a>    n_informative<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb11-365"><a href="#cb11-365" aria-hidden="true" tabindex="-1"></a>    n_redundant<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb11-366"><a href="#cb11-366" aria-hidden="true" tabindex="-1"></a>    n_clusters_per_class<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb11-367"><a href="#cb11-367" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">4</span></span>
<span id="cb11-368"><a href="#cb11-368" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-369"><a href="#cb11-369" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-370"><a href="#cb11-370" aria-hidden="true" tabindex="-1"></a><span class="co"># define the model</span></span>
<span id="cb11-371"><a href="#cb11-371" aria-hidden="true" tabindex="-1"></a>birch_model <span class="op">=</span> Birch(threshold<span class="op">=</span><span class="fl">0.03</span>, n_clusters<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb11-372"><a href="#cb11-372" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-373"><a href="#cb11-373" aria-hidden="true" tabindex="-1"></a><span class="co"># train the model</span></span>
<span id="cb11-374"><a href="#cb11-374" aria-hidden="true" tabindex="-1"></a>birch_model.fit(training_data)</span>
<span id="cb11-375"><a href="#cb11-375" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-376"><a href="#cb11-376" aria-hidden="true" tabindex="-1"></a><span class="co"># assign each data point to a cluster</span></span>
<span id="cb11-377"><a href="#cb11-377" aria-hidden="true" tabindex="-1"></a>birch_result <span class="op">=</span> birch_model.predict(training_data)</span>
<span id="cb11-378"><a href="#cb11-378" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-379"><a href="#cb11-379" aria-hidden="true" tabindex="-1"></a><span class="co"># get all of the unique clusters</span></span>
<span id="cb11-380"><a href="#cb11-380" aria-hidden="true" tabindex="-1"></a>birch_clusters <span class="op">=</span> unique(birch_result)</span>
<span id="cb11-381"><a href="#cb11-381" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-382"><a href="#cb11-382" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the BIRCH clusters</span></span>
<span id="cb11-383"><a href="#cb11-383" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> birch_cluster <span class="kw">in</span> birch_clusters:</span>
<span id="cb11-384"><a href="#cb11-384" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get data points that fall in this cluster</span></span>
<span id="cb11-385"><a href="#cb11-385" aria-hidden="true" tabindex="-1"></a>    index <span class="op">=</span> where(birch_result <span class="op">==</span> birch_cluster)</span>
<span id="cb11-386"><a href="#cb11-386" aria-hidden="true" tabindex="-1"></a>    <span class="co"># make the plot</span></span>
<span id="cb11-387"><a href="#cb11-387" aria-hidden="true" tabindex="-1"></a>    pyplot.scatter(training_data[index, <span class="dv">0</span>], training_data[index, <span class="dv">1</span>])</span>
<span id="cb11-388"><a href="#cb11-388" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-389"><a href="#cb11-389" aria-hidden="true" tabindex="-1"></a><span class="co"># show the BIRCH plot</span></span>
<span id="cb11-390"><a href="#cb11-390" aria-hidden="true" tabindex="-1"></a>pyplot.show()</span>
<span id="cb11-391"><a href="#cb11-391" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-392"><a href="#cb11-392" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb11-393"><a href="#cb11-393" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-394"><a href="#cb11-394" aria-hidden="true" tabindex="-1"></a><span class="fu">### OPTICS algorithm:</span></span>
<span id="cb11-395"><a href="#cb11-395" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-396"><a href="#cb11-396" aria-hidden="true" tabindex="-1"></a>OPTICS (Ordering Points To Identify the Clustering Structure) is a density-based clustering algorithm that excels in identifying clusters in data with varying density. It orders data points to detect different density clusters efficiently, similar to DBSCAN, and assigns specific cluster membership distances to each point. It doesn't require specifying the number of clusters in advance, making it suitable for various dataset shapes and sizes</span>
<span id="cb11-397"><a href="#cb11-397" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-398"><a href="#cb11-398" aria-hidden="true" tabindex="-1"></a>**Implementation**</span>
<span id="cb11-399"><a href="#cb11-399" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-402"><a href="#cb11-402" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb11-403"><a href="#cb11-403" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> unique</span>
<span id="cb11-404"><a href="#cb11-404" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> where</span>
<span id="cb11-405"><a href="#cb11-405" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot</span>
<span id="cb11-406"><a href="#cb11-406" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb11-407"><a href="#cb11-407" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> OPTICS</span>
<span id="cb11-408"><a href="#cb11-408" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-409"><a href="#cb11-409" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize the data set we'll work with</span></span>
<span id="cb11-410"><a href="#cb11-410" aria-hidden="true" tabindex="-1"></a>training_data, _ <span class="op">=</span> make_classification(</span>
<span id="cb11-411"><a href="#cb11-411" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb11-412"><a href="#cb11-412" aria-hidden="true" tabindex="-1"></a>    n_features<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb11-413"><a href="#cb11-413" aria-hidden="true" tabindex="-1"></a>    n_informative<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb11-414"><a href="#cb11-414" aria-hidden="true" tabindex="-1"></a>    n_redundant<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb11-415"><a href="#cb11-415" aria-hidden="true" tabindex="-1"></a>    n_clusters_per_class<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb11-416"><a href="#cb11-416" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">4</span></span>
<span id="cb11-417"><a href="#cb11-417" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-418"><a href="#cb11-418" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-419"><a href="#cb11-419" aria-hidden="true" tabindex="-1"></a><span class="co"># define the model</span></span>
<span id="cb11-420"><a href="#cb11-420" aria-hidden="true" tabindex="-1"></a>optics_model <span class="op">=</span> OPTICS(eps<span class="op">=</span><span class="fl">0.75</span>, min_samples<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb11-421"><a href="#cb11-421" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-422"><a href="#cb11-422" aria-hidden="true" tabindex="-1"></a><span class="co"># assign each data point to a cluster</span></span>
<span id="cb11-423"><a href="#cb11-423" aria-hidden="true" tabindex="-1"></a>optics_result <span class="op">=</span> optics_model.fit_predict(training_data)</span>
<span id="cb11-424"><a href="#cb11-424" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-425"><a href="#cb11-425" aria-hidden="true" tabindex="-1"></a><span class="co"># get all of the unique clusters</span></span>
<span id="cb11-426"><a href="#cb11-426" aria-hidden="true" tabindex="-1"></a>optics_clusters <span class="op">=</span> unique(optics_result)</span>
<span id="cb11-427"><a href="#cb11-427" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-428"><a href="#cb11-428" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the OPTICS clusters</span></span>
<span id="cb11-429"><a href="#cb11-429" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> optics_cluster <span class="kw">in</span> optics_clusters:</span>
<span id="cb11-430"><a href="#cb11-430" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get data points that fall in this cluster</span></span>
<span id="cb11-431"><a href="#cb11-431" aria-hidden="true" tabindex="-1"></a>    index <span class="op">=</span> where(optics_result <span class="op">==</span> optics_cluster)</span>
<span id="cb11-432"><a href="#cb11-432" aria-hidden="true" tabindex="-1"></a>    <span class="co"># make the plot</span></span>
<span id="cb11-433"><a href="#cb11-433" aria-hidden="true" tabindex="-1"></a>    pyplot.scatter(training_data[index, <span class="dv">0</span>], training_data[index, <span class="dv">1</span>])</span>
<span id="cb11-434"><a href="#cb11-434" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-435"><a href="#cb11-435" aria-hidden="true" tabindex="-1"></a><span class="co"># show the OPTICS plot</span></span>
<span id="cb11-436"><a href="#cb11-436" aria-hidden="true" tabindex="-1"></a>pyplot.show()</span>
<span id="cb11-437"><a href="#cb11-437" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-438"><a href="#cb11-438" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
<span id="cb11-439"><a href="#cb11-439" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-440"><a href="#cb11-440" aria-hidden="true" tabindex="-1"></a><span class="fu">### Affinity Propagation clustering algorithm:</span></span>
<span id="cb11-441"><a href="#cb11-441" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-442"><a href="#cb11-442" aria-hidden="true" tabindex="-1"></a>Affinity Propagation (AP) is a unique clustering algorithm that doesn't require specifying the number of clusters beforehand. It uses data point communication to discover clusters, and exemplars are found as a consensus among data points. AP is ideal for scenarios where the number of clusters is uncertain, such as computer vision problems. This clustering algorithm models data points as nodes in a network and finds exemplar data points that represent clusters through message exchange. While effective for high-dimensional data and various cluster shapes and sizes, AP can be sensitive to the initial selection of exemplars and may not perform well on large datasets.</span>
<span id="cb11-443"><a href="#cb11-443" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-444"><a href="#cb11-444" aria-hidden="true" tabindex="-1"></a>**Implementation**</span>
<span id="cb11-445"><a href="#cb11-445" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-448"><a href="#cb11-448" aria-hidden="true" tabindex="-1"></a><span class="in">```{python}</span></span>
<span id="cb11-449"><a href="#cb11-449" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> unique</span>
<span id="cb11-450"><a href="#cb11-450" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> numpy <span class="im">import</span> where</span>
<span id="cb11-451"><a href="#cb11-451" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> matplotlib <span class="im">import</span> pyplot</span>
<span id="cb11-452"><a href="#cb11-452" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.datasets <span class="im">import</span> make_classification</span>
<span id="cb11-453"><a href="#cb11-453" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> AffinityPropagation</span>
<span id="cb11-454"><a href="#cb11-454" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-455"><a href="#cb11-455" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize the data set we'll work with</span></span>
<span id="cb11-456"><a href="#cb11-456" aria-hidden="true" tabindex="-1"></a>training_data, _ <span class="op">=</span> make_classification(</span>
<span id="cb11-457"><a href="#cb11-457" aria-hidden="true" tabindex="-1"></a>    n_samples<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb11-458"><a href="#cb11-458" aria-hidden="true" tabindex="-1"></a>    n_features<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb11-459"><a href="#cb11-459" aria-hidden="true" tabindex="-1"></a>    n_informative<span class="op">=</span><span class="dv">2</span>,</span>
<span id="cb11-460"><a href="#cb11-460" aria-hidden="true" tabindex="-1"></a>    n_redundant<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb11-461"><a href="#cb11-461" aria-hidden="true" tabindex="-1"></a>    n_clusters_per_class<span class="op">=</span><span class="dv">1</span>,</span>
<span id="cb11-462"><a href="#cb11-462" aria-hidden="true" tabindex="-1"></a>    random_state<span class="op">=</span><span class="dv">4</span></span>
<span id="cb11-463"><a href="#cb11-463" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb11-464"><a href="#cb11-464" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-465"><a href="#cb11-465" aria-hidden="true" tabindex="-1"></a><span class="co"># define the model</span></span>
<span id="cb11-466"><a href="#cb11-466" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> AffinityPropagation(damping<span class="op">=</span><span class="fl">0.7</span>)</span>
<span id="cb11-467"><a href="#cb11-467" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-468"><a href="#cb11-468" aria-hidden="true" tabindex="-1"></a><span class="co"># train the model</span></span>
<span id="cb11-469"><a href="#cb11-469" aria-hidden="true" tabindex="-1"></a>model.fit(training_data)</span>
<span id="cb11-470"><a href="#cb11-470" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-471"><a href="#cb11-471" aria-hidden="true" tabindex="-1"></a><span class="co"># assign each data point to a cluster</span></span>
<span id="cb11-472"><a href="#cb11-472" aria-hidden="true" tabindex="-1"></a>result <span class="op">=</span> model.predict(training_data)</span>
<span id="cb11-473"><a href="#cb11-473" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-474"><a href="#cb11-474" aria-hidden="true" tabindex="-1"></a><span class="co"># get all of the unique clusters</span></span>
<span id="cb11-475"><a href="#cb11-475" aria-hidden="true" tabindex="-1"></a>clusters <span class="op">=</span> unique(result)</span>
<span id="cb11-476"><a href="#cb11-476" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-477"><a href="#cb11-477" aria-hidden="true" tabindex="-1"></a><span class="co"># plot the clusters</span></span>
<span id="cb11-478"><a href="#cb11-478" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> cluster <span class="kw">in</span> clusters:</span>
<span id="cb11-479"><a href="#cb11-479" aria-hidden="true" tabindex="-1"></a>    <span class="co"># get data points that fall in this cluster</span></span>
<span id="cb11-480"><a href="#cb11-480" aria-hidden="true" tabindex="-1"></a>    index <span class="op">=</span> where(result <span class="op">==</span> cluster)</span>
<span id="cb11-481"><a href="#cb11-481" aria-hidden="true" tabindex="-1"></a>    <span class="co"># make the plot</span></span>
<span id="cb11-482"><a href="#cb11-482" aria-hidden="true" tabindex="-1"></a>    pyplot.scatter(training_data[index, <span class="dv">0</span>], training_data[index, <span class="dv">1</span>])</span>
<span id="cb11-483"><a href="#cb11-483" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-484"><a href="#cb11-484" aria-hidden="true" tabindex="-1"></a><span class="co"># show the plot</span></span>
<span id="cb11-485"><a href="#cb11-485" aria-hidden="true" tabindex="-1"></a>pyplot.show()</span>
<span id="cb11-486"><a href="#cb11-486" aria-hidden="true" tabindex="-1"></a><span class="in">```</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->



</body></html>