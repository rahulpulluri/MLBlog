[
  {
    "objectID": "index.html#blogs-related-to-machine-learning-concept",
    "href": "index.html#blogs-related-to-machine-learning-concept",
    "title": "Welcome to the Age of Machine Learning",
    "section": "Blogs Related to Machine Learning Concept",
    "text": "Blogs Related to Machine Learning Concept\n\n\n\n\n  \n\n\n\n\nLinear Regression\n\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2023\n\n\nRahul Pulluri\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nProbability and Random Variables\n\n\n\n\n\n\n\n\n\n\n\n\nNov 24, 2023\n\n\nRahul Pulluri\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly/Outlier detection\n\n\n\n\n\n\n\n\n\n\n\n\nNov 24, 2023\n\n\nRahul Pulluri\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nRahul Pulluri\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nNonlinear Regression\n\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nRahul Pulluri\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2023\n\n\nRahul Pulluri\n\n\n12 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/anomaly detection/index.html",
    "href": "posts/anomaly detection/index.html",
    "title": "Anomaly/Outlier detection",
    "section": "",
    "text": "Image Source: https://miro.medium.com/v2/resize:fit:720/format:webp/1*BaOiTGWAoFi-3_-E-rJGdg.jpeg"
  },
  {
    "objectID": "posts/anomaly detection/index.html#anomalyoutlier-detection-an-overview",
    "href": "posts/anomaly detection/index.html#anomalyoutlier-detection-an-overview",
    "title": "Anomaly/Outlier detection",
    "section": "Anomaly/Outlier Detection: An Overview",
    "text": "Anomaly/Outlier Detection: An Overview\nAnomaly detection, also known as outlier detection, is a process in data analysis where unusual patterns, items, or events in a dataset are identified. These anomalies are often referred to as outliers—data points that deviate significantly from the majority of the data. Anomaly detection is crucial as these outliers can indicate critical incidents, such as bank fraud, structural defects, system faults, or errors in text data."
  },
  {
    "objectID": "posts/anomaly detection/index.html#key-concepts",
    "href": "posts/anomaly detection/index.html#key-concepts",
    "title": "Anomaly/Outlier detection",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nAnomalies:\nA data point or a pattern that does not conform to the expected behavior. Anomalies can be:\nPoint Anomalies: Individual data points that are significantly different from the rest of the data. Contextual Anomalies: Anomalies that are context-specific. These might not be outliers in a different context. Collective Anomalies: A group of data points that collectively deviate from the overall data pattern but might not be anomalies when considered individually.\n\n\nOutlier:\nOften used interchangeably with anomalies, outliers are data points that differ drastically from the rest of the dataset."
  },
  {
    "objectID": "posts/anomaly detection/index.html#techniques-in-anomaly-detection",
    "href": "posts/anomaly detection/index.html#techniques-in-anomaly-detection",
    "title": "Anomaly/Outlier detection",
    "section": "Techniques in Anomaly Detection",
    "text": "Techniques in Anomaly Detection\nAnomaly detection in machine learning can be approached in various ways:\n\nStatistical Methods:\n\nSimple statistical metrics like mean, median, standard deviation, and interquartile ranges are used to identify outliers.\nMethods like Z-score and Grubbs’ test can flag data points that deviate from the expected distribution.\n\n\n\nMachine Learning Methods:\n\nSupervised Learning: This requires a dataset with labeled anomalies. Algorithms like logistic regression, support vector machines, or neural networks can be trained to recognize and predict anomalies.\nUnsupervised Learning: Useful when you don’t have labeled data. Algorithms like k-means clustering, DBSCAN, or autoencoders can detect anomalies by understanding the data’s inherent structure and distribution.\nSemi-Supervised Learning: Involves training on a primarily normal dataset to understand the pattern of normality, against which anomalies can be detected.\n\n\n\nProximity-Based Methods:\nThese methods identify anomalies based on the distance or similarity between data points. For example, k-nearest neighbors (KNN) can be used to detect points that are far from their neighbors.\n\n\nDensity-Based Methods:\nTechniques like Local Outlier Factor (LOF) focus on the density of the area around a data point. Anomalies are typically located in low-density regions.\n\n\nClustering-Based Methods:\nAlgorithms like K-means or Hierarchical Clustering can group similar data together. Points that do not fit well into any cluster may be considered anomalies."
  },
  {
    "objectID": "posts/anomaly detection/index.html#applications-of-anomaly-detection",
    "href": "posts/anomaly detection/index.html#applications-of-anomaly-detection",
    "title": "Anomaly/Outlier detection",
    "section": "Applications of Anomaly Detection",
    "text": "Applications of Anomaly Detection\nFraud Detection: In banking and finance, detecting unusual transactions that could indicate fraud.\nIntrusion Detection: In cybersecurity, identifying unusual patterns that could signify a security breach.\nFault Detection: In industrial settings, detecting irregularities in machine or system operations to preempt failures.\nHealth Monitoring: In healthcare, identifying unusual patterns in patient data that could indicate medical issues.\nQuality Control: In manufacturing, detecting products that don’t meet quality standards."
  },
  {
    "objectID": "posts/anomaly detection/index.html#anomaly-detection-method-with-weight-and-height-dataset",
    "href": "posts/anomaly detection/index.html#anomaly-detection-method-with-weight-and-height-dataset",
    "title": "Anomaly/Outlier detection",
    "section": "Anomaly Detection Method with Weight and Height Dataset",
    "text": "Anomaly Detection Method with Weight and Height Dataset\nImporting libraries and loading data\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\n\n\ndf = pd.read_csv(\"R:/MLBlog/posts/anomaly detection/weight-height.csv\")\nFEATURE_COLUMNS = ['Height', 'Weight']\nprint(df.shape)\ndf.sample(5).T\n\n(10000, 3)\n\n\n\n\n\n\n\n\n\n3079\n6246\n7955\n1967\n8314\n\n\n\n\nGender\nMale\nFemale\nFemale\nMale\nFemale\n\n\nHeight\n71.053249\n58.964069\n66.417472\n69.16247\n59.548895\n\n\nWeight\n215.258776\n91.78555\n139.74328\n190.84907\n118.860323"
  },
  {
    "objectID": "posts/anomaly detection/index.html#interquartile-range-iqr",
    "href": "posts/anomaly detection/index.html#interquartile-range-iqr",
    "title": "Anomaly/Outlier detection",
    "section": "Interquartile range (IQR)",
    "text": "Interquartile range (IQR)\n\ndef plot_results(df, df0, method_str=\"IQR\"):\n    # Plotting the data\n    plt.scatter(df['Height'], df['Weight'], label='Data', alpha=0.25)\n    plt.scatter(df0['Height'], df0['Weight'], color='r', alpha=1, label=f'Outliers, {method_str}')\n    plt.xlabel('Height')\n    plt.ylabel('Weight')\n    plt.title(f'Outlier Detection using {method_str}')\n    plt.legend()\n    plt.show();\n    return None\n\n\ndef detect_outliers_iqr(dataframe, column):\n    '''Uses Interquartile range (IQR) method to detect outliers'''\n    # Calculate the first quartile (Q1)\n    Q1 = dataframe[column].quantile(0.25)\n    # Calculate the third quartile (Q3)\n    Q3 = dataframe[column].quantile(0.75)\n    # Calculate the interquartile range (IQR)\n    IQR = Q3 - Q1\n    # Define the lower and upper bounds for outliers\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    # Find the outliers\n    dataframe[f'outliers_iqr_{column}'] = ((dataframe[column] &lt; lower_bound) | (dataframe[column] &gt; upper_bound)).astype(int)\n    return dataframe\n\n\nfor col in ['Weight', 'Height']:\n    df = detect_outliers_iqr(df, col)\ndf0 = df[df['outliers_iqr_Height']+df['outliers_iqr_Weight']&gt;0].copy()\nprint(df0.shape)\nplot_results(df, df0, method_str=\"IQR\")\n\n(8, 5)"
  },
  {
    "objectID": "posts/anomaly detection/index.html#isolation-forest",
    "href": "posts/anomaly detection/index.html#isolation-forest",
    "title": "Anomaly/Outlier detection",
    "section": "Isolation forest",
    "text": "Isolation forest\n\ndef detect_outliers_isolation_forest(dataframe):\n    # Create an Isolation Forest instance\n    clf = IsolationForest(contamination=0.01, n_estimators=100, bootstrap=False, random_state=42)\n    # Fit the model\n    clf.fit(dataframe[FEATURE_COLUMNS])\n    # Predict outliers\n    outliers = clf.predict(dataframe[FEATURE_COLUMNS])\n    # Create a boolean mask for outliers\n    outliers_mask = outliers == -1\n    # Mark the outliers\n    dataframe[f'outliers_iforest'] = (outliers_mask).astype(int)\n    return dataframe\n\n# Detect outliers using the IF method on column\ndf = detect_outliers_isolation_forest(df)\n\nprint(\"Outliers:\")\n    \n# Plotting the data\ndf0 = df[df['outliers_iforest']&gt;0].copy()\nprint(df0.shape)\nplot_results(df, df0, method_str=\"Isolation Forest\")\ndf.describe()\n\nOutliers:\n(100, 6)\n\n\n\n\n\n\n\n\n\n\n\n\nHeight\nWeight\noutliers_iqr_Weight\noutliers_iqr_Height\noutliers_iforest\n\n\n\n\ncount\n10000.000000\n10000.000000\n10000.0000\n10000.000000\n10000.000000\n\n\nmean\n66.367560\n161.440357\n0.0001\n0.000800\n0.010000\n\n\nstd\n3.847528\n32.108439\n0.0100\n0.028274\n0.099504\n\n\nmin\n54.263133\n64.700127\n0.0000\n0.000000\n0.000000\n\n\n25%\n63.505620\n135.818051\n0.0000\n0.000000\n0.000000\n\n\n50%\n66.318070\n161.212928\n0.0000\n0.000000\n0.000000\n\n\n75%\n69.174262\n187.169525\n0.0000\n0.000000\n0.000000\n\n\nmax\n78.998742\n269.989699\n1.0000\n1.000000\n1.000000"
  },
  {
    "objectID": "posts/anomaly detection/index.html#local-outlier-factor",
    "href": "posts/anomaly detection/index.html#local-outlier-factor",
    "title": "Anomaly/Outlier detection",
    "section": "Local Outlier Factor",
    "text": "Local Outlier Factor\n\ndef detect_outliers_lof(dataframe):\n    # Create an LOF instance\n    lof = LocalOutlierFactor(n_neighbors=20, contamination='auto')\n    # Fit the model and predict outlier scores\n    outlier_scores = lof.fit_predict(dataframe[FEATURE_COLUMNS])\n    # Create a boolean mask for outliers\n    outliers_mask = outlier_scores == -1\n    # Mark the outliers\n    dataframe[f'outliers_lof'] = (outliers_mask).astype(int)\n    return dataframe\n\n# Detect outliers using the LOF method on\ndf = detect_outliers_lof(df)\n\nprint(\"Outliers:\")\n    \n# Plotting the data\ndf0 = df[df['outliers_lof']+df['outliers_lof']&gt;0].copy()\nprint(df0.shape)\nplot_results(df, df0, method_str=\"LocalOutlierFactor\")\n\ndf.describe()\n\nOutliers:\n(132, 7)\n\n\n\n\n\n\n\n\n\n\n\n\nHeight\nWeight\noutliers_iqr_Weight\noutliers_iqr_Height\noutliers_iforest\noutliers_lof\n\n\n\n\ncount\n10000.000000\n10000.000000\n10000.0000\n10000.000000\n10000.000000\n10000.000000\n\n\nmean\n66.367560\n161.440357\n0.0001\n0.000800\n0.010000\n0.013200\n\n\nstd\n3.847528\n32.108439\n0.0100\n0.028274\n0.099504\n0.114136\n\n\nmin\n54.263133\n64.700127\n0.0000\n0.000000\n0.000000\n0.000000\n\n\n25%\n63.505620\n135.818051\n0.0000\n0.000000\n0.000000\n0.000000\n\n\n50%\n66.318070\n161.212928\n0.0000\n0.000000\n0.000000\n0.000000\n\n\n75%\n69.174262\n187.169525\n0.0000\n0.000000\n0.000000\n0.000000\n\n\nmax\n78.998742\n269.989699\n1.0000\n1.000000\n1.000000\n1.000000"
  },
  {
    "objectID": "posts/anomaly detection/index.html#one-class-svm",
    "href": "posts/anomaly detection/index.html#one-class-svm",
    "title": "Anomaly/Outlier detection",
    "section": "One-class SVM",
    "text": "One-class SVM\n\ndef detect_outliers_svm(dataframe):\n    # Create a One-Class SVM instance\n    svm = OneClassSVM(nu=0.01)\n    # Fit the model\n    svm.fit(dataframe[FEATURE_COLUMNS])\n    # Predict outlier scores\n    outlier_scores = svm.decision_function(dataframe[FEATURE_COLUMNS])\n    # Create a boolean mask for outliers\n    outliers_mask = outlier_scores &lt; 0\n    # Mark the outliers\n    dataframe[f'outliers_svm'] = (outliers_mask).astype(int)\n    return dataframe\n\n# Detect outliers using the SVM method\ndf = detect_outliers_svm(df)\n\nprint(\"Outliers:\")\n    \n# Plotting the data\ndf0 = df[df['outliers_svm']&gt;0].copy()\nprint(df0.shape)\nplot_results(df, df0, method_str=\"One-class SVM\")\ndf.describe()\n\nOutliers:\n(101, 8)\n\n\n\n\n\n\n\n\n\n\n\n\nHeight\nWeight\noutliers_iqr_Weight\noutliers_iqr_Height\noutliers_iforest\noutliers_lof\noutliers_svm\n\n\n\n\ncount\n10000.000000\n10000.000000\n10000.0000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n\n\nmean\n66.367560\n161.440357\n0.0001\n0.000800\n0.010000\n0.013200\n0.010100\n\n\nstd\n3.847528\n32.108439\n0.0100\n0.028274\n0.099504\n0.114136\n0.099995\n\n\nmin\n54.263133\n64.700127\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n63.505620\n135.818051\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n66.318070\n161.212928\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n75%\n69.174262\n187.169525\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nmax\n78.998742\n269.989699\n1.0000\n1.000000\n1.000000\n1.000000\n1.000000"
  },
  {
    "objectID": "posts/anomaly detection/index.html#autoencoder",
    "href": "posts/anomaly detection/index.html#autoencoder",
    "title": "Anomaly/Outlier detection",
    "section": "Autoencoder",
    "text": "Autoencoder\n\nclass Autoencoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super(Autoencoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, 2*hidden_dim),\n            nn.ReLU(),\n            nn.Linear(2*hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(hidden_dim, 2*hidden_dim),\n            nn.ReLU(),\n            nn.Linear(2*hidden_dim, input_dim),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n\nclass OutlierDataset(Dataset):\n    def __init__(self, data):\n        self.data = torch.tensor(data, dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\ndef detect_outliers_autoencoder(dataframe, hidden_dim=16, num_epochs=10, batch_size=32):\n    # Convert dataframe to standard scaled numpy array\n    scaler = StandardScaler()\n    data = scaler.fit_transform(dataframe[FEATURE_COLUMNS])\n\n    # Create an outlier dataset\n    dataset = OutlierDataset(data)\n\n    # Split data into training and validation sets\n    val_split = int(0.2 * len(dataset))\n    train_set, val_set = torch.utils.data.random_split(dataset, [len(dataset) - val_split, val_split])\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_set, batch_size=batch_size)\n\n    # Initialize the autoencoder model\n    input_dim = data.shape[1]\n    model = Autoencoder(input_dim, hidden_dim)\n\n    # Set the loss function and optimizer\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=3e-2, weight_decay=1e-8)\n\n    # Train the autoencoder\n    for epoch in range(num_epochs):\n        train_loss = 0.0\n        val_loss = 0.0\n\n        # Training\n        model.train()\n        for batch in train_loader:\n            # Zero the gradients\n            optimizer.zero_grad()\n            # Forward pass\n            outputs = model(batch)\n            # Compute the reconstruction loss\n            loss = criterion(outputs, batch)\n            # Backward pass and optimization\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * batch.size(0)\n\n        # Validation\n        model.eval()\n        with torch.no_grad():\n            for batch in val_loader:\n                outputs = model(batch)\n                loss = criterion(outputs, batch)\n                val_loss += loss.item() * batch.size(0)\n\n        train_loss /= len(train_set)\n        val_loss /= len(val_set)\n\n\n        print(f'Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n        \n    # Calculate reconstruction error for each data point\n    reconstructed = model(dataset.data)\n    mse_loss = nn.MSELoss(reduction='none')\n    error = torch.mean(mse_loss(reconstructed, dataset.data), dim=1)\n\n    \n    # Define a threshold for outlier detection\n    threshold = np.percentile(error.detach().numpy(), 99)\n\n    # Create a boolean mask for outliers\n    outliers_mask = (error &gt; threshold)\n\n    # Mark the outliers\n    dataframe[f'outliers_autoenc'] = (outliers_mask)\n    dataframe[f'outliers_autoenc'] = dataframe[f'outliers_autoenc'].astype(int)\n    return dataframe\n\n# Detect outliers using the autoencoder method\ndf = detect_outliers_autoencoder(df)\n\nprint(\"Outliers:\")\n    \n# Plotting the data\ndf0 = df[df['outliers_autoenc']&gt;0].copy()\nprint(df0.shape)\nplot_results(df, df0, method_str=\"One-class Autoencoders\")\n\ndf.describe()\n\nEpoch 1/10: Train Loss: 0.5783, Val Loss: 0.5444\nEpoch 2/10: Train Loss: 0.5579, Val Loss: 0.5471\nEpoch 3/10: Train Loss: 0.5576, Val Loss: 0.5437\nEpoch 4/10: Train Loss: 0.5571, Val Loss: 0.5435\nEpoch 5/10: Train Loss: 0.5575, Val Loss: 0.5440\nEpoch 6/10: Train Loss: 0.5571, Val Loss: 0.5440\nEpoch 7/10: Train Loss: 0.5573, Val Loss: 0.5438\nEpoch 8/10: Train Loss: 0.5572, Val Loss: 0.5436\nEpoch 9/10: Train Loss: 0.5575, Val Loss: 0.5441\nEpoch 10/10: Train Loss: 0.5571, Val Loss: 0.5434\nOutliers:\n(100, 9)\n\n\n\n\n\n\n\n\n\n\n\n\nHeight\nWeight\noutliers_iqr_Weight\noutliers_iqr_Height\noutliers_iforest\noutliers_lof\noutliers_svm\noutliers_autoenc\n\n\n\n\ncount\n10000.000000\n10000.000000\n10000.0000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n\n\nmean\n66.367560\n161.440357\n0.0001\n0.000800\n0.010000\n0.013200\n0.010100\n0.010000\n\n\nstd\n3.847528\n32.108439\n0.0100\n0.028274\n0.099504\n0.114136\n0.099995\n0.099504\n\n\nmin\n54.263133\n64.700127\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n63.505620\n135.818051\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n66.318070\n161.212928\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n75%\n69.174262\n187.169525\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nmax\n78.998742\n269.989699\n1.0000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000"
  },
  {
    "objectID": "posts/classification/index.html#definition-and-overview-of-classification",
    "href": "posts/classification/index.html#definition-and-overview-of-classification",
    "title": "Classification",
    "section": "Definition and Overview of Classification",
    "text": "Definition and Overview of Classification\nClassification in machine learning and statistics is a supervised learning approach where the objective is to categorize data into predefined classes. In simpler terms, it involves deciding which category or class a new observation belongs to, based on a training set of data containing observations whose category membership is known."
  },
  {
    "objectID": "posts/classification/index.html#key-components",
    "href": "posts/classification/index.html#key-components",
    "title": "Classification",
    "section": "Key Components",
    "text": "Key Components\nClasses or Categories: These are the distinct groups or categories that data points are classified into. For instance, in a binary classification, there are two classes, while in multi-class classification, there could be three or more.\nFeatures: These are individual independent variables that act as the input for the process. Each feature contributes to determining the output class.\nLabels: In the training dataset, each data point is tagged with the correct label, which the algorithm then learns to predict."
  },
  {
    "objectID": "posts/classification/index.html#how-classification-works",
    "href": "posts/classification/index.html#how-classification-works",
    "title": "Classification",
    "section": "How Classification Works",
    "text": "How Classification Works\nTraining Phase: The algorithm is trained on a labeled dataset, where it learns the relationship between features and the corresponding class labels.\nModel Building: The algorithm creates a model that maps inputs (features) to desired outputs (labels). This model represents the learned patterns from the data.\nTesting and Prediction: The trained model is then used to predict the class labels of new, unseen data. The performance of the model is typically evaluated using metrics like accuracy, precision, recall, and F1 score."
  },
  {
    "objectID": "posts/classification/index.html#types-of-classification",
    "href": "posts/classification/index.html#types-of-classification",
    "title": "Classification",
    "section": "Types of Classification",
    "text": "Types of Classification\nBinary Classification: Involves two classes. Common examples include spam detection (spam or not spam) and medical diagnoses (sick or healthy).\nMulticlass Classification: Involves more than two classes, but each instance is assigned to only one class. An example would be classifying types of fruits.\nMultilabel Classification: Each instance may be assigned to multiple classes. For example, a news article might be categorized into multiple genres like sports, politics, and finance."
  },
  {
    "objectID": "posts/classification/index.html#applications",
    "href": "posts/classification/index.html#applications",
    "title": "Classification",
    "section": "Applications",
    "text": "Applications\nMedical Diagnosis: Identifying diseases based on symptoms and test results.\nSpam Filtering: Categorizing emails as spam or non-spam.\nSentiment Analysis: Classifying the sentiment of text data (positive, negative, neutral).\nImage Recognition: Categorizing images into various classes like animals, objects, etc. Credit Scoring: Assessing creditworthiness as high-risk or low-risk.\nOnce trained, the algorithm can classify new, unseen data by assessing its similarity to the patterns it has learned. It predicts the likelihood of the new data point falling into one of the predefined categories. This process is akin to your email provider recognizing whether an incoming email is spam or not based on past experiences."
  },
  {
    "objectID": "posts/classification/index.html#classification-algorithms",
    "href": "posts/classification/index.html#classification-algorithms",
    "title": "Classification",
    "section": "Classification Algorithms",
    "text": "Classification Algorithms\n\nLogistic Regression:\nOverview: Logistic Regression is used for binary classification problems. It models the probability that each input belongs to a particular category. Mechanism: This algorithm uses a logistic function to squeeze the output of a linear equation between 0 and 1. The result is the probability that the given input point belongs to a certain class.\n\n\n\nImage Source: https://editor.analyticsvidhya.com/uploads/82109Webp.net-resize.jpg\n\n\nPros: Simple and efficient. Provides a probability score for observations. Low variance, avoiding overfitting.\nCons: Struggles with non-linear data. Assumes no missing values and that predictors are independent.\nApplications: Commonly used in fields like credit scoring, medical fields for disease diagnosis, and predictive analytics.\n\n\nNaive Bayes:\nOverview: Based on Bayes’ Theorem, it assumes independence among predictors. Mechanism: It calculates the probability of each class and the conditional probability of each class given each input value. These probabilities are then used to classify the data.\n\n\n\nImage Source: https://av-eks-blogoptimized.s3.amazonaws.com/Bayes_rule-300x172-300x172-111664.png\n\n\nPros: Fast and efficient. Performs well with a smaller amount of data. Handles multi-class prediction problems well.\nCons: The assumption of independent features is often unrealistic. Can be outperformed by more complex models.\nApplications: Widely used in spam filtering, text analysis, and sentiment analysis.\n\n\nK-Nearest Neighbors (KNN):\nOverview: A non-parametric method used for classification and regression. Mechanism: Classifies data based on how its neighbors are classified. It finds the K nearest points to the new data point and classifies it based on the majority class of these points.\n\n\n\nImage Source: https://static.javatpoint.com/tutorial/machine-learning/images/k-nearest-neighbor-algorithm-for-machine-learning2.png\n\n\nPros: Simple and intuitive. No need to build a model or tune parameters. Flexible to feature/distance choices.\nCons: Slows significantly as data size increases. Sensitive to irrelevant or redundant features.\nApplications: Used in recommendation systems, image recognition, and more.\n\n\nSupport Vector Machine (SVM):\nOverview: Effective in high dimensional spaces and best suited for binary classification. Mechanism: Constructs a hyperplane in a multidimensional space to separate different classes. SVM finds the best margin (distance between the line and the support vectors) to separate the classes.\n\n\n\nImage Source: https://static.javatpoint.com/tutorial/machine-learning/images/support-vector-machine-algorithm5.png\n\n\nPros: Effective in high-dimensional spaces. Uses a subset of training points (support vectors), so it’s memory efficient.\nCons: Not suitable for larger datasets. Does not perform well with noisy data.\nApplications: Used in face detection, text and hypertext categorization, classification of images.\n\n\nDecision Tree:\nOverview: A tree-structure algorithm, where each node represents a feature, each branch a decision rule, and each leaf a class. Mechanism: Splits the data into subsets based on feature values. This process is repeated recursively, resulting in a tree with decision nodes and leaf nodes.\n\n\n\nImage Source: https://lh4.googleusercontent.com/v9UQUwaQTAXVH90b-Ugyw2_61_uErfYvTBtG-RNRNB_eHUFq9AmAN_2IOdfOETnbXImnQVN-wPC7_YzDgf7urCeyhyx5UZmuSwV8BVsV8VnHxl1KtgpuxDifJ4pLE23ooYXLlnc\n\n\nPros: Easy to interpret and explain. Requires little data preparation. Can handle both numerical and categorical data.\nCons: Prone to overfitting, especially with complex trees. Small changes in data can lead to a different tree.\nApplications: Used in customer segmentation, fraud detection, and risk assessment."
  },
  {
    "objectID": "posts/classification/index.html#example-spam-classification-with-naive-bayes-and-support-vector-machine",
    "href": "posts/classification/index.html#example-spam-classification-with-naive-bayes-and-support-vector-machine",
    "title": "Classification",
    "section": "Example: Spam Classification with Naive Bayes and Support Vector Machine",
    "text": "Example: Spam Classification with Naive Bayes and Support Vector Machine\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom sklearn import feature_extraction, model_selection, naive_bayes, metrics, svm\nfrom IPython.display import Image\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline  \n\nExploring the Dataset\n\ndata = pd.read_csv('R:/MLBlog/posts/classification/spam.csv', encoding='latin-1')\ndata.head(n=10)\n\n\n\n\n\n\n\n\nv1\nv2\nUnnamed: 2\nUnnamed: 3\nUnnamed: 4\n\n\n\n\n0\nham\nGo until jurong point, crazy.. Available only ...\nNaN\nNaN\nNaN\n\n\n1\nham\nOk lar... Joking wif u oni...\nNaN\nNaN\nNaN\n\n\n2\nspam\nFree entry in 2 a wkly comp to win FA Cup fina...\nNaN\nNaN\nNaN\n\n\n3\nham\nU dun say so early hor... U c already then say...\nNaN\nNaN\nNaN\n\n\n4\nham\nNah I don't think he goes to usf, he lives aro...\nNaN\nNaN\nNaN\n\n\n5\nspam\nFreeMsg Hey there darling it's been 3 week's n...\nNaN\nNaN\nNaN\n\n\n6\nham\nEven my brother is not like to speak with me. ...\nNaN\nNaN\nNaN\n\n\n7\nham\nAs per your request 'Melle Melle (Oru Minnamin...\nNaN\nNaN\nNaN\n\n\n8\nspam\nWINNER!! As a valued network customer you have...\nNaN\nNaN\nNaN\n\n\n9\nspam\nHad your mobile 11 months or more? U R entitle...\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\nDistribution spam/non-spam plots\n\ncount_Class=pd.value_counts(data[\"v1\"], sort= True)\ncount_Class.plot(kind= 'bar', color= [\"blue\", \"orange\"])\nplt.title('Bar chart')\nplt.show()\n\n\n\n\n\ncount_Class.plot(kind = 'pie',  autopct='%1.0f%%')\nplt.title('Pie chart')\nplt.ylabel('')\nplt.show()\n\n\n\n\nText Analytics\nWe want to find the frequencies of words in the spam and non-spam messages. The words of the messages will be model features.\nWe use the function Counter.\n\ncount1 = Counter(\" \".join(data[data['v1']=='ham'][\"v2\"]).split()).most_common(20)\ndf1 = pd.DataFrame.from_dict(count1)\ndf1 = df1.rename(columns={0: \"words in non-spam\", 1 : \"count\"})\ncount2 = Counter(\" \".join(data[data['v1']=='spam'][\"v2\"]).split()).most_common(20)\ndf2 = pd.DataFrame.from_dict(count2)\ndf2 = df2.rename(columns={0: \"words in spam\", 1 : \"count_\"})\n\n\ndf1.plot.bar(legend = False)\ny_pos = np.arange(len(df1[\"words in non-spam\"]))\nplt.xticks(y_pos, df1[\"words in non-spam\"])\nplt.title('More frequent words in non-spam messages')\nplt.xlabel('words')\nplt.ylabel('number')\nplt.show()\n\n\n\n\n\ndf2.plot.bar(legend = False, color = 'orange')\ny_pos = np.arange(len(df2[\"words in spam\"]))\nplt.xticks(y_pos, df2[\"words in spam\"])\nplt.title('More frequent words in spam messages')\nplt.xlabel('words')\nplt.ylabel('number')\nplt.show()\n\n\n\n\nWe can see that the majority of frequent words in both classes are stop words such as ‘to’, ‘a’, ‘or’ and so on.\nWith stop words we refer to the most common words in a lenguage, there is no simgle, universal list of stop words.\nFeature engineering\nText preprocessing, tokenizing and filtering of stopwords are included in a high level component that is able to build a dictionary of features and transform documents to feature vectors.\nWe remove the stop words in order to improve the analytics\n\nf = feature_extraction.text.CountVectorizer(stop_words = 'english')\nX = f.fit_transform(data[\"v2\"])\nnp.shape(X)\n\n(5572, 8404)\n\n\nWe have created more than 8400 new features. The new feature j in the row i is equal to 1 if the word wj appears in the text example i . It is zero if not.\nPredictive Analysis\nMy goal is to predict if a new sms is spam or non-spam. I assume that is much worse misclassify non-spam than misclassify an spam. (I don’t want to have false positives)\nThe reason is because I normally don’t check the spam messages.\nThe two possible situations are:\n\nNew spam sms in my inbox. (False negative). OUTCOME: I delete it.\nNew non-spam sms in my spam folder (False positive). OUTCOME: I probably don’t read it.\n\nI prefer the first option!!!\nFirst we transform the variable spam/non-spam into binary variable, then we split our data set in training set and test set.\n\ndata[\"v1\"]=data[\"v1\"].map({'spam':1,'ham':0})\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, data['v1'], test_size=0.33, random_state=42)\nprint([np.shape(X_train), np.shape(X_test)])\n\n[(3733, 8404), (1839, 8404)]\n\n\nMultinomial naive bayes classifier\nWe train different bayes models changing the regularization parameter α.\nWe evaluate the accuracy, recall and precision of the model with the test set.\n\nlist_alpha = np.arange(1/100000, 20, 0.11)\nscore_train = np.zeros(len(list_alpha))\nscore_test = np.zeros(len(list_alpha))\nrecall_test = np.zeros(len(list_alpha))\nprecision_test= np.zeros(len(list_alpha))\ncount = 0\nfor alpha in list_alpha:\n    bayes = naive_bayes.MultinomialNB(alpha=alpha)\n    bayes.fit(X_train, y_train)\n    score_train[count] = bayes.score(X_train, y_train)\n    score_test[count]= bayes.score(X_test, y_test)\n    recall_test[count] = metrics.recall_score(y_test, bayes.predict(X_test))\n    precision_test[count] = metrics.precision_score(y_test, bayes.predict(X_test))\n    count = count + 1 \n\nLet’s see the first 10 learning models and their metrics!\n\nmatrix = np.matrix(np.c_[list_alpha, score_train, score_test, recall_test, precision_test])\nmodels = pd.DataFrame(data = matrix, columns = \n             ['alpha', 'Train Accuracy', 'Test Accuracy', 'Test Recall', 'Test Precision'])\nmodels.head(n=10)\n\n\n\n\n\n\n\n\nalpha\nTrain Accuracy\nTest Accuracy\nTest Recall\nTest Precision\n\n\n\n\n0\n0.00001\n0.998661\n0.974443\n0.920635\n0.895753\n\n\n1\n0.11001\n0.997857\n0.976074\n0.936508\n0.893939\n\n\n2\n0.22001\n0.997857\n0.977162\n0.936508\n0.900763\n\n\n3\n0.33001\n0.997589\n0.977162\n0.936508\n0.900763\n\n\n4\n0.44001\n0.997053\n0.977162\n0.936508\n0.900763\n\n\n5\n0.55001\n0.996250\n0.976618\n0.936508\n0.897338\n\n\n6\n0.66001\n0.996518\n0.976074\n0.932540\n0.896947\n\n\n7\n0.77001\n0.996518\n0.976074\n0.924603\n0.903101\n\n\n8\n0.88001\n0.996250\n0.976074\n0.924603\n0.903101\n\n\n9\n0.99001\n0.995982\n0.976074\n0.920635\n0.906250\n\n\n\n\n\n\n\nI select the model with the most test precision\n\nbest_index = models['Test Precision'].idxmax()\nmodels.iloc[best_index, :]\n\nalpha             15.730010\nTrain Accuracy     0.979641\nTest Accuracy      0.969549\nTest Recall        0.777778\nTest Precision     1.000000\nName: 143, dtype: float64\n\n\nMy best model does not produce any false positive, which is our goal.\nLet’s see if there is more than one model with 100% precision !\n\nmodels[models['Test Precision']==1].head(n=5)\n\n\n\n\n\n\n\n\nalpha\nTrain Accuracy\nTest Accuracy\nTest Recall\nTest Precision\n\n\n\n\n143\n15.73001\n0.979641\n0.969549\n0.777778\n1.0\n\n\n144\n15.84001\n0.979641\n0.969549\n0.777778\n1.0\n\n\n145\n15.95001\n0.979641\n0.969549\n0.777778\n1.0\n\n\n146\n16.06001\n0.979373\n0.969549\n0.777778\n1.0\n\n\n147\n16.17001\n0.979373\n0.969549\n0.777778\n1.0\n\n\n\n\n\n\n\nBetween these models with the highest possible precision, we are going to select which has more test accuracy.\n\nbest_index = models[models['Test Precision']==1]['Test Accuracy'].idxmax()\nbayes = naive_bayes.MultinomialNB(alpha=list_alpha[best_index])\nbayes.fit(X_train, y_train)\nmodels.iloc[best_index, :]\n\nalpha             15.730010\nTrain Accuracy     0.979641\nTest Accuracy      0.969549\nTest Recall        0.777778\nTest Precision     1.000000\nName: 143, dtype: float64\n\n\nConfusion matrix with naive bayes classifier\n\nm_confusion_test = metrics.confusion_matrix(y_test, bayes.predict(X_test))\npd.DataFrame(data = m_confusion_test, columns = ['Predicted 0', 'Predicted 1'],\n            index = ['Actual 0', 'Actual 1'])\n\n\n\n\n\n\n\n\nPredicted 0\nPredicted 1\n\n\n\n\nActual 0\n1587\n0\n\n\nActual 1\n56\n196\n\n\n\n\n\n\n\nWe misclassify 56 spam messages as non-spam emails whereas we don’t misclassify any non-spam message.\nSupport Vector Machine\nWe are going to apply the same reasoning applying the support vector machine model with the gaussian kernel.\nWe train different models changing the regularization parameter C.\nWe evaluate the accuracy, recall and precision of the model with the test set.\n\nlist_C = np.arange(500, 2000, 100) #100000\nscore_train = np.zeros(len(list_C))\nscore_test = np.zeros(len(list_C))\nrecall_test = np.zeros(len(list_C))\nprecision_test= np.zeros(len(list_C))\ncount = 0\nfor C in list_C:\n    svc = svm.SVC(C=C)\n    svc.fit(X_train, y_train)\n    score_train[count] = svc.score(X_train, y_train)\n    score_test[count]= svc.score(X_test, y_test)\n    recall_test[count] = metrics.recall_score(y_test, svc.predict(X_test))\n    precision_test[count] = metrics.precision_score(y_test, svc.predict(X_test))\n    count = count + 1 \n\nLet’s see the first 10 learning models and their metrics!\n\nmatrix = np.matrix(np.c_[list_C, score_train, score_test, recall_test, precision_test])\nmodels = pd.DataFrame(data = matrix, columns = \n             ['C', 'Train Accuracy', 'Test Accuracy', 'Test Recall', 'Test Precision'])\nmodels.head(n=10)\n\n\n\n\n\n\n\n\nC\nTrain Accuracy\nTest Accuracy\nTest Recall\nTest Precision\n\n\n\n\n0\n500.0\n1.0\n0.979337\n0.853175\n0.99537\n\n\n1\n600.0\n1.0\n0.979337\n0.853175\n0.99537\n\n\n2\n700.0\n1.0\n0.979337\n0.853175\n0.99537\n\n\n3\n800.0\n1.0\n0.979337\n0.853175\n0.99537\n\n\n4\n900.0\n1.0\n0.979337\n0.853175\n0.99537\n\n\n5\n1000.0\n1.0\n0.979337\n0.853175\n0.99537\n\n\n6\n1100.0\n1.0\n0.979337\n0.853175\n0.99537\n\n\n7\n1200.0\n1.0\n0.979337\n0.853175\n0.99537\n\n\n8\n1300.0\n1.0\n0.979337\n0.853175\n0.99537\n\n\n9\n1400.0\n1.0\n0.979337\n0.853175\n0.99537\n\n\n\n\n\n\n\nI select the model with the most test precision\n\nbest_index = models['Test Precision'].idxmax()\nmodels.iloc[best_index, :]\n\nC                 500.000000\nTrain Accuracy      1.000000\nTest Accuracy       0.979337\nTest Recall         0.853175\nTest Precision      0.995370\nName: 0, dtype: float64\n\n\nMy best model does not produce any false positive, which is our goal.\nConfusion matrix with support vector machine classifier.\n\nm_confusion_test = metrics.confusion_matrix(y_test, svc.predict(X_test))\npd.DataFrame(data = m_confusion_test, columns = ['Predicted 0', 'Predicted 1'],\n            index = ['Actual 0', 'Actual 1'])\n\n\n\n\n\n\n\n\nPredicted 0\nPredicted 1\n\n\n\n\nActual 0\n1586\n1\n\n\nActual 1\n37\n215\n\n\n\n\n\n\n\nWe misclassify 37 spam as non-spam messages whereas we don’t misclassify any non-spam message.\nConclusion\nThe best model I have found is support vector machine with 98.3% accuracy.\nIt classifies every non-spam message correctly (Model precision)\nIt classifies the 87.7% of spam messages correctly (Model recall)"
  },
  {
    "objectID": "posts/nonlinearRegression/index.html#definition",
    "href": "posts/nonlinearRegression/index.html#definition",
    "title": "Nonlinear Regression",
    "section": "Definition",
    "text": "Definition\nNon-linear regression in machine learning is a powerful tool for modeling complex relationships between variables, where these relationships cannot be adequately described using a straight line. In machine learning, non-linear regression is used to predict outcomes based on non-linear interactions of predictor variables. It’s particularly useful in scenarios where the underlying data patterns are inherently non-linear."
  },
  {
    "objectID": "posts/nonlinearRegression/index.html#mathematical-formulation",
    "href": "posts/nonlinearRegression/index.html#mathematical-formulation",
    "title": "Nonlinear Regression",
    "section": "Mathematical Formulation",
    "text": "Mathematical Formulation\nThe general form of a non-linear regression model is: Y = f(X, β) + ε\n\nY is the dependent variable.\nX is the independent variable.\nβ represents the parameters of the model.\nf is a non-linear function.\nε is the error term."
  },
  {
    "objectID": "posts/nonlinearRegression/index.html#characteristics",
    "href": "posts/nonlinearRegression/index.html#characteristics",
    "title": "Nonlinear Regression",
    "section": "Characteristics",
    "text": "Characteristics\nNature of Relationship: Unlike linear regression, where the relationship between variables is a straight line, non-linear regression deals with data where the relationship is curvilinear. This means the graph of the relationship forms a curve, not a straight line.\nTypes of Non-Linear Relationships: Relationships can be exponential, logarithmic, power, or more complex types. For instance, a common non-linear model is the exponential growth model, represented as:\n\nY = a * e^(bX)\nHere, e is the base of the natural logarithm, a and b are model parameters, Y is the dependent variable, and X is the independent variable.\n\nFlexibility: Non-linear regression can model more complex relationships and patterns in data compared to linear models. It’s more flexible in fitting data curves, but this comes with the cost of increased complexity in calculation and interpretation."
  },
  {
    "objectID": "posts/nonlinearRegression/index.html#types-of-non-linear-models",
    "href": "posts/nonlinearRegression/index.html#types-of-non-linear-models",
    "title": "Nonlinear Regression",
    "section": "Types of Non-linear Models:",
    "text": "Types of Non-linear Models:\nPolynomial Regression: Extends linear models by adding polynomial terms, making the model curve.\nDecision Trees and Random Forests: Can model non-linear relationships by splitting the data into branches based on decision rules.\nNeural Networks: Highly capable of capturing non-linear relationships through layers of neurons with non-linear activation functions.\nSupport Vector Machines with Non-linear Kernels: Use kernel functions (like RBF) to project data into higher dimensions where it is linearly separable."
  },
  {
    "objectID": "posts/nonlinearRegression/index.html#applications",
    "href": "posts/nonlinearRegression/index.html#applications",
    "title": "Nonlinear Regression",
    "section": "Applications",
    "text": "Applications\nScientific Data: Often used in scientific data where the rate of change of a variable accelerates or decelerates rapidly, or where the effect of an independent variable is not proportional over its range.\nGrowth Curves: Common in biological systems, such as modeling population growth or the spread of diseases.\nEconomic Data: Used in economics for functions like utility curves, supply/demand curves."
  },
  {
    "objectID": "posts/nonlinearRegression/index.html#examples-of-nonlinear-regression",
    "href": "posts/nonlinearRegression/index.html#examples-of-nonlinear-regression",
    "title": "Nonlinear Regression",
    "section": "Examples of Nonlinear Regression",
    "text": "Examples of Nonlinear Regression\nI.Introduction\nWhen you’re looking at data that seems to form curves or non-straight patterns, using linear regression might not give you the most accurate results. Linear regression is like a tool designed for straight-line relationships, so when your data behaves in a curvy way, it doesn’t quite fit the tool’s assumptions. That’s where non-linear regression steps in. It’s like having a set of tools that can handle curves and bends in the data. These tools, like polynomial, exponential, or logarithmic regression, can adjust to the data’s curves and complexities, giving you more precise predictions and a better match to the real nature of the data."
  },
  {
    "objectID": "posts/nonlinearRegression/index.html#linear",
    "href": "posts/nonlinearRegression/index.html#linear",
    "title": "Nonlinear Regression",
    "section": "1. Linear",
    "text": "1. Linear\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the linear function (e.g., L(x) = ax + b)\na, b = 2, 1\n\n# Generate x values\nx = np.linspace(0, 5, 100)\n\n# Calculate the corresponding y values using the linear function\ny = a * x + b\n\n# Generate random data points with noise\nnp.random.seed(0)\nnoise = np.random.normal(0, 0.5, len(x))\ndata_y = y + noise\n\n# Plot the linear function and data points\nplt.plot(x, y, label='Linear Function', color='blue')\nplt.scatter(x, data_y, label='Data Points', color='red', alpha=0.5)\nplt.xlabel('x')\nplt.ylabel('L(x)')\nplt.title('Linear Function')\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/nonlinearRegression/index.html#polynomial",
    "href": "posts/nonlinearRegression/index.html#polynomial",
    "title": "Nonlinear Regression",
    "section": "2. Polynomial",
    "text": "2. Polynomial\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define coefficients of the polynomial equation (e.g., P(x) = ax^3 + bx^2 + cx + d)\na, b, c, d = 1, -3, 3, -1\n\n# Generate x values\nx = np.linspace(-2, 2, 100)\n\n# Calculate the corresponding y values using the polynomial equation\ny = a * x**3 + b * x**2 + c * x + d\n\n# Generate random data points with noise\nnp.random.seed(0)\nnoise = np.random.normal(0, 0.2, len(x))\ndata_y = y + noise\n\n# Plot the polynomial function and data points\nplt.plot(x, y, label='Polynomial Function', color='blue')\nplt.scatter(x, data_y, label='Data Points', color='red', alpha=0.5)\nplt.xlabel('x')\nplt.ylabel('P(x)')\nplt.title('Polynomial Function')\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/nonlinearRegression/index.html#quadratic",
    "href": "posts/nonlinearRegression/index.html#quadratic",
    "title": "Nonlinear Regression",
    "section": "3. Quadratic",
    "text": "3. Quadratic\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define coefficients of the quadratic equation (e.g., Q(x) = ax^2 + bx + c)\na, b, c = 1, -2, 1\n\n# Generate x values\nx = np.linspace(-2, 3, 100)\n\n# Calculate the corresponding y values using the quadratic equation\ny = a * x**2 + b * x + c\n\n# Generate random data points with noise\nnp.random.seed(0)\nnoise = np.random.normal(0, 0.2, len(x))\ndata_y = y + noise\n\n# Plot the quadratic function and data points\nplt.plot(x, y, label='Quadratic Function', color='blue')\nplt.scatter(x, data_y, label='Data Points', color='red', alpha=0.5)\nplt.xlabel('x')\nplt.ylabel('Q(x)')\nplt.title('Quadratic Function')\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/nonlinearRegression/index.html#exponential",
    "href": "posts/nonlinearRegression/index.html#exponential",
    "title": "Nonlinear Regression",
    "section": "4. Exponential",
    "text": "4. Exponential\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the exponential function (e.g., E(x) = a * e^(bx))\na, b = 1, 0.5\n\n# Generate x values\nx = np.linspace(0, 5, 100)\n\n# Calculate the corresponding y values using the exponential function\ny = a * np.exp(b * x)\n\n# Generate random data points with noise\nnp.random.seed(0)\nnoise = np.random.normal(0, 0.2, len(x))\ndata_y = y + noise\n\n# Plot the exponential function and data points\nplt.plot(x, y, label='Exponential Function', color='blue')\nplt.scatter(x, data_y, label='Data Points', color='red', alpha=0.5)\nplt.xlabel('x')\nplt.ylabel('E(x)')\nplt.title('Exponential Function')\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/nonlinearRegression/index.html#logarithmic",
    "href": "posts/nonlinearRegression/index.html#logarithmic",
    "title": "Nonlinear Regression",
    "section": "5. Logarithmic",
    "text": "5. Logarithmic\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the logarithmic function (e.g., L(x) = a * ln(bx))\na, b = 1, 2\n\n# Generate x values\nx = np.linspace(0.1, 5, 100)\n\n# Calculate the corresponding y values using the logarithmic function\ny = a * np.log(b * x)\n\n# Generate random data points with noise\nnp.random.seed(0)\nnoise = np.random.normal(0, 0.2, len(x))\ndata_y = y + noise\n\n# Plot the logarithmic function and data points\nplt.plot(x, y, label='Logarithmic Function', color='blue')\nplt.scatter(x, data_y, label='Data Points', color='red', alpha=0.5)\nplt.xlabel('x')\nplt.ylabel('L(x)')\nplt.title('Logarithmic Function')\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/nonlinearRegression/index.html#sigmoidallogistic",
    "href": "posts/nonlinearRegression/index.html#sigmoidallogistic",
    "title": "Nonlinear Regression",
    "section": "6. Sigmoidal/Logistic",
    "text": "6. Sigmoidal/Logistic\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Define the sigmoidal/logistic function (e.g., S(x) = 1 / (1 + e^(-x)))\nx = np.linspace(-5, 5, 100)\n\n# Calculate the corresponding y values using the sigmoidal/logistic function\ny = 1 / (1 + np.exp(-x))\n\n# Generate random data points with noise\nnp.random.seed(0)\nnoise = np.random.normal(0, 0.05, len(x))\ndata_y = y + noise\n\n# Plot the sigmoidal/logistic function and data points\nplt.plot(x, y, label='Sigmoidal/Logistic Function', color='blue')\nplt.scatter(x, data_y, label='Data Points', color='red', alpha=0.5)\nplt.xlabel('x')\nplt.ylabel('S(x)')\nplt.title('Sigmoidal/Logistic')\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/clustering/index.html",
    "href": "posts/clustering/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering in machine learning is a type of unsupervised learning technique that involves grouping similar data points together into clusters. In this context, “unsupervised” means that the learning process is conducted without any predefined labels or categories; the algorithm itself discovers the inherent groupings in the data."
  },
  {
    "objectID": "posts/clustering/index.html#what-is-clustering",
    "href": "posts/clustering/index.html#what-is-clustering",
    "title": "Clustering",
    "section": "",
    "text": "Clustering in machine learning is a type of unsupervised learning technique that involves grouping similar data points together into clusters. In this context, “unsupervised” means that the learning process is conducted without any predefined labels or categories; the algorithm itself discovers the inherent groupings in the data."
  },
  {
    "objectID": "posts/clustering/index.html#types-of-clustering-algorithms",
    "href": "posts/clustering/index.html#types-of-clustering-algorithms",
    "title": "Clustering",
    "section": "Types of Clustering Algorithms",
    "text": "Types of Clustering Algorithms\nClustering algorithms come in various forms, each suited to handle different kinds of data and objectives:\nDensity-Based Clustering: These algorithms identify clusters as areas of high density separated by areas of low density. They excel in finding clusters of arbitrary shapes and typically do not assign outliers to any cluster.\nDistribution-Based Clustering: In this approach, clusters are formed based on the likelihood of data points belonging to a certain distribution. A typical model is the Gaussian distribution, where the probability of belonging to a cluster decreases as the distance from the cluster’s center increases.\nCentroid-Based Clustering: This is a widely used approach where clusters are represented by a central vector or a centroid. Each data point is assigned to the cluster with the nearest centroid. K-means is a popular example of centroid-based clustering.\nHierarchical-Based Clustering: These algorithms create a tree of clusters, which is particularly useful for hierarchical or nested data structures. It can be either divisive (top-down approach) or agglomerative (bottom-up approach)."
  },
  {
    "objectID": "posts/clustering/index.html#applications",
    "href": "posts/clustering/index.html#applications",
    "title": "Clustering",
    "section": "Applications:",
    "text": "Applications:\nCustomer Segmentation: Grouping customers based on purchasing behavior, interests, demographics, etc.\nAnomaly Detection: Identifying unusual data points which can be useful in fraud detection.\nPattern Recognition: In areas like bioinformatics, speech recognition, and image analysis."
  },
  {
    "objectID": "posts/clustering/index.html#common-clustering-algorithms",
    "href": "posts/clustering/index.html#common-clustering-algorithms",
    "title": "Clustering",
    "section": "Common Clustering algorithms:",
    "text": "Common Clustering algorithms:\n\nK-Means Clustering algorithm:\nK-means clustering, the most commonly used clustering algorithm, partitions a dataset into K distinct clusters. It does this by assigning each data point to the nearest of K centroids, which are iteratively recalculated as the mean of the points assigned to them. This process repeats until the centroids stabilize, effectively minimizing the variance within each cluster. Widely appreciated for its simplicity and efficiency, this method assumes spherical clusters and can be sensitive to the initial placement of centroids and the presence of outliers.\nImplementation\n\nfrom numpy import unique, where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import KMeans, DBSCAN\n\n# Initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# Define the KMeans model\nkmeans_model = KMeans(n_clusters=2)\n\n# Assign each data point to a cluster using KMeans\nkmeans_result = kmeans_model.fit_predict(training_data)\n\n# Define the DBSCAN model\ndbscan_model = DBSCAN(eps=0.3, min_samples=9)  # Adjust 'eps' and 'min_samples' as needed\n\n# Assign each data point to a cluster using DBSCAN\ndbscan_result = dbscan_model.fit_predict(training_data)\n\n# Get all of the unique clusters from DBSCAN results\ndbscan_clusters = unique(dbscan_result)\n\n# Plot the DBSCAN clusters\nfor dbscan_cluster in dbscan_clusters:\n    # Get data points that fall in this cluster\n    index = where(dbscan_result == dbscan_cluster)\n    # Make the plot\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n\n# Show the DBSCAN plot\npyplot.show()\n\nC:\\Users\\rahul\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\n\n\n\n\n\nDBSCAN clustering algorithm:\nDBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a clustering algorithm that identifies clusters based on the density of data points, without requiring the number of clusters to be predefined. It categorizes points as core points, border points, or noise, depending on the number of nearby points (based on a set distance eps) and a minimum number of points (minPts) required to form a dense region. DBSCAN excels in discovering clusters of arbitrary shapes and sizes, and effectively distinguishes outliers, making it robust in handling noisy datasets. This makes it particularly useful in applications like anomaly detection, geospatial analysis, and image processing.\nImplementation\n\nfrom numpy import unique\nfrom numpy import where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import DBSCAN\n\n# initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# define the model\ndbscan_model = DBSCAN(eps=0.25, min_samples=9)\n\n# train the model\ndbscan_model.fit(training_data)\n\n# assign each data point to a cluster\ndbscan_result = dbscan_model.labels_\n\n# get all of the unique clusters\ndbscan_clusters = unique(dbscan_result)\n\n# plot the DBSCAN clusters\nfor cluster in dbscan_clusters:\n    # get data points that fall in this cluster\n    index = where(dbscan_result == cluster)\n    # make the plot\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n\n# show the DBSCAN plot\npyplot.show()\n\n\n\n\n\n\nHierarchical Clustering:\nThis creates a tree of clusters. It can be either agglomerative (merging small clusters into larger ones) or divisive (splitting clusters).\nAgglomerative Hierarchy clustering algorithm: This bottom-up approach starts with each data point as an individual cluster and iteratively merges the closest pairs of clusters until all points are merged into a single cluster or a specified number of clusters is reached. The process of choosing which clusters to merge at each step is based on a distance metric and a linkage criterion (like single, complete, or average linkage). Agglomerative clustering is particularly useful for small to medium-sized datasets and is often visualized using a dendrogram, which shows the hierarchical relationship between clusters.\nImplementation\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# Define the model\nagglomerative_model = AgglomerativeClustering(n_clusters=2)\n\n# Assign each data point to a cluster\nagglomerative_result = agglomerative_model.fit_predict(training_data)\n\n# Get all unique clusters\nagglomerative_clusters = np.unique(agglomerative_result)\n\n# Plot the clusters\nfor cluster_label in agglomerative_clusters:\n    # Get data points that belong to this cluster\n    cluster_points = training_data[agglomerative_result == cluster_label]\n    # Make the plot\n    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {cluster_label}')\n\n# Show the Agglomerative Hierarchy plot\nplt.legend()\nplt.show()\n\n\n\n\nDivisive Hierarchy clustering algorithm: Conversely, Divisive clustering is a top-down approach. It begins with all data points in a single cluster and recursively splits the most heterogeneous cluster until each cluster contains only one data point or until a specified number of clusters is achieved. This method often involves analyzing the data to determine the best way to perform the splits, typically based on a measure of dissimilarity between data points.\nImplementation\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\n\n# Initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# Define the model for divisive clustering\ndef divisive_clustering(data, stopping_condition):\n    clusters = [data]  # Start with all data points in one cluster\n\n    while len(clusters) &lt; stopping_condition:\n        # Select the cluster to split (you can define your splitting criterion here)\n        cluster_to_split = select_cluster_to_split(clusters)\n\n        # Split the selected cluster into two subclusters\n        subcluster1, subcluster2 = split_cluster(cluster_to_split)\n\n        # Remove the original cluster and add the two subclusters\n        clusters.remove(cluster_to_split)\n        clusters.append(subcluster1)\n        clusters.append(subcluster2)\n\n    return clusters\n\n# Define your custom splitting criterion (e.g., based on variance, dissimilarity, etc.)\ndef select_cluster_to_split(clusters):\n    # For simplicity, we'll select the largest cluster to split.\n    return max(clusters, key=len)\n\n# Define a function to split a cluster (you can implement your splitting logic)\ndef split_cluster(cluster):\n    midpoint = len(cluster) // 2\n    subcluster1 = cluster[:midpoint]\n    subcluster2 = cluster[midpoint:]\n    return subcluster1, subcluster2\n\n# Perform divisive clustering with a specified stopping condition\nclusters = divisive_clustering(training_data.tolist(), stopping_condition=4)\n\n# Plot the divisive clusters\nfor i, cluster in enumerate(clusters):\n    cluster = np.array(cluster)  # Convert back to NumPy array\n    plt.scatter(cluster[:, 0], cluster[:, 1], label=f'Cluster {i+1}')\n\nplt.legend()\nplt.show()\n\n\n\n\n\n\nGaussian Mixture Model algorithm:\nThe Gaussian Mixture Model (GMM) algorithm is a probabilistic approach used for clustering and density estimation, overcoming K-means’ circular data limitation. GMM assumes data is generated from a mixture of Gaussian distributions, making it suitable for various data shapes. It iteratively estimates Gaussian parameters (mean, covariance, mixing coefficients) to fit data, providing soft cluster assignments based on probabilities. GMMs are versatile but sensitive to initializations, requiring a predefined cluster count.\nImplementation\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\nfrom sklearn.mixture import GaussianMixture\n\n# Initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# Define the model\ngaussian_model = GaussianMixture(n_components=2)\n\n# Train the model\ngaussian_model.fit(training_data)\n\n# Assign each data point to a cluster\ngaussian_result = gaussian_model.predict(training_data)\n\n# Get all unique clusters\ngaussian_clusters = np.unique(gaussian_result)\n\n# Plot the Gaussian Mixture clusters\nfor cluster_label in gaussian_clusters:\n    # Get data points that belong to this cluster\n    cluster_points = training_data[gaussian_result == cluster_label]\n    # Make the plot\n    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {cluster_label}')\n\n# Show the Gaussian Mixture plot\nplt.legend()\nplt.show()\n\n\n\n\n\n\nMean-Shift clustering algorithm:\nThe Mean-Shift clustering algorithm is a density-based method used to find clusters in data without specifying the number of clusters in advance. It estimates the density of data points, iteratively shifts them towards denser regions, and identifies clusters where points converge. Mean-Shift is versatile and suitable for various cluster shapes and sizes, commonly used in applications like image segmentation and object tracking.\nImplementation\n\nfrom numpy import unique\nfrom numpy import where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import MeanShift\n\n# initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# define the model\nmean_model = MeanShift()\n\n# assign each data point to a cluster\nmean_result = mean_model.fit_predict(training_data)\n\n# get all of the unique clusters\nmean_clusters = unique(mean_result)\n\n# plot Mean-Shift the clusters\nfor mean_cluster in mean_clusters:\n    # get data points that fall in this cluster\n    index = where(mean_result == mean_cluster)\n    # make the plot\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n\n# show the Mean-Shift plot\npyplot.show()\n\n\n\n\n\n\nBIRCH algorithm:\nThe BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies) algorithm is a memory-efficient hierarchical clustering technique designed for large datasets, particularly numeric data. It builds a tree-like structure by recursively splitting data points into compact summaries that preserve distribution information, allowing scalable clustering. BIRCH is commonly combined with other clustering techniques and is well-suited for high-dimensional data. However, it requires data transformations for handling categorical values. This approach is frequently employed in exploratory data analysis and data compression, making it a valuable tool for various applications.\nImplementation\n\nfrom numpy import unique\nfrom numpy import where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import Birch\n\n# initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# define the model\nbirch_model = Birch(threshold=0.03, n_clusters=2)\n\n# train the model\nbirch_model.fit(training_data)\n\n# assign each data point to a cluster\nbirch_result = birch_model.predict(training_data)\n\n# get all of the unique clusters\nbirch_clusters = unique(birch_result)\n\n# plot the BIRCH clusters\nfor birch_cluster in birch_clusters:\n    # get data points that fall in this cluster\n    index = where(birch_result == birch_cluster)\n    # make the plot\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n\n# show the BIRCH plot\npyplot.show()\n\n\n\n\n\n\nOPTICS algorithm:\nOPTICS (Ordering Points To Identify the Clustering Structure) is a density-based clustering algorithm that excels in identifying clusters in data with varying density. It orders data points to detect different density clusters efficiently, similar to DBSCAN, and assigns specific cluster membership distances to each point. It doesn’t require specifying the number of clusters in advance, making it suitable for various dataset shapes and sizes\nImplementation\n\nfrom numpy import unique\nfrom numpy import where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import OPTICS\n\n# initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# define the model\noptics_model = OPTICS(eps=0.75, min_samples=10)\n\n# assign each data point to a cluster\noptics_result = optics_model.fit_predict(training_data)\n\n# get all of the unique clusters\noptics_clusters = unique(optics_result)\n\n# plot the OPTICS clusters\nfor optics_cluster in optics_clusters:\n    # get data points that fall in this cluster\n    index = where(optics_result == optics_cluster)\n    # make the plot\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n\n# show the OPTICS plot\npyplot.show()\n\n\n\n\n\n\nAffinity Propagation clustering algorithm:\nAffinity Propagation (AP) is a unique clustering algorithm that doesn’t require specifying the number of clusters beforehand. It uses data point communication to discover clusters, and exemplars are found as a consensus among data points. AP is ideal for scenarios where the number of clusters is uncertain, such as computer vision problems. This clustering algorithm models data points as nodes in a network and finds exemplar data points that represent clusters through message exchange. While effective for high-dimensional data and various cluster shapes and sizes, AP can be sensitive to the initial selection of exemplars and may not perform well on large datasets.\nImplementation\n\nfrom numpy import unique\nfrom numpy import where\nfrom matplotlib import pyplot\nfrom sklearn.datasets import make_classification\nfrom sklearn.cluster import AffinityPropagation\n\n# initialize the data set we'll work with\ntraining_data, _ = make_classification(\n    n_samples=1000,\n    n_features=2,\n    n_informative=2,\n    n_redundant=0,\n    n_clusters_per_class=1,\n    random_state=4\n)\n\n# define the model\nmodel = AffinityPropagation(damping=0.7)\n\n# train the model\nmodel.fit(training_data)\n\n# assign each data point to a cluster\nresult = model.predict(training_data)\n\n# get all of the unique clusters\nclusters = unique(result)\n\n# plot the clusters\nfor cluster in clusters:\n    # get data points that fall in this cluster\n    index = where(result == cluster)\n    # make the plot\n    pyplot.scatter(training_data[index, 0], training_data[index, 1])\n\n# show the plot\npyplot.show()"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Rahul Pulluri",
    "section": "",
    "text": "I am Rahul Pulluri and I am from India.\nPursuing Masters in Computer Science at Virginia Tech. As a Master of Engineering student in Computer Science at Virginia Tech, I am deeply immersed in the transformative world of Machine Learning (ML). My academic focus is on unraveling the complexities of ML algorithms and techniques, understanding their theoretical foundations, and exploring their vast potential in practical applications. This journey is driven by a fascination with how ML can revolutionize data analysis and decision-making across various sectors. My studies involve a thorough exploration of the principles behind these advanced algorithms, enabling me to grasp how they can be effectively applied to real-world scenarios.\nThe MEng program at Virginia Tech is instrumental in shaping my expertise in ML. It provides a balanced mix of rigorous academic learning and practical, hands-on experience, allowing me to apply theoretical knowledge to tangible problems. This approach not only enhances my understanding of machine learning but also prepares me to contribute meaningfully in this field. Through this program, I am honing my skills to become an innovative practitioner in ML, capable of leveraging this technology to create impactful, ethical, and forward-thinking solutions in various domains, including digital health and beyond."
  },
  {
    "objectID": "posts/linearRegression/index.html#definition",
    "href": "posts/linearRegression/index.html#definition",
    "title": "Linear Regression",
    "section": "Definition:",
    "text": "Definition:\n\nLinear regression is a statistical method used in data science and machine learning for predictive analysis.\nIt establishes a linear relationship between an independent variable (predictor) and a dependent variable (outcome) for prediction.\nThe method is suitable for continuous or numeric variables like sales, salary, age, etc."
  },
  {
    "objectID": "posts/linearRegression/index.html#importance-in-various-fields",
    "href": "posts/linearRegression/index.html#importance-in-various-fields",
    "title": "Linear Regression",
    "section": "Importance in Various Fields:",
    "text": "Importance in Various Fields:\n\nUsed in stock market forecasting, portfolio management, scientific analysis, and more.\nA simple representation is a sloped straight line in a graph, depicting the best fit line for a set of data."
  },
  {
    "objectID": "posts/linearRegression/index.html#benefits-of-linear-regression",
    "href": "posts/linearRegression/index.html#benefits-of-linear-regression",
    "title": "Linear Regression",
    "section": "Benefits of Linear Regression:",
    "text": "Benefits of Linear Regression:\n\nEasy to implement and interpret.\nScalable and optimal for online settings due to its computational efficiency."
  },
  {
    "objectID": "posts/linearRegression/index.html#linear-regression-equation",
    "href": "posts/linearRegression/index.html#linear-regression-equation",
    "title": "Linear Regression",
    "section": "Linear Regression Equation:",
    "text": "Linear Regression Equation:\nThe equation Y = mX + b, where ‘m’ is the slope and ‘b’ is the intercept, describes the relationship. In machine learning, it’s often expressed as y(x) = p0 + p1  x, where p0 and p1 are parameters to be determined."
  },
  {
    "objectID": "posts/linearRegression/index.html#types-of-linear-regression",
    "href": "posts/linearRegression/index.html#types-of-linear-regression",
    "title": "Linear Regression",
    "section": "Types of Linear Regression:",
    "text": "Types of Linear Regression:\n\nSimple Linear Regression:\nDefinition: Simple Linear Regression is a statistical method that allows us to summarize and study relationships between two continuous (quantitative) variables. This method assumes a linear relationship between the dependent variable and the independent variable.\nEquation: Y = β0 + β1 * X + ε\n\nY: Dependent variable\nX: Independent variable\nβ0: Intercept of the regression line\nβ1: Slope of the regression line\nε: Error term\n\nApplication: For example, predicting the price of a house (dependent variable) based on its size (independent variable)."
  },
  {
    "objectID": "posts/linearRegression/index.html#multiple-linear-regression",
    "href": "posts/linearRegression/index.html#multiple-linear-regression",
    "title": "Linear Regression",
    "section": "Multiple Linear Regression:",
    "text": "Multiple Linear Regression:\nDefinition: Multiple Linear Regression is an extension of simple linear regression and is used to predict the outcome of a dependent variable based on the value of two or more independent variables. This method helps in understanding how changes in independent variables are associated with changes in the dependent variable.\nEquation: Y = β0 + β1 * X1 + β2 * X2 + … + βn * Xn + ε\n\nY: Dependent variable\nX1, X2, …, Xn: Independent variables\nβ0: Intercept\nβ1, β2, …, βn: Slopes for each independent variable\nε: Error term\n\nApplication: Predicting a person’s weight based on their height, age, and diet (three independent variables)."
  },
  {
    "objectID": "posts/linearRegression/index.html#logistic-regression",
    "href": "posts/linearRegression/index.html#logistic-regression",
    "title": "Linear Regression",
    "section": "Logistic Regression:",
    "text": "Logistic Regression:\nDefinition: Logistic Regression is a statistical method for analyzing a dataset in which there are one or more independent variables that determine an outcome. The outcome is measured with a dichotomous variable (in which there are only two possible outcomes). It is used for predicting the probability of a binary outcome based on one or more predictor variables.\nEquation: log(p/(1-p)) = β0 + β1 * X1 + … + βn * Xn\n\np: Probability of the dependent event occurring\nX1, …, Xn: Predictor variables\nβ0: Intercept\nβ1, …, βn: Coefficients for each predictor\n\nApplication: Determining whether an email is spam or not spam, based on features like the email’s content, sender, etc."
  },
  {
    "objectID": "posts/linearRegression/index.html#ordinal-regression",
    "href": "posts/linearRegression/index.html#ordinal-regression",
    "title": "Linear Regression",
    "section": "Ordinal Regression:",
    "text": "Ordinal Regression:\nDefinition: Ordinal Regression is used when the dependent variable is ordinal, meaning it reflects a scale of magnitude. It models the relationship between a set of predictor variables and an ordinal scale dependent variable, where the categories have a natural order, but the intervals between them are not assumed to be equidistant.\nCharacteristics: The categories have a ranked order, but the intervals between the ranks are not necessarily equal.\nApplication: Rating a movie as poor, fair, good, very good, and excellent. Here, the ratings have a natural order but the difference between each category is not quantified."
  },
  {
    "objectID": "posts/linearRegression/index.html#multinomial-logistic-regression",
    "href": "posts/linearRegression/index.html#multinomial-logistic-regression",
    "title": "Linear Regression",
    "section": "Multinomial Logistic Regression:",
    "text": "Multinomial Logistic Regression:\nDefinition: Multinomial Logistic Regression is a classification method that generalizes logistic regression to multiclass problems, i.e., where the dependent variable can have more than two possible nominal (unordered) outcomes. It is used when the outcome involves more than two categories.\nCharacteristics: Similar to logistic regression, but suitable for more than two classes.\nApplication: Predicting the choice of transportation (like car, bus, train, bike) based on factors like distance, cost, and time."
  },
  {
    "objectID": "posts/linearRegression/index.html#example-yearly-amount-spent-prediction",
    "href": "posts/linearRegression/index.html#example-yearly-amount-spent-prediction",
    "title": "Linear Regression",
    "section": "Example: Yearly Amount Spent Prediction",
    "text": "Example: Yearly Amount Spent Prediction\nImport Libraries and Reading Data\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.set_option('display.float_format', lambda x: '%.2f' % x)\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\ndf = pd.read_csv(\"R:/Blog/posts/linearRegression/Ecommerce Customers.csv\")\ndf.head()\n\n\n\n\n\n\n\n\nEmail\nAddress\nAvatar\nAvg. Session Length\nTime on App\nTime on Website\nLength of Membership\nYearly Amount Spent\n\n\n\n\n0\nmstephenson@fernandez.com\n835 Frank Tunnel\\nWrightmouth, MI 82180-9605\nViolet\n34.50\n12.66\n39.58\n4.08\n587.95\n\n\n1\nhduke@hotmail.com\n4547 Archer Common\\nDiazchester, CA 06566-8576\nDarkGreen\n31.93\n11.11\n37.27\n2.66\n392.20\n\n\n2\npallen@yahoo.com\n24645 Valerie Unions Suite 582\\nCobbborough, D...\nBisque\n33.00\n11.33\n37.11\n4.10\n487.55\n\n\n3\nriverarebecca@gmail.com\n1414 David Throughway\\nPort Jason, OH 22070-1220\nSaddleBrown\n34.31\n13.72\n36.72\n3.12\n581.85\n\n\n4\nmstephens@davidson-herman.com\n14023 Rodriguez Passage\\nPort Jacobville, PR 3...\nMediumAquaMarine\n33.33\n12.80\n37.54\n4.45\n599.41\n\n\n\n\n\n\n\nThe variables to be used in the data were determined.\n\ndf = df.drop(['Email',\"Address\",\"Avatar\"], axis=1) \n\nAdvanced Functional Exploratory Data Analysis\nGeneral structure of the data is analyzed\n\ndef check_df(dataframe, head=5):\n    print(\"##################### Columns #####################\")\n    print(dataframe.columns)\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head #####################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail #####################\")\n    print(dataframe.tail(head))\n    print(\"##################### NA #####################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.describe([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n\ncheck_df(df)\n\n##################### Columns #####################\nIndex(['Avg. Session Length', 'Time on App', 'Time on Website',\n       'Length of Membership', 'Yearly Amount Spent'],\n      dtype='object')\n##################### Shape #####################\n(500, 5)\n##################### Types #####################\nAvg. Session Length     float64\nTime on App             float64\nTime on Website         float64\nLength of Membership    float64\nYearly Amount Spent     float64\ndtype: object\n##################### Head #####################\n   Avg. Session Length  Time on App  Time on Website  Length of Membership  \\\n0                34.50        12.66            39.58                  4.08   \n1                31.93        11.11            37.27                  2.66   \n2                33.00        11.33            37.11                  4.10   \n3                34.31        13.72            36.72                  3.12   \n4                33.33        12.80            37.54                  4.45   \n\n   Yearly Amount Spent  \n0               587.95  \n1               392.20  \n2               487.55  \n3               581.85  \n4               599.41  \n##################### Tail #####################\n     Avg. Session Length  Time on App  Time on Website  Length of Membership  \\\n495                33.24        13.57            36.42                  3.75   \n496                34.70        11.70            37.19                  3.58   \n497                32.65        11.50            38.33                  4.96   \n498                33.32        12.39            36.84                  2.34   \n499                33.72        12.42            35.77                  2.74   \n\n     Yearly Amount Spent  \n495               573.85  \n496               529.05  \n497               551.62  \n498               456.47  \n499               497.78  \n##################### NA #####################\nAvg. Session Length     0\nTime on App             0\nTime on Website         0\nLength of Membership    0\nYearly Amount Spent     0\ndtype: int64\n##################### Quantiles #####################\n                      count   mean   std    min     0%     5%    50%    95%  \\\nAvg. Session Length  500.00  33.05  0.99  29.53  29.53  31.45  33.08  34.59   \nTime on App          500.00  12.05  0.99   8.51   8.51  10.53  11.98  13.67   \nTime on Website      500.00  37.06  1.01  33.91  33.91  35.46  37.07  38.78   \nLength of Membership 500.00   3.53  1.00   0.27   0.27   1.81   3.53   5.08   \nYearly Amount Spent  500.00 499.31 79.31 256.67 256.67 376.29 498.89 628.15   \n\n                        99%   100%    max  \nAvg. Session Length   35.43  36.14  36.14  \nTime on App           14.22  15.13  15.13  \nTime on Website       39.25  40.01  40.01  \nLength of Membership   5.84   6.92   6.92  \nYearly Amount Spent  701.00 765.52 765.52  \n\n\nA scatterplot to observe the relationship between the variables was created.\n\nsns.pairplot(df, kind = \"reg\")\n\n\n\n\nAnalysis of Variable Types\nIt is necessary to determine the types of variables. Thus, we can determine the types of the variables and make them suitable for the model.\nIt gives the names of the numeric, categorical but cardinal variables in the data set.\ncat_cols: Categorical variable list\nnum_cols: Numeric variable list\ncat_but_car: Categorical but cardinal variable list\nThe function named grab_col_names helps to determine the types of variables.\n\ndef grab_col_names(dataframe, cat_th=10, car_th=20):\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() &lt; cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() &gt; car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car\n\ncat_cols, num_cols, cat_but_car = grab_col_names(df)\n\nObservations: 500\nVariables: 5\ncat_cols: 0\nnum_cols: 5\ncat_but_car: 0\nnum_but_cat: 0\n\n\nOutlier Analysis\nValues that go far beyond the general trend in the data are called outliers. Especially in linear methods, the effects of outliers are more severe.Outliers cause bias in the data set.For all these reasons, it needs to be analyzed.\n\ndef outlier_thresholds(dataframe, col_name, q1=0.10, q3=0.90):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\ndef check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] &gt; up_limit) | (dataframe[col_name] &lt; low_limit)].any(axis=None):\n        return True\n    else:\n        return False\nfor col in num_cols:\n    print(col, check_outlier(df, col))\n\nAvg. Session Length False\nTime on App False\nTime on Website False\nLength of Membership False\nYearly Amount Spent False\n\n\nAvg. Session Length False Time on App False Time on Website False Length of Membership False Yearly Amount Spent False\nAnalysis Of Missing Values\nMissing values may cause problems while setting up the model. It must be detected and necessary actions must be taken.\nNo missing values were found for the relevant data.\n\ndef missing_values_table(dataframe, na_name=False):\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() &gt; 0]\n\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[na_columns].isnull().sum() / dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df, end=\"\\n\")\n\n    if na_name:\n        return na_columns\n\nmissing_values_table(df)\n\nEmpty DataFrame\nColumns: [n_miss, ratio]\nIndex: []\n\n\nEmpty DataFrame Columns: [n_miss, ratio] Index: []\nCorrelation Analysis\nValues with high correlation affect the target variable to a similar extent. Therefore, we can eliminate one of the variables with high correlation between two variables and use the other.\nWhen the data was examined, no variable with a high correlation of more than 90% was found.\n\ndef high_correlated_cols(dataframe, plot=False, corr_th=0.90):\n    corr = dataframe.corr()\n    cor_matrix = corr.abs()\n    upper_triangle_matrix = cor_matrix.where(np.triu(np.ones(cor_matrix.shape), k=1).astype(bool))\n    drop_list = [col for col in upper_triangle_matrix.columns if any(upper_triangle_matrix[col] &gt; corr_th)]\n    if plot:\n        import seaborn as sns\n        import matplotlib.pyplot as plt\n        sns.set(rc={'figure.figsize': (10, 5)})\n        sns.heatmap(corr, cmap=\"RdBu\", annot=True)\n        plt.show(block=True)\n    return drop_list\n\n\nhigh_correlated_cols(df,plot=True)\n\n\n\n\n[]"
  },
  {
    "objectID": "posts/linearRegression/index.html#linear-regression",
    "href": "posts/linearRegression/index.html#linear-regression",
    "title": "Linear Regression",
    "section": "LINEAR REGRESSION",
    "text": "LINEAR REGRESSION\n\nLinear regression models the relationship between dependent and independent variable/variables linearly.\nIn order to create the model, dependent and independent variables were defined.\n\nX = df.drop('Yearly Amount Spent', axis=1) \n\ny = df[[\"Yearly Amount Spent\"]]\n\nBuilding the Model\nBuilding the Model A train and test set by dividing the data into two is created . By training the model with one part and testing the model with the other part, it can be determined how successful the model is.\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)\n\nreg_model = LinearRegression().fit(X_train, y_train)\n\n# constant (b - bias)\nprint(reg_model.intercept_)\n\n# coefficients (w - weights)\nprint(reg_model.coef_)\n\n[-1047.73920526]\n[[25.78854257 38.85150472  0.25638467 61.49204989]]\n\n\nPrediction of dependent variable\n\ny_pred = reg_model.predict(X_test)"
  },
  {
    "objectID": "posts/linearRegression/index.html#evaluating-forecast-success",
    "href": "posts/linearRegression/index.html#evaluating-forecast-success",
    "title": "Linear Regression",
    "section": "Evaluating Forecast Success",
    "text": "Evaluating Forecast Success\n!(rmse.png)\n\nnp.sqrt(mean_squared_error(y_test, y_pred))\n\n8.84848631350032\n\n\nThe ratio of independent variables description of dependent variable\n\nreg_model.score(X_test, y_test)\n\n0.9892888134002329"
  },
  {
    "objectID": "posts/linearRegression/index.html#visualization-of-the-model",
    "href": "posts/linearRegression/index.html#visualization-of-the-model",
    "title": "Linear Regression",
    "section": "Visualization of the Model",
    "text": "Visualization of the Model\nFinally, the actual values corresponding to the predicted values of the model are shown in the graph.\n\ny_pred = pd.DataFrame(y_pred)\ny_test = y_test.reset_index(drop=True)\ndf_ = pd.concat([y_test,y_pred], axis=1)\ndf_.columns = [\"y_test\",\"y_pred\"]\nplt.figure(figsize=(15,10))\nplt.plot(df_)\nplt.legend([\"ACTUAL VALUES\" , \"MODEL PREDICTION\"])\n\n&lt;matplotlib.legend.Legend at 0x1fe04f5f7f0&gt;"
  },
  {
    "objectID": "posts/probability/index.html#definition",
    "href": "posts/probability/index.html#definition",
    "title": "Probability and Random Variables",
    "section": "Definition",
    "text": "Definition\nProbability theory is the mathematical study of uncertainty. It plays a central role in machine learning, as the design of learning algorithms often relies on probabilistic assumption of the data. This set of notes attempts to cover some basic probability theory that serves as a background for the class."
  },
  {
    "objectID": "posts/probability/index.html#probability-space",
    "href": "posts/probability/index.html#probability-space",
    "title": "Probability and Random Variables",
    "section": "Probability Space",
    "text": "Probability Space\nWhen we speak about probability, we often refer to the probability of an event of uncertain nature taking place. For example, we speak about the probability of rain next Tuesday. Therefore, in order to discuss probability theory formally, we must first clarify what the possible events are to which we would like to attach probability.\nFormally, a probability space is defined by the triple (Ω, F, P), where\n\n• Ω is the space of possible outcomes (or outcome space),\n• F ⊆ 2^Ω (the power set of Ω) is the space of (measurable) events (or event space),\n• P is the probability measure (or probability distribution) that maps an event E ∈ F to a real value between 0 and 1 (think of P as a function).\n\nGiven the outcome space Ω, there is some restrictions as to what subset of 2^Ω can be considered an event space F:\n\n• The trivial event Ω and the empty event ∅ is in F.\n• The event space F is closed under (countable) union, i.e., if α, β ∈ F, then α ∪ β ∈ F.\n• The even space F is closed under complement, i.e., if α ∈ F, then (Ω  α) ∈ F.\n\nExample 1. Suppose we throw a (six-sided) dice. The space of possible outcomes Ω = {1, 2, 3, 4, 5, 6}. We may decide that the events of interest is whether the dice throw is odd or even. This event space will be given by F = {∅, {1, 3, 5}, {2, 4, 6}, Ω}.\nNote that when the outcome space Ω is finite, as in the previous example, we often take the event space F to be 2Ω. This treatment is not fully general, but it is often sufficient for practical purposes. However, when the outcome space is infinite, we must be careful to define what the event space is.\nGiven an event space F, the probability measure P must satisfy certain axioms.\n• (non-negativity) For all α ∈ F, P(α) ≥ 0.\n• (trivial event) P(Ω) = 1.\n• (additivity) For all α, β ∈ F and α ∩ β = ∅, P(α ∪ β) = P(α) + P(β).\nExample 2. Returning to our dice example, suppose we now take the event space F to be 2Ω. Further, we define a probability distribution P over F such that\nP({1}) = P({2}) = · · · = P({6}) = 1/6\nthen this distribution P completely specifies the probability of any given event happening (through the additivity axiom). For example, the probability of an even dice throw will be\nP({2, 4, 6}) = P({2}) + P({4}) + P({6}) = 1/6 + 1/6 + 1/6 = 1/2\nsince each of these events are disjoint."
  },
  {
    "objectID": "posts/probability/index.html#random-variables",
    "href": "posts/probability/index.html#random-variables",
    "title": "Probability and Random Variables",
    "section": "Random Variables",
    "text": "Random Variables\nRandom variables play an important role in probability theory. The most important fact about random variables is that they are not variables. They are actually functions that map outcomes (in the outcome space) to real values. In terms of notation, we usually denote random variables by a capital letter. Let’s see an example.\nExample 3. Again, consider the process of throwing a dice. Let X be a random variable that depends on the outcome of the throw. A natural choice for X would be to map the outcome i to the value i, i.e., mapping the event of throwing an “one” to the value of 1. Note that we could have chosen some strange mappings too. For example, we could have a random variable Y that maps all outcomes to 0, which would be a very boring function, or a random variable Z that maps the outcome i to the value of 2^i if i is odd and the value of −i if i is even, which would be quite strange indeed.\nIn a sense, random variables allow us to abstract away from the formal notion of event space, as we can define random variables that capture the appropriate events. For example, consider the event space of odd or even dice throw in Example 1. We could have defined a random variable that takes on value 1 if outcome i is odd and 0 otherwise. These type of binary random variables are very common in practice, and are known as indicator variables, taking its name from its use to indicate whether a certain event has happened. So why did we introduce event space? That is because when one studies probability theory (more 2 rigorously) using measure theory, the distinction between outcome space and event space will be very important. This topic is too advanced to be covered in this short review note.In any case, it is good to keep in mind that event space is not always simply the power set of the outcome space.\nFrom here onwards, we will talk mostly about probability with respect to random variables. While some probability concepts can be defined meaningfully without using them, random variables allow us to provide a more uniform treatment of probability theory. For notations, the probability of a random variable X taking on the value of a will be denoted by either\n                 P(X = a) or Px(a)\nWe will also denote the range of a random variable X by V al(X)."
  },
  {
    "objectID": "posts/probability/index.html#probability-distribution",
    "href": "posts/probability/index.html#probability-distribution",
    "title": "Probability and Random Variables",
    "section": "Probability Distribution",
    "text": "Probability Distribution\nProbability distributions are fundamental to understanding random variables in statistics and probability theory. They provide a systematic way to describe the likelihood of different outcomes from a random process. Let’s explore this concept in detail for both discrete and continuous random variables:"
  },
  {
    "objectID": "posts/probability/index.html#probability-distributions-for-discrete-random-variables",
    "href": "posts/probability/index.html#probability-distributions-for-discrete-random-variables",
    "title": "Probability and Random Variables",
    "section": "Probability Distributions for Discrete Random Variables",
    "text": "Probability Distributions for Discrete Random Variables\n\nProbability Mass Function (PMF):\nThe PMF is a function that gives the probability that a discrete random variable is exactly equal to some value. It satisfies two conditions: The sum of probabilities for all possible outcomes equals 1, and the probability for each individual outcome is between 0 and 1.\nKey Discrete Distributions:\nBinomial Distribution: Models the number of successes in a fixed number of independent Bernoulli trials (like flipping a coin several times). It is characterized by two parameters: the number of trials (n) and the probability of success (p) in each trial.\n\nimport numpy as np\nfrom scipy.stats import binom\n\n# Define parameters\nn = 10  # Number of trials\np = 0.5  # Probability of success in each trial\n\n# Generate binomial random variables\nrv = binom(n, p)\n\n# Probability Mass Function (PMF)\nx = np.arange(0, n+1)\npmf = rv.pmf(x)\n\n# Plot the PMF\nimport matplotlib.pyplot as plt\nplt.bar(x, pmf)\nplt.xlabel('Number of Successes')\nplt.ylabel('Probability')\nplt.title('Binomial Distribution PMF')\nplt.show()\n\n\n\n\nPoisson Distribution: Used for counting the number of events that occur in a fixed interval of time or space. It is characterized by its rate parameter (λ), which is the average number of events in the given interval.\n\nimport numpy as np\nfrom scipy.stats import poisson\n\n# Define rate parameter (average events in the interval)\nlambda_ = 3.0\n\n# Generate Poisson random variables\nrv = poisson(mu=lambda_)\n\n# Probability Mass Function (PMF)\nx = np.arange(0, 11)  # Example: Counting events from 0 to 10\npmf = rv.pmf(x)\n\n# Plot the PMF\nimport matplotlib.pyplot as plt\nplt.bar(x, pmf)\nplt.xlabel('Number of Events')\nplt.ylabel('Probability')\nplt.title('Poisson Distribution PMF')\nplt.show()\n\n\n\n\nGeometric Distribution: Describes the number of Bernoulli trials needed to get one success. Its key parameter is the probability of success (p) in each trial.\n\nimport numpy as np\nfrom scipy.stats import geom\n\n# Define probability of success in each trial\np = 0.2\n\n# Generate Geometric random variables\nrv = geom(p)\n\n# Probability Mass Function (PMF)\nx = np.arange(1, 11)  # Number of trials needed (1 to 10)\npmf = rv.pmf(x)\n\n# Plot the PMF\nimport matplotlib.pyplot as plt\nplt.bar(x, pmf)\nplt.xlabel('Number of Trials Needed')\nplt.ylabel('Probability')\nplt.title('Geometric Distribution PMF')\nplt.show()"
  },
  {
    "objectID": "posts/probability/index.html#probability-distributions-for-continuous-random-variables",
    "href": "posts/probability/index.html#probability-distributions-for-continuous-random-variables",
    "title": "Probability and Random Variables",
    "section": "Probability Distributions for Continuous Random Variables",
    "text": "Probability Distributions for Continuous Random Variables\n\n**Probability Density Function (PDF):\nThe PDF is used to specify the probability of the random variable falling within a particular range of values, as opposed to taking on any one specific value. The probability for any single point is zero for a continuous distribution. Instead, the area under the PDF curve within a range of values indicates the probability of falling within that range.\nKey Continuous Distributions:\nNormal (Gaussian) Distribution: One of the most important probability distributions, known for its bell-shaped curve. It is characterized by two parameters: the mean (μ), which indicates the center of the distribution, and the standard deviation (σ), which measures the spread.\n\nimport numpy as np\nfrom scipy.stats import norm\n\n# Define parameters\nmu = 0  # Mean\nsigma = 1  # Standard Deviation\n\n# Generate normal random variables\nrv = norm(loc=mu, scale=sigma)\n\n# Probability Density Function (PDF)\nx = np.linspace(-3, 3, 100)  # Range of values\npdf = rv.pdf(x)\n\n# Plot the PDF\nimport matplotlib.pyplot as plt\nplt.plot(x, pdf)\nplt.xlabel('Value')\nplt.ylabel('Probability Density')\nplt.title('Normal Distribution PDF')\nplt.show()\n\n\n\n\nExponential Distribution: Used to model the time elapsed between events in a process with a constant average rate (e.g., radioactive decay). It is characterized by its rate parameter (λ).\n\nimport numpy as np\nfrom scipy.stats import expon\n\n# Define rate parameter (λ)\nlambda_ = 0.5\n\n# Generate Exponential random variables\nrv = expon(scale=1/lambda_)\n\n# Probability Density Function (PDF)\nx = np.linspace(0, 10, 100)  # Range of values\npdf = rv.pdf(x)\n\n# Plot the PDF\nimport matplotlib.pyplot as plt\nplt.plot(x, pdf)\nplt.xlabel('Time')\nplt.ylabel('Probability Density')\nplt.title('Exponential Distribution PDF')\nplt.show()\n\n\n\n\nUniform Distribution: Describes a situation where all outcomes are equally likely. In its continuous form, it’s defined by two parameters, a and b, which are the minimum and maximum values of the distribution.\n\nimport numpy as np\nfrom scipy.stats import uniform\n\n# Define parameters\na = 0  # Minimum value\nb = 1  # Maximum value\n\n# Generate Uniform random variables\nrv = uniform(loc=a, scale=b-a)\n\n# Probability Density Function (PDF)\nx = np.linspace(a-0.1, b+0.1, 100)  # Range of values\npdf = rv.pdf(x)\n\n# Plot the PDF\nimport matplotlib.pyplot as plt\nplt.plot(x, pdf)\nplt.xlabel('Value')\nplt.ylabel('Probability Density')\nplt.title('Uniform Distribution PDF')\nplt.show()"
  }
]