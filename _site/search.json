[
  {
    "objectID": "index.html#blogs-related-to-machine-learning-concept",
    "href": "index.html#blogs-related-to-machine-learning-concept",
    "title": "Welcome to the Age of Machine Learning",
    "section": "Blogs Related to Machine Learning Concept",
    "text": "Blogs Related to Machine Learning Concept\n\n\n\n\n  \n\n\n\n\nLinear Regression\n\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2023\n\n\nRahul Pulluri\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nProbability and Random Variables\n\n\n\n\n\n\n\n\n\n\n\n\nNov 24, 2023\n\n\nRahul Pulluri\n\n\n8 min\n\n\n\n\n\n\n  \n\n\n\n\nAnomaly/Outlier detection\n\n\n\n\n\n\n\n\n\n\n\n\nNov 24, 2023\n\n\nRahul Pulluri\n\n\n9 min\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2023\n\n\nRahul Pulluri\n\n\n11 min\n\n\n\n\n\n\n  \n\n\n\n\nNonlinear Regression\n\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2023\n\n\nRahul Pulluri\n\n\n5 min\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\n\n\n\n\n\nOct 27, 2023\n\n\nRahul Pulluri\n\n\n12 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/anomaly detection/index.html",
    "href": "posts/anomaly detection/index.html",
    "title": "Anomaly/Outlier detection",
    "section": "",
    "text": "Image Source: https://miro.medium.com/v2/resize:fit:720/format:webp/1*BaOiTGWAoFi-3_-E-rJGdg.jpeg"
  },
  {
    "objectID": "posts/anomaly detection/index.html#anomalyoutlier-detection-an-overview",
    "href": "posts/anomaly detection/index.html#anomalyoutlier-detection-an-overview",
    "title": "Anomaly/Outlier detection",
    "section": "Anomaly/Outlier Detection: An Overview",
    "text": "Anomaly/Outlier Detection: An Overview\nAnomaly detection, also known as outlier detection, is a process in data analysis where unusual patterns, items, or events in a dataset are identified. These anomalies are often referred to as outliers—data points that deviate significantly from the majority of the data. Anomaly detection is crucial as these outliers can indicate critical incidents, such as bank fraud, structural defects, system faults, or errors in text data."
  },
  {
    "objectID": "posts/anomaly detection/index.html#key-concepts",
    "href": "posts/anomaly detection/index.html#key-concepts",
    "title": "Anomaly/Outlier detection",
    "section": "Key Concepts",
    "text": "Key Concepts\n\nAnomalies:\nA data point or a pattern that does not conform to the expected behavior. Anomalies can be:\nPoint Anomalies: Individual data points that are significantly different from the rest of the data. Contextual Anomalies: Anomalies that are context-specific. These might not be outliers in a different context. Collective Anomalies: A group of data points that collectively deviate from the overall data pattern but might not be anomalies when considered individually.\n\n\nOutlier:\nOften used interchangeably with anomalies, outliers are data points that differ drastically from the rest of the dataset."
  },
  {
    "objectID": "posts/anomaly detection/index.html#techniques-in-anomaly-detection",
    "href": "posts/anomaly detection/index.html#techniques-in-anomaly-detection",
    "title": "Anomaly/Outlier detection",
    "section": "Techniques in Anomaly Detection",
    "text": "Techniques in Anomaly Detection\nAnomaly detection in machine learning can be approached in various ways:\n\nStatistical Methods:\n\nSimple statistical metrics like mean, median, standard deviation, and interquartile ranges are used to identify outliers.\nMethods like Z-score and Grubbs’ test can flag data points that deviate from the expected distribution.\n\n\n\nMachine Learning Methods:\n\nSupervised Learning: This requires a dataset with labeled anomalies. Algorithms like logistic regression, support vector machines, or neural networks can be trained to recognize and predict anomalies.\nUnsupervised Learning: Useful when you don’t have labeled data. Algorithms like k-means clustering, DBSCAN, or autoencoders can detect anomalies by understanding the data’s inherent structure and distribution.\nSemi-Supervised Learning: Involves training on a primarily normal dataset to understand the pattern of normality, against which anomalies can be detected.\n\n\n\nProximity-Based Methods:\nThese methods identify anomalies based on the distance or similarity between data points. For example, k-nearest neighbors (KNN) can be used to detect points that are far from their neighbors.\n\n\nDensity-Based Methods:\nTechniques like Local Outlier Factor (LOF) focus on the density of the area around a data point. Anomalies are typically located in low-density regions.\n\n\nClustering-Based Methods:\nAlgorithms like K-means or Hierarchical Clustering can group similar data together. Points that do not fit well into any cluster may be considered anomalies."
  },
  {
    "objectID": "posts/anomaly detection/index.html#applications-of-anomaly-detection",
    "href": "posts/anomaly detection/index.html#applications-of-anomaly-detection",
    "title": "Anomaly/Outlier detection",
    "section": "Applications of Anomaly Detection",
    "text": "Applications of Anomaly Detection\nFraud Detection: In banking and finance, detecting unusual transactions that could indicate fraud.\nIntrusion Detection: In cybersecurity, identifying unusual patterns that could signify a security breach.\nFault Detection: In industrial settings, detecting irregularities in machine or system operations to preempt failures.\nHealth Monitoring: In healthcare, identifying unusual patterns in patient data that could indicate medical issues.\nQuality Control: In manufacturing, detecting products that don’t meet quality standards."
  },
  {
    "objectID": "posts/anomaly detection/index.html#anomaly-detection-method-with-weight-and-height-dataset",
    "href": "posts/anomaly detection/index.html#anomaly-detection-method-with-weight-and-height-dataset",
    "title": "Anomaly/Outlier detection",
    "section": "Anomaly Detection Method with Weight and Height Dataset",
    "text": "Anomaly Detection Method with Weight and Height Dataset\nImporting libraries and loading data\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.svm import OneClassSVM\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\n\n\ndf = pd.read_csv(\"R:/MLBlog/posts/anomaly detection/weight-height.csv\")\nFEATURE_COLUMNS = ['Height', 'Weight']\nprint(df.shape)\ndf.sample(5).T\n\n(10000, 3)\n\n\n\n\n\n\n\n\n\n3079\n6246\n7955\n1967\n8314\n\n\n\n\nGender\nMale\nFemale\nFemale\nMale\nFemale\n\n\nHeight\n71.053249\n58.964069\n66.417472\n69.16247\n59.548895\n\n\nWeight\n215.258776\n91.78555\n139.74328\n190.84907\n118.860323"
  },
  {
    "objectID": "posts/anomaly detection/index.html#interquartile-range-iqr",
    "href": "posts/anomaly detection/index.html#interquartile-range-iqr",
    "title": "Anomaly/Outlier detection",
    "section": "Interquartile range (IQR)",
    "text": "Interquartile range (IQR)\n\ndef plot_results(df, df0, method_str=\"IQR\"):\n    # Plotting the data\n    plt.scatter(df['Height'], df['Weight'], label='Data', alpha=0.25)\n    plt.scatter(df0['Height'], df0['Weight'], color='r', alpha=1, label=f'Outliers, {method_str}')\n    plt.xlabel('Height')\n    plt.ylabel('Weight')\n    plt.title(f'Outlier Detection using {method_str}')\n    plt.legend()\n    plt.show();\n    return None\n\n\ndef detect_outliers_iqr(dataframe, column):\n    '''Uses Interquartile range (IQR) method to detect outliers'''\n    # Calculate the first quartile (Q1)\n    Q1 = dataframe[column].quantile(0.25)\n    # Calculate the third quartile (Q3)\n    Q3 = dataframe[column].quantile(0.75)\n    # Calculate the interquartile range (IQR)\n    IQR = Q3 - Q1\n    # Define the lower and upper bounds for outliers\n    lower_bound = Q1 - 1.5 * IQR\n    upper_bound = Q3 + 1.5 * IQR\n    # Find the outliers\n    dataframe[f'outliers_iqr_{column}'] = ((dataframe[column] &lt; lower_bound) | (dataframe[column] &gt; upper_bound)).astype(int)\n    return dataframe\n\n\nfor col in ['Weight', 'Height']:\n    df = detect_outliers_iqr(df, col)\ndf0 = df[df['outliers_iqr_Height']+df['outliers_iqr_Weight']&gt;0].copy()\nprint(df0.shape)\nplot_results(df, df0, method_str=\"IQR\")\n\n(8, 5)"
  },
  {
    "objectID": "posts/anomaly detection/index.html#isolation-forest",
    "href": "posts/anomaly detection/index.html#isolation-forest",
    "title": "Anomaly/Outlier detection",
    "section": "Isolation forest",
    "text": "Isolation forest\n\ndef detect_outliers_isolation_forest(dataframe):\n    # Create an Isolation Forest instance\n    clf = IsolationForest(contamination=0.01, n_estimators=100, bootstrap=False, random_state=42)\n    # Fit the model\n    clf.fit(dataframe[FEATURE_COLUMNS])\n    # Predict outliers\n    outliers = clf.predict(dataframe[FEATURE_COLUMNS])\n    # Create a boolean mask for outliers\n    outliers_mask = outliers == -1\n    # Mark the outliers\n    dataframe[f'outliers_iforest'] = (outliers_mask).astype(int)\n    return dataframe\n\n# Detect outliers using the IF method on column\ndf = detect_outliers_isolation_forest(df)\n\nprint(\"Outliers:\")\n    \n# Plotting the data\ndf0 = df[df['outliers_iforest']&gt;0].copy()\nprint(df0.shape)\nplot_results(df, df0, method_str=\"Isolation Forest\")\ndf.describe()\n\nOutliers:\n(100, 6)\n\n\n\n\n\n\n\n\n\n\n\n\nHeight\nWeight\noutliers_iqr_Weight\noutliers_iqr_Height\noutliers_iforest\n\n\n\n\ncount\n10000.000000\n10000.000000\n10000.0000\n10000.000000\n10000.000000\n\n\nmean\n66.367560\n161.440357\n0.0001\n0.000800\n0.010000\n\n\nstd\n3.847528\n32.108439\n0.0100\n0.028274\n0.099504\n\n\nmin\n54.263133\n64.700127\n0.0000\n0.000000\n0.000000\n\n\n25%\n63.505620\n135.818051\n0.0000\n0.000000\n0.000000\n\n\n50%\n66.318070\n161.212928\n0.0000\n0.000000\n0.000000\n\n\n75%\n69.174262\n187.169525\n0.0000\n0.000000\n0.000000\n\n\nmax\n78.998742\n269.989699\n1.0000\n1.000000\n1.000000"
  },
  {
    "objectID": "posts/anomaly detection/index.html#local-outlier-factor",
    "href": "posts/anomaly detection/index.html#local-outlier-factor",
    "title": "Anomaly/Outlier detection",
    "section": "Local Outlier Factor",
    "text": "Local Outlier Factor\n\ndef detect_outliers_lof(dataframe):\n    # Create an LOF instance\n    lof = LocalOutlierFactor(n_neighbors=20, contamination='auto')\n    # Fit the model and predict outlier scores\n    outlier_scores = lof.fit_predict(dataframe[FEATURE_COLUMNS])\n    # Create a boolean mask for outliers\n    outliers_mask = outlier_scores == -1\n    # Mark the outliers\n    dataframe[f'outliers_lof'] = (outliers_mask).astype(int)\n    return dataframe\n\n# Detect outliers using the LOF method on\ndf = detect_outliers_lof(df)\n\nprint(\"Outliers:\")\n    \n# Plotting the data\ndf0 = df[df['outliers_lof']+df['outliers_lof']&gt;0].copy()\nprint(df0.shape)\nplot_results(df, df0, method_str=\"LocalOutlierFactor\")\n\ndf.describe()\n\nOutliers:\n(132, 7)\n\n\n\n\n\n\n\n\n\n\n\n\nHeight\nWeight\noutliers_iqr_Weight\noutliers_iqr_Height\noutliers_iforest\noutliers_lof\n\n\n\n\ncount\n10000.000000\n10000.000000\n10000.0000\n10000.000000\n10000.000000\n10000.000000\n\n\nmean\n66.367560\n161.440357\n0.0001\n0.000800\n0.010000\n0.013200\n\n\nstd\n3.847528\n32.108439\n0.0100\n0.028274\n0.099504\n0.114136\n\n\nmin\n54.263133\n64.700127\n0.0000\n0.000000\n0.000000\n0.000000\n\n\n25%\n63.505620\n135.818051\n0.0000\n0.000000\n0.000000\n0.000000\n\n\n50%\n66.318070\n161.212928\n0.0000\n0.000000\n0.000000\n0.000000\n\n\n75%\n69.174262\n187.169525\n0.0000\n0.000000\n0.000000\n0.000000\n\n\nmax\n78.998742\n269.989699\n1.0000\n1.000000\n1.000000\n1.000000"
  },
  {
    "objectID": "posts/anomaly detection/index.html#one-class-svm",
    "href": "posts/anomaly detection/index.html#one-class-svm",
    "title": "Anomaly/Outlier detection",
    "section": "One-class SVM",
    "text": "One-class SVM\n\ndef detect_outliers_svm(dataframe):\n    # Create a One-Class SVM instance\n    svm = OneClassSVM(nu=0.01)\n    # Fit the model\n    svm.fit(dataframe[FEATURE_COLUMNS])\n    # Predict outlier scores\n    outlier_scores = svm.decision_function(dataframe[FEATURE_COLUMNS])\n    # Create a boolean mask for outliers\n    outliers_mask = outlier_scores &lt; 0\n    # Mark the outliers\n    dataframe[f'outliers_svm'] = (outliers_mask).astype(int)\n    return dataframe\n\n# Detect outliers using the SVM method\ndf = detect_outliers_svm(df)\n\nprint(\"Outliers:\")\n    \n# Plotting the data\ndf0 = df[df['outliers_svm']&gt;0].copy()\nprint(df0.shape)\nplot_results(df, df0, method_str=\"One-class SVM\")\ndf.describe()\n\nOutliers:\n(101, 8)\n\n\n\n\n\n\n\n\n\n\n\n\nHeight\nWeight\noutliers_iqr_Weight\noutliers_iqr_Height\noutliers_iforest\noutliers_lof\noutliers_svm\n\n\n\n\ncount\n10000.000000\n10000.000000\n10000.0000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n\n\nmean\n66.367560\n161.440357\n0.0001\n0.000800\n0.010000\n0.013200\n0.010100\n\n\nstd\n3.847528\n32.108439\n0.0100\n0.028274\n0.099504\n0.114136\n0.099995\n\n\nmin\n54.263133\n64.700127\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n63.505620\n135.818051\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n66.318070\n161.212928\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n75%\n69.174262\n187.169525\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nmax\n78.998742\n269.989699\n1.0000\n1.000000\n1.000000\n1.000000\n1.000000"
  },
  {
    "objectID": "posts/anomaly detection/index.html#autoencoder",
    "href": "posts/anomaly detection/index.html#autoencoder",
    "title": "Anomaly/Outlier detection",
    "section": "Autoencoder",
    "text": "Autoencoder\n\nclass Autoencoder(nn.Module):\n    def __init__(self, input_dim, hidden_dim):\n        super(Autoencoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, 2*hidden_dim),\n            nn.ReLU(),\n            nn.Linear(2*hidden_dim, hidden_dim),\n            nn.ReLU()\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(hidden_dim, 2*hidden_dim),\n            nn.ReLU(),\n            nn.Linear(2*hidden_dim, input_dim),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n\nclass OutlierDataset(Dataset):\n    def __init__(self, data):\n        self.data = torch.tensor(data, dtype=torch.float32)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        return self.data[idx]\n\ndef detect_outliers_autoencoder(dataframe, hidden_dim=16, num_epochs=10, batch_size=32):\n    # Convert dataframe to standard scaled numpy array\n    scaler = StandardScaler()\n    data = scaler.fit_transform(dataframe[FEATURE_COLUMNS])\n\n    # Create an outlier dataset\n    dataset = OutlierDataset(data)\n\n    # Split data into training and validation sets\n    val_split = int(0.2 * len(dataset))\n    train_set, val_set = torch.utils.data.random_split(dataset, [len(dataset) - val_split, val_split])\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_set, batch_size=batch_size)\n\n    # Initialize the autoencoder model\n    input_dim = data.shape[1]\n    model = Autoencoder(input_dim, hidden_dim)\n\n    # Set the loss function and optimizer\n    criterion = nn.MSELoss()\n    optimizer = optim.Adam(model.parameters(), lr=3e-2, weight_decay=1e-8)\n\n    # Train the autoencoder\n    for epoch in range(num_epochs):\n        train_loss = 0.0\n        val_loss = 0.0\n\n        # Training\n        model.train()\n        for batch in train_loader:\n            # Zero the gradients\n            optimizer.zero_grad()\n            # Forward pass\n            outputs = model(batch)\n            # Compute the reconstruction loss\n            loss = criterion(outputs, batch)\n            # Backward pass and optimization\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item() * batch.size(0)\n\n        # Validation\n        model.eval()\n        with torch.no_grad():\n            for batch in val_loader:\n                outputs = model(batch)\n                loss = criterion(outputs, batch)\n                val_loss += loss.item() * batch.size(0)\n\n        train_loss /= len(train_set)\n        val_loss /= len(val_set)\n\n\n        print(f'Epoch {epoch+1}/{num_epochs}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n        \n    # Calculate reconstruction error for each data point\n    reconstructed = model(dataset.data)\n    mse_loss = nn.MSELoss(reduction='none')\n    error = torch.mean(mse_loss(reconstructed, dataset.data), dim=1)\n\n    \n    # Define a threshold for outlier detection\n    threshold = np.percentile(error.detach().numpy(), 99)\n\n    # Create a boolean mask for outliers\n    outliers_mask = (error &gt; threshold)\n\n    # Mark the outliers\n    dataframe[f'outliers_autoenc'] = (outliers_mask)\n    dataframe[f'outliers_autoenc'] = dataframe[f'outliers_autoenc'].astype(int)\n    return dataframe\n\n# Detect outliers using the autoencoder method\ndf = detect_outliers_autoencoder(df)\n\nprint(\"Outliers:\")\n    \n# Plotting the data\ndf0 = df[df['outliers_autoenc']&gt;0].copy()\nprint(df0.shape)\nplot_results(df, df0, method_str=\"One-class Autoencoders\")\n\ndf.describe()\n\nEpoch 1/10: Train Loss: 0.5783, Val Loss: 0.5444\nEpoch 2/10: Train Loss: 0.5579, Val Loss: 0.5471\nEpoch 3/10: Train Loss: 0.5576, Val Loss: 0.5437\nEpoch 4/10: Train Loss: 0.5571, Val Loss: 0.5435\nEpoch 5/10: Train Loss: 0.5575, Val Loss: 0.5440\nEpoch 6/10: Train Loss: 0.5571, Val Loss: 0.5440\nEpoch 7/10: Train Loss: 0.5573, Val Loss: 0.5438\nEpoch 8/10: Train Loss: 0.5572, Val Loss: 0.5436\nEpoch 9/10: Train Loss: 0.5575, Val Loss: 0.5441\nEpoch 10/10: Train Loss: 0.5571, Val Loss: 0.5434\nOutliers:\n(100, 9)\n\n\n\n\n\n\n\n\n\n\n\n\nHeight\nWeight\noutliers_iqr_Weight\noutliers_iqr_Height\noutliers_iforest\noutliers_lof\noutliers_svm\noutliers_autoenc\n\n\n\n\ncount\n10000.000000\n10000.000000\n10000.0000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n10000.000000\n\n\nmean\n66.367560\n161.440357\n0.0001\n0.000800\n0.010000\n0.013200\n0.010100\n0.010000\n\n\nstd\n3.847528\n32.108439\n0.0100\n0.028274\n0.099504\n0.114136\n0.099995\n0.099504\n\n\nmin\n54.263133\n64.700127\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n63.505620\n135.818051\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n50%\n66.318070\n161.212928\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n75%\n69.174262\n187.169525\n0.0000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\nmax\n78.998742\n269.989699\n1.0000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000"
  }
]